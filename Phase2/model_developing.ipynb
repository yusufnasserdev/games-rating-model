{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "er0QnrBCHvsa"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dYhdkduKHvsb"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-14T14:19:38.839351Z",
          "start_time": "2023-04-14T14:19:38.815268Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSZ7Y2KYHvsc",
        "outputId": "78b702ab-bc18-45d2-a027-ba47fd38d7bb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import plotly.graph_objs as go\n",
        "import seaborn as sns\n",
        "\n",
        "import cv2\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif, f_regression, mutual_info_regression\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "from skimage.feature import local_binary_pattern\n",
        "\n",
        "from category_encoders import TargetEncoder,  CountEncoder\n",
        "\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from scipy import stats\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from scipy import stats\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHminn2LHvsf"
      },
      "outputs": [],
      "source": [
        "# Download nltk packages if not already downloaded\n",
        "# nltk.download()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "hv1Te7_-Hvsg"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-14T10:33:46.390000Z",
          "start_time": "2023-04-14T10:33:46.279076Z"
        },
        "id": "GaIZuBBIHvsh"
      },
      "outputs": [],
      "source": [
        "dateparse = lambda x: datetime.strptime(x, '%d/%m/%Y')\n",
        "\n",
        "df_origin = pd.read_csv('games-classification-dataset.csv', \n",
        "                        parse_dates=['Original Release Date' , 'Current Version Release Date'], \n",
        "                        date_parser=dateparse)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eMfXwc88Hvsh"
      },
      "source": [
        "## Data Gathering"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "k8YwULxZHvsh"
      },
      "source": [
        "### Download the icons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9LthNw9Hvsi",
        "outputId": "75a23749-29bc-4e64-9ad2-6fb715c883f0"
      },
      "outputs": [],
      "source": [
        "# Convert to string\n",
        "df_origin['Icon URL'] = df_origin['Icon URL'].astype(str)\n",
        "\n",
        "# Download the images\n",
        "def download_image(url, filename):\n",
        "    r = requests.get(url, stream=True)\n",
        "    if r.status_code == 200:\n",
        "        with open(filename, 'wb') as f:\n",
        "            r.raw.decode_content = True\n",
        "            shutil.copyfileobj(r.raw, f)\n",
        "\n",
        "\n",
        "# Create a folder to store the images\n",
        "if not os.path.exists('icons'):\n",
        "    os.makedirs('icons')\n",
        "\n",
        "# Download the images if they don't exist\n",
        "for i, row in tqdm(df_origin.iterrows(), total=df_origin.shape[0]):\n",
        "    if not os.path.exists('icons/' + str(i) + '.png'):\n",
        "        download_image(row['Icon URL'], 'icons/' + str(i) + '.png')\n",
        "        \n",
        "# Replace the URL with the icon filename which is the index of the row\n",
        "df_origin['Icon URL'] = df_origin.apply(lambda row : f'icons/{row.name}.png', axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vakpxzSJHvsk"
      },
      "source": [
        "### Download the reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYthqi66Hvsk"
      },
      "outputs": [],
      "source": [
        "def web_scrapping():\n",
        "    data = pd.DataFrame(columns = [\"ID\",\"Reviews\"])\n",
        "    # Read CSV file\n",
        "    # with open('games-regression-dataset.csv', newline='') as csvfile:\n",
        "    with open('games-regression-dataset.csv', newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header row\n",
        "        for row in reader:\n",
        "            url = row[0]  # URL is in first column\n",
        "            filename = 'Reviews/'+os.path.basename(url)  # Extract filename from URL\n",
        "            url +=  \"?see-all=reviews\"\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:  # Check if request was successful\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                blocks = soup.findAll(\"blockquote\")\n",
        "                review_list = []\n",
        "                for blockquote in blocks:\n",
        "                    review = blockquote.find('p').text\n",
        "                    review_list.append(review)\n",
        "                if len(review_list)!=0:\n",
        "                    filename = re.sub(r'[^\\d]+', '', filename)\n",
        "                    new_row = {'ID': filename,\"Reviews\": review_list}\n",
        "                    data = data._append(new_row, ignore_index=True)\n",
        "                    \n",
        "    data.to_csv('Reviews.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b_eA5aWHvsl"
      },
      "outputs": [],
      "source": [
        "def reviews_splitting(data):\n",
        "\n",
        "    for i in range (len(data)):\n",
        "        data.at[i, 'Reviews'] = data.at[i, \"Reviews\"].split(\"',\")\n",
        "        data.at[i,\"ID\"] =data.at[i,\"ID\"]\n",
        "        \n",
        "    data = data.explode('Reviews')\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9WGY_OLHvsl"
      },
      "outputs": [],
      "source": [
        "def reviews_cleaning(data):\n",
        "  # Convert text to lowercase\n",
        "  data['Reviews'] = data['Reviews'].apply(lambda x: str(x).lower())\n",
        "\n",
        "  # Replace newline characters with an empty string\n",
        "  data['Reviews'] = data['Reviews'].apply(lambda x: re.sub(r'\\\\n', ' ', x))\n",
        "\n",
        "  # Remove black squares\n",
        "  data['Reviews'] = data['Reviews'].apply(lambda x: re.sub(r'\\\\u25a0', '', x))\n",
        "\n",
        "  # Remove special characters and punctuations\n",
        "  data['Reviews'] = data['Reviews'].apply(lambda x: re.sub(r'[^\\w\\s]+', '', x))\n",
        "\n",
        "  # Remove numbers\n",
        "  data['Reviews'] = data['Reviews'].apply(lambda x: \" \".join([word for word in x.split() if not any(char.isdigit() for char in word)]))\n",
        "\n",
        "  # Remove extra whitespaces\n",
        "  data['Reviews'] = data['Reviews'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
        "\n",
        "  # Remove stop words\n",
        "  data['Reviews'] = data['Reviews'].apply(lambda x: \" \".join([word for word in x.lower().split() if word not in stop_words]))\n",
        "\n",
        "  # Remove empty strings\n",
        "  data = data[data['Reviews'].apply(lambda x: len(x)>0)]\n",
        "  \n",
        "  # Group by ID\n",
        "  data = data.groupby('ID')['Reviews'].apply(list).reset_index()\n",
        "  \n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILz0DEpUHvsm"
      },
      "outputs": [],
      "source": [
        "# Web scrapping to get the html of the reviews, Only run once\n",
        "# web_scrapping()\n",
        "\n",
        "df_reviews = pd.read_csv('Reviews.csv')\n",
        "\n",
        "df_reviews = reviews_splitting(df_reviews)\n",
        "\n",
        "df_reviews = reviews_cleaning(df_reviews)\n",
        "\n",
        "## Merge The Sentiment with the original dataset\n",
        "df_origin = df_origin.merge(df_reviews, on='ID', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV_0Bqj8Hvsm",
        "outputId": "f552d942-f31a-4165-fb97-5e972234d9d9"
      },
      "outputs": [],
      "source": [
        "df_origin.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HaAuHBE-Hvsn"
      },
      "source": [
        "## Reviews Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOyXYz9BHvsn"
      },
      "outputs": [],
      "source": [
        "def reviews_preprocess(data, test=False, df_old=None):\n",
        "  \n",
        "  # Apply sentiment_analysis\n",
        "  \n",
        "  if test == False:\n",
        "    sia_reviews = SentimentIntensityAnalyzer()\n",
        "    pickle.dump(sia_reviews, open('encoders/sia_reviews.pkl', 'wb'))\n",
        "  else:\n",
        "    sia_reviews = pickle.load(open('encoders/sia_reviews.pkl', 'rb'))\n",
        "  \n",
        "  # Only preprocess the reviews that are not null\n",
        "  data['Reviews'] = data['Reviews'].apply(lambda x: [sia_reviews.polarity_scores(review)['compound'] for review in x] if isinstance(x, list) and len(x)>0 else [])\n",
        "\n",
        "  # Get the lowest, highest and average Reviews\n",
        "  data['lowest_review'] = data['Reviews'].apply(lambda x: min(x) if len(x) > 0 else None)\n",
        "  data['highest_review'] = data['Reviews'].apply(lambda x: max(x) if len(x) > 0 else None)\n",
        "  data['average_review'] = data['Reviews'].apply(lambda x: np.median(x) if len(x) > 0 else None)\n",
        "  \n",
        "  if test == True:\n",
        "    data['lowest_review'] = data['lowest_review'].fillna(df_old['lowest_review'].median())\n",
        "    data['highest_review'] = data['highest_review'].fillna(df_old['highest_review'].median())\n",
        "    data['average_review'] = data['average_review'].fillna(df_old['average_review'].median())\n",
        "  else:\n",
        "    data['lowest_review'] = data['lowest_review'].fillna(data['lowest_review'].median())\n",
        "    data['highest_review'] = data['highest_review'].fillna(data['highest_review'].median())\n",
        "    data['average_review'] = data['average_review'].fillna(data['average_review'].median())\n",
        "  \n",
        "  return data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "6LHKjpiyHvsn"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PIk-60WFHvso"
      },
      "source": [
        "### General information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-13T16:07:00.183708Z",
          "start_time": "2023-04-13T16:07:00.127616Z"
        },
        "id": "udUwpb-hHvso"
      },
      "outputs": [],
      "source": [
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-13T14:33:05.138536Z",
          "start_time": "2023-04-13T14:33:05.138536Z"
        },
        "id": "_hCjHrwzHvso"
      },
      "outputs": [],
      "source": [
        "# df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-13T14:33:07.893814Z",
          "start_time": "2023-04-13T14:33:07.893814Z"
        },
        "id": "J_hY99liHvsp"
      },
      "outputs": [],
      "source": [
        "# df.isnull().sum()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oX0TOxVhHvsp"
      },
      "source": [
        "### Genres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-13T14:33:21.139604Z",
          "start_time": "2023-04-13T14:33:21.139604Z"
        },
        "id": "_0D6Q5nyHvsp"
      },
      "outputs": [],
      "source": [
        "def genres_analysis(_df):\n",
        "    _df['Genres'] = _df['Genres'].astype(str)\n",
        "    _df['Genres'] = _df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
        "\n",
        "    genre_counts = _df.explode('Genres').groupby('Genres').size().sort_values(ascending=False)\n",
        "    print(genre_counts)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bnQiH-nyHvsp"
      },
      "source": [
        "### Developer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-13T14:33:43.637371Z",
          "start_time": "2023-04-13T14:33:43.637371Z"
        },
        "id": "A-iyxVnWHvsq"
      },
      "outputs": [],
      "source": [
        "def dev_analysis(_df):\n",
        "\n",
        "    print(_df['Developer'].value_counts())\n",
        "    \n",
        "    # print the number developers with more than 1 game\n",
        "    print(len(_df['Developer'].value_counts()[_df['Developer'].value_counts() > 1]))\n",
        "\n",
        "    print(_df['Developer'].unique().size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pLpXI91_Hvsq"
      },
      "source": [
        "### Dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwLFQHQSHvsq"
      },
      "outputs": [],
      "source": [
        "def date_analysis(_df):\n",
        "    # Plot the distribution of the date columns\n",
        "\n",
        "    fig, ax = plt.subplots(5, 2, figsize=(20, 20))\n",
        "\n",
        "    # df = date_preprocessing(df)\n",
        "\n",
        "    # game_age distribution\n",
        "    sns.histplot(_df['game_age'], ax=ax[0, 0])\n",
        "    sns.boxplot(_df['game_age'], ax=ax[0, 1], orient='h')\n",
        "\n",
        "    # last_update distribution\n",
        "    sns.histplot(_df['last_update'], ax=ax[1, 0])\n",
        "    sns.boxplot(_df['last_update'], ax=ax[1, 1], orient='h')\n",
        "\n",
        "    # Original Release Date distribution\n",
        "    sns.histplot(_df['Original Release Date'], ax=ax[2, 0])\n",
        "    sns.boxplot(_df['Original Release Date'], ax=ax[2, 1], orient='h')\n",
        "\n",
        "    # Current Version Release Date distribution\n",
        "    sns.histplot(_df['Current Version Release Date'], ax=ax[3, 0])\n",
        "    sns.boxplot(_df['Current Version Release Date'], ax=ax[3, 1], orient='h')\n",
        "\n",
        "    # maintaning_period distribution\n",
        "    sns.histplot(_df['maintaning_period'], ax=ax[4, 0])\n",
        "    sns.boxplot(_df['maintaning_period'], ax=ax[4, 1], orient='h')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Date outliers are legitimate data points that are worth keeping, they are not errors nor anomalies"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jJxiCYoXHvsr"
      },
      "source": [
        "### Languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-13T14:33:49.943309Z",
          "start_time": "2023-04-13T14:33:49.943309Z"
        },
        "id": "XULWiDzHHvsr"
      },
      "outputs": [],
      "source": [
        "def lang_analysis(_df):\n",
        "    _df['Languages'] = _df['Languages'].astype(str)\n",
        "    _df['Languages'] = _df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
        "\n",
        "    langs_counts = _df.explode('Languages').groupby('Languages').size().sort_values(ascending=False)\n",
        "    print(langs_counts[1:30])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-cVy1UEHvss"
      },
      "outputs": [],
      "source": [
        "# df.hist(figsize=(15, 15))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ov19V8buHvss"
      },
      "source": [
        "## Dates preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-14T10:36:05.737187Z",
          "start_time": "2023-04-14T10:36:05.699973Z"
        },
        "id": "OiCX3NwvHvss"
      },
      "outputs": [],
      "source": [
        "def date_preprocess(_df, test=False, df_old=None):\n",
        "    # Convert the datetime to ordinal\n",
        "    _df['Original Release Date'] = _df['Original Release Date'].apply(lambda x: x.toordinal())\n",
        "    _df['Current Version Release Date'] = _df['Current Version Release Date'].apply(lambda x: x.toordinal())\n",
        "    \n",
        "    if test == False:\n",
        "        # Fill the missing values with the median\n",
        "        _df['Original Release Date'] = _df['Original Release Date'].fillna(_df['Original Release Date'].median())\n",
        "        _df['Current Version Release Date'] = _df['Current Version Release Date'].fillna(_df['Current Version Release Date'].median())\n",
        "    else:\n",
        "        # Fill the missing values with the median of the old dataset\n",
        "        _df['Original Release Date'] = _df['Original Release Date'].fillna(df_old['Original Release Date'].median())\n",
        "        _df['Current Version Release Date'] = _df['Current Version Release Date'].fillna(df_old['Current Version Release Date'].median())\n",
        "    \n",
        "    # Create a new column with the age of the game\n",
        "    _df['game_age'] = datetime.now().toordinal() - _df['Original Release Date']\n",
        "\n",
        "    # Create a new column with the time since the last update\n",
        "    _df['last_update'] = datetime.now().toordinal() - _df['Current Version Release Date']\n",
        "    \n",
        "    # Create a new column with the maintaning period\n",
        "    _df['maintaning_period'] = _df['game_age'] - _df['last_update']\n",
        "\n",
        "    return _df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "p6ocFra6Hvst"
      },
      "source": [
        "## Developer preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IJvJX4g3Hvst"
      },
      "source": [
        "### Target encoding approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQhoy2JOHvst"
      },
      "outputs": [],
      "source": [
        "def dev_preprocess_target_enc(_df, test=False):\n",
        "    \"\"\"Preprocesses and encodes the 'Developer' column using target encoding.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): DataFrame (either train or test, depending on the value of test)\n",
        "        test (bool, optional): Boolean flag indicating whether the data is for testing (True) or training (False). Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: the preprocessed DataFrame with the new 'dev_avg' column\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert Developer column to string\n",
        "    _df['Developer'] = _df['Developer'].astype(str)\n",
        "    _df['Developer'] = _df['Developer'].str.replace(\"'\", \"\").str.strip('[]')  \n",
        "      \n",
        "    # Replace Developers with less than 2 games with 'Other'\n",
        "    if not test:\n",
        "        dev_counts = _df['Developer'].value_counts()\n",
        "        other = dev_counts[dev_counts < 2].index\n",
        "        _df['Developer'] = _df['Developer'].replace(other, np.nan)\n",
        "\n",
        "    # Perform target encoding on Developer column\n",
        "    if test:\n",
        "        te = pickle.load(open('encoders/dev_te.pkl', 'rb'))\n",
        "    else:\n",
        "        te = TargetEncoder(cols=['Developer'], smoothing=4).fit(_df[['Developer']], _df['Rate'])\n",
        "        pickle.dump(te, open('encoders/dev_te.pkl', 'wb'))\n",
        "\n",
        "    _df['dev_avg'] = te.transform(_df[['Developer']])\n",
        "\n",
        "    return _df\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3eV4TBH1Hvsu"
      },
      "source": [
        "### Frequency encoding approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRZuQNQCHvsu"
      },
      "outputs": [],
      "source": [
        "def dev_preprocess_freq_enc(_df, test):\n",
        "    # Convert to string\n",
        "    _df['Developer'] = _df['Developer'].astype(str)\n",
        "    _df['Developer'] = _df['Developer'].str.replace(\"'\", \"\").str.strip('[]')\n",
        "    \n",
        "    if not test:\n",
        "        ce = CountEncoder(cols=['Developer']).fit(_df[['Developer']])\n",
        "        pickle.dump(ce, open('encoders/dev_ce.pkl', 'wb'))\n",
        "    else:\n",
        "        ce = pickle.load(open('encoders/dev_ce.pkl', 'rb'))\n",
        "    \n",
        "    _df['dev_freq'] = ce.transform(_df[['Developer']])['Developer']\n",
        "        \n",
        "    return _df\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "EkJCyNcFHvsu"
      },
      "source": [
        "## Genres preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QMQ_ypD6Hvsu"
      },
      "source": [
        "### 1. NLP approach (Bag of Words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-14T10:35:21.245168Z",
          "start_time": "2023-04-14T10:35:20.885068Z"
        },
        "id": "ZhB6EiwAHvsv"
      },
      "outputs": [],
      "source": [
        "def genres_preprocess_bow(_df, test=False):\n",
        "    # Convert the genres column to a list of strings\n",
        "    _df['Genres'] = _df['Genres'].astype(str)\n",
        "    _df['Genres'] = _df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
        "\n",
        "    # drop Games, Strategy, Entertainment from the Genres column\n",
        "    _df['Genres'] = _df['Genres'].apply(lambda x: [genre for genre in x if genre not in ['Games', 'Strategy', 'Entertainment']])\n",
        "\n",
        "    # Join the list of genres into a single string\n",
        "    genres = _df['Genres'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Create a count Vectorizer and fit it to the genres\n",
        "    count_vec = CountVectorizer()\n",
        "    bow_genres = count_vec.fit_transform(genres)\n",
        "    \n",
        "    # Save the count vectorizer to be used on the test set\n",
        "    pickle.dump(count_vec, open('encoders/count_vec_genre.pkl', 'wb'))\n",
        "\n",
        "    # Apply principal component analysis to reduce the dimensionality\n",
        "    pca = PCA(n_components=10)\n",
        "    pca_genres = pca.fit_transform(bow_genres.toarray())\n",
        "    \n",
        "    # Save the pca to be used on the test set\n",
        "    pickle.dump(pca, open('encoders/genre_pca.pkl', 'wb'))\n",
        "\n",
        "    # Add the PCA-transformed genres to the original dataframe\n",
        "    for i in range(10):\n",
        "        _df[f'genreN_{i}'] = pca_genres[:, i]\n",
        "\n",
        "    return _df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6G-6R8lGHvsv"
      },
      "source": [
        "### 2. Dummy variables approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca_dTu-8Hvs0"
      },
      "outputs": [],
      "source": [
        "def genres_preprocess_dummies(_df, test=False):\n",
        "    # Convert the genres column to a list of strings\n",
        "    _df['Genres'] = _df['Genres'].astype(str)\n",
        "    _df['Genres'] = _df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
        "    \n",
        "    # drop Games, Strategy, Entertainment from the Genres column\n",
        "    _df['Genres'] = _df['Genres'].apply(lambda x: [genre for genre in x if genre not in ['Games', 'Strategy', 'Entertainment']])\n",
        "    \n",
        "    if not test:\n",
        "        \n",
        "        # Replace genres with counts less than 100 with 'infrequent' as it would represent a very small percentage of the data (less than 2%)\n",
        "        threshold = _df.shape[0] * 0.02\n",
        "        \n",
        "        other = _df['Genres'].explode().value_counts()[_df['Genres'].explode().value_counts() < threshold].index\n",
        "        _df['Genres'] = _df['Genres'].apply(lambda x: [genre if genre not in other else 'infrequent' for genre in x])\n",
        "        \n",
        "        # Get dummy variables for the genres\n",
        "        genres = pd.get_dummies(_df['Genres'].apply(pd.Series).stack(), prefix=\"genre\", dummy_na=False).sum(level=0)\n",
        "        \n",
        "        # Save the genres dummies to be used on the test set\n",
        "        genres.to_csv('encoders/genres.csv', index=False)\n",
        "    \n",
        "    else:\n",
        "        # Load saved genres dummy variables\n",
        "        saved_dummies = pd.read_csv('encoders/genres.csv')\n",
        "\n",
        "        # Get the genres that are not in the saved dummy variables\n",
        "        other = [genre for genre in _df['Genres'].explode().unique() if genre not in saved_dummies.columns]\n",
        "\n",
        "        # Replace the genres that are not in the saved dummy variables with 'infrequent'\n",
        "        _df['Genres'] = _df['Genres'].apply(lambda x: ['infrequent' if genre in other else genre for genre in x])\n",
        "\n",
        "        # Preprocess test data using the saved dummy variables\n",
        "        genres = pd.get_dummies(_df['Genres'].apply(pd.Series).stack(), prefix=\"genre\", dummy_na=False).sum(level=0)\n",
        "        genres = genres.reindex(columns=saved_dummies.columns, fill_value=0)\n",
        "        \n",
        "        # Fill the dummy columns with 0 if nan\n",
        "        genres = genres.fillna(0)\n",
        "\n",
        "    \n",
        "    # Add the dummy variables to the original dataframe\n",
        "    _df = pd.concat([_df, genres], axis=1)\n",
        "    \n",
        "    # Fill the NaN values with 0\n",
        "    genre_cols = [col for col in _df.columns if col.startswith('genre')] # get all columns with prefix 'genre'\n",
        "    _df[genre_cols] = _df[genre_cols].fillna(0) # fill the NaN values with 0\n",
        "    \n",
        "    return _df\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "HXRGNv6yHvs2"
      },
      "source": [
        "## Languages preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3k4nxTCHvs2"
      },
      "source": [
        "### 1. NLP approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-14T10:35:38.695352Z",
          "start_time": "2023-04-14T10:35:38.599717Z"
        },
        "id": "CCjvulfAHvs2"
      },
      "outputs": [],
      "source": [
        "def lang_preprocessing_bow(_df):\n",
        "    # Convert the langs column to a list of strings\n",
        "    _df['Languages'] = _df['Languages'].astype(str)\n",
        "    _df['Languages'] = _df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
        "    \n",
        "    # drop English from the Languages column\n",
        "    _df['Languages'] = _df['Languages'].apply(lambda x: [lang for lang in x if lang not in ['EN']])\n",
        "    \n",
        "    # Join the list of langs into a single string\n",
        "    languages = _df['Languages'].apply(lambda x: ' '.join(x))\n",
        "    \n",
        "    # Create a count Vectorizer and fit it to the langs\n",
        "    count_vec = CountVectorizer()\n",
        "    bow_languages = count_vec.fit_transform(languages)\n",
        "    \n",
        "    # Save the count vectorizer to be used on the test set\n",
        "    pickle.dump(count_vec, open('encoders/count_vec_lang.pkl', 'wb'))\n",
        "    \n",
        "    # Apply principal component analysis to reduce the dimensionality\n",
        "    pca = PCA(n_components=10)\n",
        "    pca_languages = pca.fit_transform(bow_languages.toarray())\n",
        "    \n",
        "    # Save the pca to be used on the test set\n",
        "    pickle.dump(pca, open('encoders/pca_lang.pkl', 'wb'))\n",
        "    \n",
        "    # Add the PCA-transformed langs to the original dataframe\n",
        "    for i in range(len(pca_languages[0])):\n",
        "        _df[f'lang_{i}'] = pca_languages[:, i]\n",
        "        \n",
        "    return _df\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZERHEg_mHvs2"
      },
      "source": [
        "### 2. Dummy variables approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH0iEpHMHvs2"
      },
      "outputs": [],
      "source": [
        "def langs_preprocess_dummies(_df, test = False):\n",
        "    # Convert the langs column to a list of strings\n",
        "    _df['Languages'] = _df['Languages'].astype(str)\n",
        "    _df['Languages'] = _df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
        "    \n",
        "    # Create a column with the number of languages supported\n",
        "    _df['langs_count'] = _df['Languages'].apply(lambda x: len(x)) \n",
        "    \n",
        "    # drop English from the Languages column\n",
        "    _df['Languages'] = _df['Languages'].apply(lambda x: [lang for lang in x if lang not in ['EN']])\n",
        "    \n",
        "    if not test:\n",
        "        # Replace languages with counts less than 100 with 'infrequent' as it would represent a very small percentage of the data (less than 10%)\n",
        "        threshold = _df.shape[0] * 0.1\n",
        "        \n",
        "        # Replace langs with counts less than 500 with 'infrequent_langs' as it would represent a very small percentage of the data (less than 10%)\n",
        "        other = _df['Languages'].explode().value_counts()[_df['Languages'].explode().value_counts() < threshold].index\n",
        "        _df['Languages'] = _df['Languages'].apply(lambda x: [lang if lang not in other else 'infrequent' for lang in x])\n",
        "\n",
        "        # Get dummy variables for the langs\n",
        "        langs = pd.get_dummies(_df['Languages'].apply(pd.Series).stack(), prefix='lang', dummy_na=False).sum(level=0)\n",
        "\n",
        "        langs.to_csv('encoders/langs.csv', index=False)\n",
        "    else:\n",
        "        saved_dummies = pd.read_csv('encoders/langs.csv')\n",
        "\n",
        "        # Get the languages that are not in the saved dummy variables\n",
        "        other = [lang for lang in _df['Languages'].explode().unique() if lang not in saved_dummies.columns]\n",
        "\n",
        "        # Replace the languages that are not in the saved dummy variables with 'infrequent'\n",
        "        _df['Languages'] = _df['Languages'].apply(lambda x: ['infrequent' if lang in other else lang for lang in x])\n",
        "\n",
        "        # Preprocess test data using the saved dummy variables\n",
        "        langs = pd.get_dummies(_df['Languages'].apply(pd.Series).stack(), prefix=\"lang\", dummy_na=False).sum(level=0)\n",
        "        langs = langs.reindex(columns=saved_dummies.columns, fill_value=0)\n",
        "\n",
        "        # Fill the dummy columns with 0 if nan\n",
        "        langs = langs.fillna(0)\n",
        "\n",
        "    # Add the dummy variables to the original dataframe\n",
        "    _df = pd.concat([_df, langs], axis=1)\n",
        "\n",
        "    # Fill NaN with 0\n",
        "    lang_cols = [col for col in _df.columns if col.startswith('lang')] # get all columns with prefix 'lang'\n",
        "    _df[lang_cols] = _df[lang_cols].fillna(0) # fill NaN with 0 for selected columns\n",
        "    \n",
        "    return _df\n",
        "    \n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Hl78fXBJHvs3"
      },
      "source": [
        "## In-app Purchases preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-14T10:35:52.935497Z",
          "start_time": "2023-04-14T10:35:52.881403Z"
        },
        "id": "FqdX_pJLHvs4"
      },
      "outputs": [],
      "source": [
        "# Free apps might skew the in-app purchases column,\n",
        "# so we might split the dataset into free and paid apps\n",
        "\n",
        "def purchases_preprocess(_df):\n",
        "    # Convert the In-app Purchases column to a list of floats\n",
        "    _df['In-app Purchases'] = _df['In-app Purchases'].astype(str)\n",
        "    _df['In-app Purchases'] = _df['In-app Purchases'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
        "\n",
        "    # Convert to float  \n",
        "    _df['In-app Purchases'] = _df['In-app Purchases'].apply(lambda x: [float(i) for i in x])\n",
        "\n",
        "    # Get the number of in-app purchases\n",
        "    _df['purchases_count'] = _df['In-app Purchases'].apply(lambda x: len(x))\n",
        "\n",
        "    # Get the lowest, highest and average purchase\n",
        "    _df['lowest_purchase'] = _df['In-app Purchases'].apply(lambda x: min(x) if len(x) > 0 else 0)\n",
        "    _df['highest_purchase'] = _df['In-app Purchases'].apply(lambda x: max(x) if len(x) > 0 else 0)\n",
        "    _df['average_purchase'] = _df['In-app Purchases'].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
        "\n",
        "    _df['lowest_purchase'] = _df['lowest_purchase'].fillna(0)\n",
        "    _df['highest_purchase'] = _df['highest_purchase'].fillna(0)\n",
        "    _df['average_purchase'] = _df['average_purchase'].fillna(0)\n",
        "    \n",
        "    return _df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "NusCKkJqHvs4"
      },
      "source": [
        "## Age Rating & Price preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-14T10:36:03.614263Z",
          "start_time": "2023-04-14T10:36:03.579637Z"
        },
        "id": "lQ5mruB9Hvs4"
      },
      "outputs": [],
      "source": [
        "def age_preprocess(_df, test=False, df_old=None):\n",
        "    # Convert to string\n",
        "    _df['Age Rating'] = _df['Age Rating'].astype(str)\n",
        "\n",
        "    # Remove the + sign\n",
        "    _df['Age Rating'] = _df['Age Rating'].str.replace('+', '')\n",
        "\n",
        "    # Convert to int\n",
        "    _df['Age Rating'] = _df['Age Rating'].astype(float)\n",
        "    \n",
        "    if test:\n",
        "        _df['Age Rating'] = _df['Age Rating'].fillna(df_old['Age Rating'].median())\n",
        "    else:\n",
        "        _df['Age Rating'] = _df['Age Rating'].fillna(_df['Age Rating'].median())\n",
        "    \n",
        "    return _df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAMLBK5SHvs5"
      },
      "outputs": [],
      "source": [
        "def price_preprocess(_df):\n",
        "    # Convert to float\n",
        "    _df['Price'] = _df['Price'].astype(float)\n",
        "\n",
        "    # fill the missing values with 0 (free)\n",
        "    _df['Price'] = _df['Price'].fillna(0)\n",
        "    \n",
        "    return _df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gethQ-I0Hvs5"
      },
      "source": [
        "## NLP preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-14T10:43:13.386866Z",
          "start_time": "2023-04-14T10:43:13.386866Z"
        },
        "id": "NX5rEEDPHvs5"
      },
      "outputs": [],
      "source": [
        "def preprocess_nlp(_df, col):\n",
        "    # Convert to string\n",
        "    _df[col] = _df[col].astype(str)\n",
        "\n",
        "    # Remove URLs and email addresses\n",
        "    _df[col] = _df[col].apply(lambda x: re.sub(r'http\\S+|www.\\S+|\\S+@\\S+', '', x))\n",
        "\n",
        "    # Remove the punctuation, numbers, and convert to lowercase\n",
        "    _df[col] = _df[col].apply(lambda x: \" \".join(re.findall(r'\\w+', x.lower())))\n",
        "\n",
        "    # Remove the stopwords\n",
        "    _df[col] = _df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n",
        "\n",
        "    # Stemming\n",
        "    st = nltk.PorterStemmer()\n",
        "    _df[col] = _df[col].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "\n",
        "    # Lemmatization\n",
        "    lem = nltk.WordNetLemmatizer()\n",
        "    _df[col] = _df[col].apply(lambda x: \" \".join([lem.lemmatize(word) for word in x.split()]))\n",
        "\n",
        "    # Remove the frequent and rare words\n",
        "    freq = pd.Series(' '.join(_df[col]).split()).value_counts()\n",
        "    common_freq = list(freq[:10].index)\n",
        "    rare_freq = list(freq[-10:].index)\n",
        "    _df[col] = _df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in common_freq+rare_freq))\n",
        "\n",
        "    # Remove the whitespaces\n",
        "    _df[col] = _df[col].apply(lambda x: \" \".join(x.strip() for x in x.split()))\n",
        "\n",
        "    # Replace NaN values with empty string\n",
        "    _df[col] = _df[col].fillna('')\n",
        "\n",
        "    # Convert text data to bag-of-words representation\n",
        "    vectorizer = CountVectorizer()\n",
        "    BoW = vectorizer.fit_transform(_df[col])\n",
        "\n",
        "    # Apply principal component analysis to reduce the dimensionality\n",
        "    pca_ = PCA(n_components=2)\n",
        "    pca_col = pca_.fit_transform(BoW.toarray())\n",
        "    \n",
        "    # Save the vectorizer and pca for later use with the test data\n",
        "    pickle.dump(vectorizer, open(f'encoders/vectorizer_{col}.pkl', 'wb'))\n",
        "    pickle.dump(pca_, open(f'encoders/pca_{col}.pkl', 'wb'))\n",
        "\n",
        "    # Add the PCA-transformed col to the original dataframe\n",
        "    for feat in range(len(pca_col[0])):\n",
        "        _df[f'{col}_PCA_{feat}'] = pca_col[:, feat]\n",
        "        \n",
        "    return _df\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-_c08Rk-Hvs5"
      },
      "source": [
        "## Description, Name & Subtitle preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8uecIVbHvs6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_excitement_score(text, _sia):\n",
        "    \n",
        "    # compute the polarity scores for the given text\n",
        "    scores = _sia.polarity_scores(text)\n",
        "    \n",
        "    # compute the excitement score as the sum of the positive and negative polarity scores\n",
        "    excitement_score = scores['pos'] + abs(scores['neg'])\n",
        "    \n",
        "    return excitement_score\n",
        "\n",
        "# define a function to compute an attractive score from a given text\n",
        "def compute_attractive_score(text, tokenizer):\n",
        "    # define a list of keywords that might make a game attractive to users\n",
        "    attractive_keywords = ['graphics', 'gameplay', 'storyline', 'characters']\n",
        "    \n",
        "    # tokenize the text into words and count how many attractive keywords appear\n",
        "    words = tokenizer(text.lower())\n",
        "    \n",
        "    num_attractive_keywords = len([word for word in words if word in attractive_keywords])\n",
        "    \n",
        "    # compute the attractive score as the ratio of attractive keywords to total words\n",
        "    attractive_score = num_attractive_keywords / len(words) if len(words) > 0 else 0\n",
        "    \n",
        "    return attractive_score\n",
        "\n",
        "def desc_preprocess(_df, test=False):\n",
        "    _df['Description'] = _df['Description'].astype(str)\n",
        "    \n",
        "    # Create column for number of words in description\n",
        "    _df['desc_word_count'] = _df['Description'].apply(lambda x: len(x.split()))\n",
        "    \n",
        "    if not test:        \n",
        "        # load the Sentiment Intensity Analyzer model from NLTK\n",
        "        sia_desc = SentimentIntensityAnalyzer()\n",
        "        pickle.dump(sia_desc, open('encoders/sia_desc.pkl', 'wb'))\n",
        "\n",
        "        tokenizer = nltk.word_tokenize\n",
        "        pickle.dump(nltk.word_tokenize, open('encoders/desc_tokenizer.pkl', 'wb'))\n",
        "        \n",
        "    else:\n",
        "        sia_desc = pickle.load(open('encoders/sia_desc.pkl', 'rb'))\n",
        "        tokenizer = pickle.load(open('encoders/desc_tokenizer.pkl', 'rb'))\n",
        "    \n",
        "    _df['excitement_score'] = _df['Description'].apply(lambda x : compute_excitement_score(x, sia_desc))\n",
        "    _df['attractive_score'] = _df['Description'].apply(lambda x: compute_attractive_score(x, tokenizer))\n",
        "    \n",
        "    return _df\n",
        "\n",
        "def name_preprocess(_df, test=False):\n",
        "    _df['Name'] = _df['Name'].astype(str)\n",
        "    \n",
        "    # Create column for number of words in subtitle\n",
        "    _df['name_word_count'] = _df['Name'].apply(lambda x: len(str(x).split(\" \")))\n",
        "    \n",
        "    if not test:\n",
        "        sia_name = SentimentIntensityAnalyzer()\n",
        "        pickle.dump(sia_name, open('encoders/sia_name.pkl', 'wb'))\n",
        "    else:\n",
        "        sia_name = pickle.load(open('encoders/sia_name.pkl', 'rb'))\n",
        "    \n",
        "    _df['name_sia'] = _df['Name'].apply(lambda x : compute_excitement_score(x, sia_name))\n",
        "    \n",
        "    return _df\n",
        "\n",
        "def sub_preprocess(_df, test=False):\n",
        "    _df['Subtitle'] = _df['Subtitle'].astype(str)\n",
        "    \n",
        "    # Create column for number of words in subtitle\n",
        "    _df['sub_word_count'] = _df['Subtitle'].apply(lambda x: len(str(x).split(\" \")))\n",
        "    \n",
        "    if not test:\n",
        "        sia_sub = SentimentIntensityAnalyzer()\n",
        "        pickle.dump(sia_sub, open('encoders/sia_sub.pkl', 'wb'))\n",
        "    else:\n",
        "        sia_sub = pickle.load(open('encoders/sia_sub.pkl', 'rb'))\n",
        "    \n",
        "    _df['sub_sia'] = _df['Subtitle'].apply(lambda x : compute_excitement_score(x, sia_sub))\n",
        "    \n",
        "    return _df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "O_sh2eoIHvs6"
      },
      "source": [
        "## Icon preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-14T10:52:48.299695Z",
          "start_time": "2023-04-14T10:52:13.578629Z"
        },
        "id": "sHunkTDXHvs6"
      },
      "outputs": [],
      "source": [
        "def preprocess_icon(img_path):\n",
        "    \n",
        "        # Load the game icon image\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (32, 32))\n",
        "\n",
        "    # Extract color features using color histograms\n",
        "    img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "    color_features = []\n",
        "    for i in range(3):\n",
        "        hist = cv2.calcHist([img_lab], [i], None, [256], [0, 256])\n",
        "        color_features.append(hist.ravel())\n",
        "    color_features = np.concatenate(color_features)\n",
        "\n",
        "    # Extract shape features using local binary patterns\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    lbp = local_binary_pattern(gray, 8, 1, method='uniform')\n",
        "    hist_lbp, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 10), range=(0, 9))\n",
        "    edge_features = hist_lbp.astype(float)\n",
        "\n",
        "    # Combine the color and shape features into a single feature vector\n",
        "    feature_vector = np.concatenate((color_features, edge_features))\n",
        "\n",
        "    # Normalize the feature vector to have unit length\n",
        "    normalized_feature_vector = feature_vector / np.linalg.norm(feature_vector)\n",
        "    \n",
        "    return normalized_feature_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WO7-CjobHvs7"
      },
      "outputs": [],
      "source": [
        "def icons_preprocess(_df, test=False):\n",
        "\n",
        "    # Create a list to store the feature vectors\n",
        "    icon_features = []\n",
        "\n",
        "    _df['Icon URL'] = _df['Icon URL'].astype(str)\n",
        "\n",
        "    # Iterate over the images and extract the features\n",
        "    for i, row in tqdm(_df.iterrows(), total=_df.shape[0]):\n",
        "        feature_vec = preprocess_icon(row['Icon URL'])\n",
        "        icon_features.append((row['Icon URL'], feature_vec))\n",
        "        \n",
        "    if not test:\n",
        "        # Apply PCA to reduce the number of features\n",
        "        pca = PCA(n_components=8)\n",
        "        pca.fit([f[1] for f in icon_features])\n",
        "\n",
        "        # Save the pca instance for later use\n",
        "        pickle.dump(pca, open('encoders/icon_pca.pkl', 'wb'))\n",
        "    \n",
        "    else:\n",
        "        pca = pickle.load(open('encoders/icon_pca.pkl', 'rb'))\n",
        "    \n",
        "    reduced_features = pca.transform([f[1] for f in icon_features])\n",
        "\n",
        "    # Convert the reduced features to a dataframe\n",
        "    icon_features_df = pd.DataFrame(reduced_features, columns=[f'icon_pca_{i}' for i in range(8)])\n",
        "    \n",
        "    # Merge the icon features with the original dataframe on the icon URL\n",
        "    _df = _df.merge(icon_features_df, on='Icon URL', how='left')\n",
        "    \n",
        "    return _df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "91gYqImm_X3n"
      },
      "source": [
        "## Rate Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhjqHDEP_WXt"
      },
      "outputs": [],
      "source": [
        "def rate_preprocess(_df):\n",
        "  # Define mapping dictionary\n",
        "  mapping = {\n",
        "    'Low': 0, \n",
        "    'Intermediate': 1,\n",
        "    'High': 2}\n",
        "\n",
        "  # Apply mapping to column\n",
        "  _df['Rate'] = _df['Rate'].apply(lambda x: mapping[x])\n",
        "  \n",
        "  return _df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KFf6bh1WHvs7"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKXKoW2eHvs7"
      },
      "outputs": [],
      "source": [
        "def preprocess_pipeline(_df, test=False):\n",
        "    _df = _df.drop(['Primary Genre', 'ID', 'URL'], axis=1)\n",
        "    \n",
        "    if test:\n",
        "        df_old = pd.read_csv('preprocessed_data.csv') \n",
        "    else:\n",
        "        df_old = None\n",
        "          \n",
        "    _df = rate_preprocess(_df)\n",
        "    _df = reviews_preprocess(_df, test, df_old)\n",
        "    _df = date_preprocess(_df, test, df_old)\n",
        "    \n",
        "    _df = dev_preprocess_freq_enc(_df, test)\n",
        "    _df = dev_preprocess_target_enc(_df, test)\n",
        "    \n",
        "    _df = genres_preprocess_dummies(_df, test)\n",
        "    _df = langs_preprocess_dummies(_df, test)\n",
        "    \n",
        "    _df = purchases_preprocess(_df)\n",
        "    \n",
        "    _df = age_preprocess(_df, test, df_old)\n",
        "    _df = price_preprocess(_df)\n",
        "    \n",
        "    _df = name_preprocess(_df, test)\n",
        "    _df = sub_preprocess(_df, test)\n",
        "    _df = desc_preprocess(_df, test)\n",
        "    \n",
        "    # _df = icons_preprocess(_df, test)\n",
        "\n",
        "    \n",
        "    _df = _df.drop(['Developer',\n",
        "              'Genres',\n",
        "              'Languages',\n",
        "              'Reviews',\n",
        "              'In-app Purchases',\n",
        "              'Description',\n",
        "              'Subtitle',\n",
        "              'Name', \n",
        "              'Icon URL'], axis=1)\n",
        "    \n",
        "    if not test:\n",
        "        _df.to_csv('preprocessed_data.csv', index=False)\n",
        "        \n",
        "    # Columns needed for the testing phase but nor for the prediction model anymore\n",
        "    _df = _df.drop(['Original Release Date',\n",
        "              'Current Version Release Date'], axis=1)\n",
        "    \n",
        "    return _df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def analyze_column(dataframe, column_name, target_column=None):\n",
        "    # Check if the column exists in the dataframe\n",
        "    if column_name not in dataframe.columns:\n",
        "        return \"Column does not exist in the dataframe\"\n",
        "    \n",
        "    # Check if the target column exists in the dataframe\n",
        "    if target_column is not None and target_column not in dataframe.columns:\n",
        "        return \"Target column does not exist in the dataframe\"\n",
        "    \n",
        "    column_data = dataframe[column_name]\n",
        "    \n",
        "    # Handle missing values\n",
        "    if column_data.isnull().sum() > 0:\n",
        "        # If there are missing values, drop them or impute them based on the nature of your problem\n",
        "        column_data = column_data.dropna()\n",
        "    \n",
        "    # Check the datatype of the column\n",
        "    column_dtype = column_data.dtype\n",
        "    \n",
        "    if column_dtype == 'object':\n",
        "        # For categorical data\n",
        "        \n",
        "        # Calculate the frequency distribution\n",
        "        freq_table = pd.crosstab(index=column_data, columns='count')\n",
        "        \n",
        "        # Plot the frequency distribution\n",
        "        plt.figure(figsize=(10,5))\n",
        "        sns.countplot(x=column_name, data=dataframe)\n",
        "        plt.title(f\"{column_name} Frequency Distribution\")\n",
        "        plt.xlabel(column_name)\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.show()\n",
        "        \n",
        "        if target_column is not None:\n",
        "            # Calculate the mean target value for each category\n",
        "            target_mean = dataframe.groupby(column_name)[target_column].mean()\n",
        "            \n",
        "            # Plot the mean target value for each category\n",
        "            plt.figure(figsize=(10,5))\n",
        "            sns.barplot(x=target_mean.index, y=target_mean.values)\n",
        "            plt.title(f\"Mean {target_column} for each {column_name}\")\n",
        "            plt.xlabel(column_name)\n",
        "            plt.ylabel(f\"Mean {target_column}\")\n",
        "            plt.show()\n",
        "        \n",
        "    elif np.issubdtype(column_dtype, np.number):\n",
        "        # For numerical data\n",
        "        \n",
        "        # Check for outliers using box plot\n",
        "        plt.figure(figsize=(10,5))\n",
        "        sns.boxplot(x=column_data)\n",
        "        plt.title(f\"{column_name} Boxplot\")\n",
        "        plt.xlabel(column_name)\n",
        "        plt.show()\n",
        "        \n",
        "        # Calculate the summary statistics\n",
        "        summary_statistics = column_data.describe()\n",
        "        print(summary_statistics)\n",
        "        \n",
        "        if target_column is not None:\n",
        "            # Plot the scatter plot between the column and the target variable\n",
        "            plt.figure(figsize=(10,5))\n",
        "            sns.scatterplot(x=column_data, y=dataframe[target_column])\n",
        "            plt.title(f\"{column_name} vs {target_column}\")\n",
        "            plt.xlabel(column_name)\n",
        "            plt.ylabel(target_column)\n",
        "            plt.show()\n",
        "            \n",
        "    else:\n",
        "        return \"Data type not recognized for the column\"\n",
        "\n",
        "\n",
        "df_analysis = preprocess_pipeline(df_origin, test=False)\n",
        "\n",
        "for col in df_analysis.columns:\n",
        "    print(analyze_column(df_analysis, col, 'Rate'))\n",
        "    print('\\n')\n",
        "    print('-----------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "ZwCKMXT1Hvs8",
        "outputId": "56c9645c-2255-4e2f-87f6-97decd6b6b9b"
      },
      "outputs": [],
      "source": [
        "# drop the rows with no reviews\n",
        "# df_origin = df_origin.dropna(subset=['Reviews'])\n",
        "\n",
        "df, df_test = train_test_split(df_origin, test_size=0.2, random_state=42)\n",
        "\n",
        "df = preprocess_pipeline(df)\n",
        "df_test = preprocess_pipeline(df_test, test=True)\n",
        "\n",
        "df_x, df_y = df.drop('Rate', axis=1), df['Rate']\n",
        "df_test_x, df_test_y = df_test.drop('Rate', axis=1), df_test['Rate']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OPW7EDf-Hvs8"
      },
      "source": [
        "## Scaling and Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scChLmTBHvs8"
      },
      "outputs": [],
      "source": [
        "def plot_scores(mse_train, r2_train, mse_test, r2_test):\n",
        "    # Calculate the differences between the training and testing scores\n",
        "    mse_diff =  abs(mse_train - mse_test)\n",
        "    r2_diff = abs(r2_train - r2_test)\n",
        "\n",
        "    # Determine whether each score is better or worse than the other\n",
        "    mse_color = 'green' if mse_diff < 0.1 else 'red'\n",
        "    r2_color = 'green' if r2_diff < 0.1 else 'red'\n",
        "    \n",
        "    # Create a bar plot of the scores with colors based on the value\n",
        "    fig = go.Figure(data=[\n",
        "        go.Bar(name='MSE Train', x=['MSE'], y=[mse_train], marker_color='navy'),\n",
        "        go.Bar(name='MSE Test', x=['MSE'], y=[mse_test], marker_color='blue'),\n",
        "        go.Bar(name='MSE Difference', x=['MSE'], y=[mse_diff], marker_color=mse_color),\n",
        "        go.Bar(name='R2 Train', x=['R2'], y=[r2_train], marker_color='purple'),\n",
        "        go.Bar(name='R2 Test', x=['R2'], y=[r2_test], marker_color='pink'),\n",
        "        go.Bar(name='R2 Difference', x=['R2'], y=[r2_diff], marker_color=r2_color)\n",
        "    ])\n",
        "\n",
        "    # Add labels and title\n",
        "    fig.update_layout(title='Model Performance', xaxis_title='Score Type', yaxis_title='Score')\n",
        "\n",
        "    # Show the plot\n",
        "    fig.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YW1zAhY8Hvs9"
      },
      "source": [
        "### Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPuIVP0lHvs9"
      },
      "outputs": [],
      "source": [
        "# Scale the features\n",
        "\n",
        "def scale_data_std(_df, test=False):\n",
        "    cols = _df.columns\n",
        "    \n",
        "    if not test:\n",
        "        scaler = StandardScaler()\n",
        "        _df = scaler.fit_transform(_df)\n",
        "        \n",
        "        # Save the scaler\n",
        "        pickle.dump(scaler, open('scalers/std_scaler.pkl', 'wb'))\n",
        "    else:\n",
        "        scaler = pickle.load(open('scalers/std_scaler.pkl', 'rb'))\n",
        "        _df = scaler.transform(_df)\n",
        "    \n",
        "    _df = pd.DataFrame(_df, columns=cols)\n",
        "    return _df\n",
        "\n",
        "def scale_data_minmax(_df, test=False):\n",
        "    cols = _df.columns\n",
        "    \n",
        "    if not test:\n",
        "        scaler = MinMaxScaler()\n",
        "        _df = scaler.fit_transform(_df)\n",
        "        \n",
        "        # Save the scaler\n",
        "        pickle.dump(scaler, open('scalers/minmax_scaler.pkl', 'wb'))\n",
        "    \n",
        "    else:\n",
        "        scaler = pickle.load(open('scalers/minmax_scaler.pkl', 'rb'))\n",
        "        _df = scaler.transform(_df)\n",
        "    \n",
        "    _df = pd.DataFrame(_df, columns=cols)\n",
        "    return _df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 848
        },
        "id": "55Cjgou9Hvs9",
        "outputId": "17452e35-8c9f-41b9-b2b0-c5cb885dedfe"
      },
      "outputs": [],
      "source": [
        "# plot correlation matrix between features and target\n",
        "\n",
        "corr = df_x.corrwith(df_y)\n",
        "corr = corr.sort_values(ascending=False)\n",
        "\n",
        "corr = corr[abs(corr) > 0.1]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.barplot(x=corr.values, y=corr.index)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG4yGb0YHvs-"
      },
      "outputs": [],
      "source": [
        "def select_features(_df_x, _df_y, test=False):\n",
        "    if not test:\n",
        "        selector = SelectKBest(f_classif, k=8)\n",
        "        selector.fit(_df_x, _df_y)\n",
        "        \n",
        "        # Save the selector\n",
        "        pickle.dump(selector, open('encoders/selector.pkl', 'wb'))\n",
        "        \n",
        "    else:\n",
        "        selector = pickle.load(open('encoders/selector.pkl', 'rb'))\n",
        "        \n",
        "    _df_x = selector.transform(_df_x)\n",
        "    \n",
        "    return _df_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUir_MN8Hvs-"
      },
      "outputs": [],
      "source": [
        "# Scale the features\n",
        "df_x_scaled = scale_data_std(df_x)\n",
        "df_test_x_scaled = scale_data_std(df_test_x, test=True)\n",
        "\n",
        "# Select the features\n",
        "df_x_select = select_features(df_x_scaled, df_y)\n",
        "df_test_x_select = select_features(df_test_x_scaled, df_test_y, test=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjdXar7QHvs-",
        "outputId": "50cdc17a-30b9-44e2-d05a-69e35f01eecc"
      },
      "outputs": [],
      "source": [
        "selector = pickle.load(open('encoders/selector.pkl', 'rb'))\n",
        "# Print the selected features\n",
        "for i in range(len(selector.get_support())): \n",
        "    if selector.get_support()[i]:\n",
        "        print(df_x.columns[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Define the number of folds for cross-validation\n",
        "n_folds = 5\n",
        "\n",
        "# Load your data and target variables into X and y arrays\n",
        "# df_origin = df_origin.dropna(subset=['Reviews'])\n",
        "X, y = df_origin.drop('Rate', axis=1), df_origin['Rate']\n",
        "\n",
        "# Instantiate the CatBoost model object\n",
        "model = SVC(kernel='rbf', C=10.0, gamma='scale', random_state=42)\n",
        "\n",
        "# Create a K-fold cross-validator object\n",
        "kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Use a loop to train and test the model on each fold of the data\n",
        "fold_scores = []\n",
        "for train_index, test_index in tqdm(kf.split(X, y), total=n_folds):\n",
        "    # Get the training and testing sets for this fold\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    \n",
        "    # Join the training data together\n",
        "    X_train = pd.concat([X_train, y_train], axis=1)\n",
        "    X_train = preprocess_pipeline(X_train)\n",
        "    X_train = X_train.drop('Rate', axis=1)\n",
        "    X_train = scale_data_std(X_train)\n",
        "    X_train = select_features(X_train, y_train)\n",
        "    \n",
        "    # Join the testing data together\n",
        "    X_test = pd.concat([X_test, y_test], axis=1)\n",
        "    X_test = preprocess_pipeline(X_test, test=True)\n",
        "    X_test = X_test.drop('Rate', axis=1)\n",
        "    X_test = scale_data_std(X_test, test=True)\n",
        "    X_test = select_features(X_test, y_test, test=True)    \n",
        "    \n",
        "    # Fit the model to the training data\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate the model on the testing data and store the results\n",
        "    fold_score = model.score(X_test, y_test)\n",
        "    print(fold_score)\n",
        "    print(confusion_matrix(y_test, model.predict(X_test)))\n",
        "    print('----------------')\n",
        "    fold_scores.append(fold_score)\n",
        "\n",
        "# Calculate the average score across all folds\n",
        "avg_score = np.mean(fold_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(avg_score)\n",
        "fold_scores"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XoNTBxJ1HvtH"
      },
      "source": [
        "## Lazy Predict "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x14Faa_f856r",
        "outputId": "a722617a-49ca-4b5c-832d-f04293e1c56e"
      },
      "outputs": [],
      "source": [
        "import lazypredict\n",
        "from lazypredict.Supervised import LazyClassifier\n",
        "\n",
        "reg = LazyClassifier(ignore_warnings=True, custom_metric=None)\n",
        "models, predictions = reg.fit(df_x_select, df_test_x_select, df_y, df_test_y)\n",
        "\n",
        "models = models.sort_values('Accuracy', ascending=False)\n",
        "models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train SVC model\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "svc = SVC(kernel='rbf', C=1.0, gamma='auto', random_state=42)\n",
        "svc.fit(df_x_select, df_y)\n",
        "\n",
        "# Print the training accuracy score\n",
        "svc_train_score = svc.score(df_x_select, df_y)\n",
        "print('SVC Train Accuracy Score: {0}'.format(svc_train_score))\n",
        "\n",
        "# Save the model\n",
        "pickle.dump(svc, open('models/svc.pkl', 'wb'))\n",
        "\n",
        "# Make predictions\n",
        "svc_pred = svc.predict(df_test_x_select)\n",
        "\n",
        "# Calculate the accuracy score\n",
        "svc_score = accuracy_score(df_test_y, svc_pred)\n",
        "print('SVC Accuracy Score: {0}'.format(svc_score))\n",
        "\n",
        "# Print the confusion matrix\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(df_test_y, svc_pred))\n",
        "\n",
        "# Print the classification report\n",
        "print('Classification Report')\n",
        "print(classification_report(df_test_y, svc_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Train Random Forest model\n",
        "rf = RandomForestClassifier(max_depth=3, random_state=42)\n",
        "rf.fit(df_x_select, df_y)\n",
        "\n",
        "# Print the training accuracy score\n",
        "rf_train_score = rf.score(df_x_select, df_y)\n",
        "print('Random Forest Train Accuracy Score: {0}'.format(rf_train_score))\n",
        "\n",
        "# Save the model\n",
        "pickle.dump(rf, open('models/rf.pkl', 'wb'))\n",
        "\n",
        "# Make predictions\n",
        "rf_pred = rf.predict(df_test_x_select)\n",
        "\n",
        "# Calculate the accuracy score\n",
        "rf_score = accuracy_score(df_test_y, rf_pred)\n",
        "print('Random Forest Accuracy Score: {0}'.format(rf_score))\n",
        "\n",
        "# Print the confusion matrix\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(df_test_y, rf_pred))\n",
        "\n",
        "# Print the classification report\n",
        "print('Classification Report')\n",
        "print(classification_report(df_test_y, rf_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Train XGBoost model\n",
        "xgb = XGBClassifier(max_depth=3, random_state=42, n_estimators=200, learning_rate=0.01, colsample_bytree=0.3, subsample=0.7, reg_alpha=0.1, reg_lambda=10.0)\n",
        "xgb.fit(df_x_select, df_y)\n",
        "\n",
        "# Print the training accuracy score\n",
        "xgb_train_score = xgb.score(df_x_select, df_y)\n",
        "print('XGBoost Train Accuracy Score: {0}'.format(xgb_train_score))\n",
        "\n",
        "# Save the model\n",
        "pickle.dump(xgb, open('models/xgb.pkl', 'wb'))\n",
        "\n",
        "# Make predictions\n",
        "xgb_pred = xgb.predict(df_test_x_select)\n",
        "\n",
        "# Calculate the accuracy score\n",
        "xgb_score = accuracy_score(df_test_y, xgb_pred)\n",
        "print('XGBoost Accuracy Score: {0}'.format(xgb_score))\n",
        "\n",
        "# Print the confusion matrix\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(df_test_y, xgb_pred))\n",
        "\n",
        "# Print the classification report\n",
        "print('Classification Report')\n",
        "print(classification_report(df_test_y, xgb_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# Train CatBoost model\n",
        "cat = CatBoostClassifier(verbose=250, max_depth=3, random_state=42)\n",
        "cat.fit(df_x_select, df_y)\n",
        "\n",
        "# Print the training accuracy score\n",
        "cat_train_score = cat.score(df_x_select, df_y)\n",
        "print('CatBoost Train Accuracy Score: {0}'.format(cat_train_score))\n",
        "\n",
        "# Save the model\n",
        "pickle.dump(cat, open('models/cat.pkl', 'wb'))\n",
        "\n",
        "# Make predictions\n",
        "cat_pred = cat.predict(df_test_x_select)\n",
        "\n",
        "# Calculate the accuracy score\n",
        "cat_score = accuracy_score(df_test_y, cat_pred)\n",
        "print('CatBoost Accuracy Score: {0}'.format(cat_score))\n",
        "\n",
        "# Print the confusion matrix\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(df_test_y, cat_pred))\n",
        "\n",
        "# Print the classification report\n",
        "print('Classification Report')\n",
        "print(classification_report(df_test_y, cat_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "57bc2b6ce032b5f0e93daa91901b7ea38a856826ef43aa9e95b6d3999f5310df"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
