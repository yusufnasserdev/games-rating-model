{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "er0QnrBCHvsa"
      },
      "source": [
        "# Regression"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dYhdkduKHvsb"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-14T14:19:38.839351Z",
          "start_time": "2023-04-14T14:19:38.815268Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSZ7Y2KYHvsc",
        "outputId": "78b702ab-bc18-45d2-a027-ba47fd38d7bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import plotly.graph_objs as go\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "import cv2\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from sklearn.impute import SimpleImputer, MissingIndicator, KNNImputer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif, f_regression, mutual_info_regression\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from skimage.feature import local_binary_pattern\n",
        "\n",
        "\n",
        "from lazypredict.Supervised import LazyRegressor, LazyClassifier\n",
        "from category_encoders import TargetEncoder,  CountEncoder\n",
        "\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from scipy import stats\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from scipy import stats\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MHminn2LHvsf"
      },
      "outputs": [],
      "source": [
        "# Download nltk packages if not already downloaded\n",
        "# nltk.download()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "hv1Te7_-Hvsg"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-14T10:33:46.390000Z",
          "start_time": "2023-04-14T10:33:46.279076Z"
        },
        "id": "GaIZuBBIHvsh"
      },
      "outputs": [],
      "source": [
        "dateparse = lambda x: datetime.strptime(x, '%d/%m/%Y')\n",
        "\n",
        "df_origin = pd.read_csv('games-regression-dataset.csv', \n",
        "                        parse_dates=['Original Release Date' , 'Current Version Release Date'], \n",
        "                        date_parser=dateparse)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eMfXwc88Hvsh"
      },
      "source": [
        "## Data Gathering"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "k8YwULxZHvsh"
      },
      "source": [
        "### Download the icons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9LthNw9Hvsi",
        "outputId": "75a23749-29bc-4e64-9ad2-6fb715c883f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5214/5214 [00:00<00:00, 9127.46it/s]\n"
          ]
        }
      ],
      "source": [
        "# Convert to string\n",
        "df_origin['Icon URL'] = df_origin['Icon URL'].astype(str)\n",
        "\n",
        "# Download the images\n",
        "def download_image(url, filename):\n",
        "    r = requests.get(url, stream=True)\n",
        "    if r.status_code == 200:\n",
        "        with open(filename, 'wb') as f:\n",
        "            r.raw.decode_content = True\n",
        "            shutil.copyfileobj(r.raw, f)\n",
        "\n",
        "\n",
        "# Create a folder to store the images\n",
        "if not os.path.exists('icons'):\n",
        "    os.makedirs('icons')\n",
        "\n",
        "# Download the images if they don't exist\n",
        "for i, row in tqdm(df_origin.iterrows(), total=df_origin.shape[0]):\n",
        "    if not os.path.exists('icons/' + str(i) + '.png'):\n",
        "        download_image(row['Icon URL'], 'icons/' + str(i) + '.png')\n",
        "        \n",
        "# Replace the URL with the icon filename which is the index of the row\n",
        "df_origin['Icon URL'] = df_origin.apply(lambda row : f'icons/{row.name}.png', axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vakpxzSJHvsk"
      },
      "source": [
        "### Download the reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bYthqi66Hvsk"
      },
      "outputs": [],
      "source": [
        "def web_scrapping():\n",
        "    data = pd.DataFrame(columns = [\"ID\",\"Reviews\"])\n",
        "    # Read CSV file\n",
        "    # with open('games-regression-dataset.csv', newline='') as csvfile:\n",
        "    with open('games-regression-dataset.csv', newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)  # Skip header row\n",
        "        for row in reader:\n",
        "            url = row[0]  # URL is in first column\n",
        "            filename = 'Reviews/'+os.path.basename(url)  # Extract filename from URL\n",
        "            url +=  \"?see-all=reviews\"\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:  # Check if request was successful\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                blocks = soup.findAll(\"blockquote\")\n",
        "                review_list = []\n",
        "                for blockquote in blocks:\n",
        "                    review = blockquote.find('p').text\n",
        "                    review_list.append(review)\n",
        "                if len(review_list)!=0:\n",
        "                    filename = re.sub(r'[^\\d]+', '', filename)\n",
        "                    new_row = {'ID': filename,\"Reviews\": review_list}\n",
        "                    data = data._append(new_row, ignore_index=True)\n",
        "                    \n",
        "    data.to_csv('Reviews.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8b_eA5aWHvsl"
      },
      "outputs": [],
      "source": [
        "def reviews_splitting(data):\n",
        "\n",
        "    for i in range (len(data)):\n",
        "        data.at[i, 'Reviews'] = data.at[i, \"Reviews\"].split(\"',\")\n",
        "        data.at[i,\"ID\"] =data.at[i,\"ID\"]\n",
        "        \n",
        "    data = data.explode('Reviews')\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O9WGY_OLHvsl"
      },
      "outputs": [],
      "source": [
        "def reviews_cleaning(data):\n",
        "  # Convert text to lowercase\n",
        "  data['Reviews'] = data['Reviews'].apply(lambda x: str(x).lower())\n",
        "\n",
        "  # Replace newline characters with an empty string\n",
        "  data['Reviews'] = data['Reviews'].apply(lambda x: re.sub(r'\\\\n', ' ', x))\n",
        "\n",
        "  # Remove black squares\n",
        "  data['Reviews'] = data['Reviews'].apply(lambda x: re.sub(r'\\\\u25a0', '', x))\n",
        "\n",
        "  # Remove special characters and punctuations\n",
        "  data['Reviews'] = data['Reviews'].apply(lambda x: re.sub(r'[^\\w\\s]+', '', x))\n",
        "\n",
        "  # Remove numbers\n",
        "  data['Reviews'] = data['Reviews'].apply(lambda x: \" \".join([word for word in x.split() if not any(char.isdigit() for char in word)]))\n",
        "\n",
        "  # Remove extra whitespaces\n",
        "  data['Reviews'] = data['Reviews'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
        "\n",
        "  # Remove stop words\n",
        "  data['Reviews'] = data['Reviews'].apply(lambda x: \" \".join([word for word in x.lower().split() if word not in stop_words]))\n",
        "\n",
        "  # Remove empty strings\n",
        "  data = data[data['Reviews'].apply(lambda x: len(x)>0)]\n",
        "  \n",
        "  # Group by ID\n",
        "  data = data.groupby('ID')['Reviews'].apply(list).reset_index()\n",
        "  \n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ILz0DEpUHvsm"
      },
      "outputs": [],
      "source": [
        "# Web scrapping to get the html of the reviews, Only run once\n",
        "# web_scrapping()\n",
        "\n",
        "df_reviews = pd.read_csv('Reviews.csv')\n",
        "\n",
        "df_reviews = reviews_splitting(df_reviews)\n",
        "\n",
        "df_reviews = reviews_cleaning(df_reviews)\n",
        "\n",
        "## Merge The Sentiment with the original dataset\n",
        "df_origin = df_origin.merge(df_reviews, on='ID', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV_0Bqj8Hvsm",
        "outputId": "f552d942-f31a-4165-fb97-5e972234d9d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5214, 19)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_origin.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "6LHKjpiyHvsn"
      },
      "source": [
        "## Data Exploration"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PIk-60WFHvso"
      },
      "source": [
        "### General information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-13T16:07:00.183708Z",
          "start_time": "2023-04-13T16:07:00.127616Z"
        },
        "id": "udUwpb-hHvso"
      },
      "outputs": [],
      "source": [
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-13T14:33:05.138536Z",
          "start_time": "2023-04-13T14:33:05.138536Z"
        },
        "id": "_hCjHrwzHvso"
      },
      "outputs": [],
      "source": [
        "# df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-13T14:33:07.893814Z",
          "start_time": "2023-04-13T14:33:07.893814Z"
        },
        "id": "J_hY99liHvsp"
      },
      "outputs": [],
      "source": [
        "# df.isnull().sum()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oX0TOxVhHvsp"
      },
      "source": [
        "### Genres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-13T14:33:21.139604Z",
          "start_time": "2023-04-13T14:33:21.139604Z"
        },
        "id": "_0D6Q5nyHvsp"
      },
      "outputs": [],
      "source": [
        "def genres_analysis(_df):\n",
        "    _df['Genres'] = _df['Genres'].astype(str)\n",
        "    _df['Genres'] = _df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
        "\n",
        "    genre_counts = _df.explode('Genres').groupby('Genres').size().sort_values(ascending=False)\n",
        "    print(genre_counts)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bnQiH-nyHvsp"
      },
      "source": [
        "### Developer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-13T14:33:43.637371Z",
          "start_time": "2023-04-13T14:33:43.637371Z"
        },
        "id": "A-iyxVnWHvsq"
      },
      "outputs": [],
      "source": [
        "def dev_analysis(_df):\n",
        "\n",
        "    print(_df['Developer'].value_counts())\n",
        "    \n",
        "    # print the number developers with more than 1 game\n",
        "    print(len(_df['Developer'].value_counts()[_df['Developer'].value_counts() > 1]))\n",
        "\n",
        "    print(_df['Developer'].unique().size)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pLpXI91_Hvsq"
      },
      "source": [
        "### Dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gwLFQHQSHvsq"
      },
      "outputs": [],
      "source": [
        "def date_analysis(_df):\n",
        "    # Plot the distribution of the date columns\n",
        "\n",
        "    fig, ax = plt.subplots(5, 2, figsize=(20, 20))\n",
        "\n",
        "    # df = date_preprocessing(df)\n",
        "\n",
        "    # game_age distribution\n",
        "    sns.histplot(_df['game_age'], ax=ax[0, 0])\n",
        "    sns.boxplot(_df['game_age'], ax=ax[0, 1], orient='h')\n",
        "\n",
        "    # last_update distribution\n",
        "    sns.histplot(_df['last_update'], ax=ax[1, 0])\n",
        "    sns.boxplot(_df['last_update'], ax=ax[1, 1], orient='h')\n",
        "\n",
        "    # Original Release Date distribution\n",
        "    sns.histplot(_df['Original Release Date'], ax=ax[2, 0])\n",
        "    sns.boxplot(_df['Original Release Date'], ax=ax[2, 1], orient='h')\n",
        "\n",
        "    # Current Version Release Date distribution\n",
        "    sns.histplot(_df['Current Version Release Date'], ax=ax[3, 0])\n",
        "    sns.boxplot(_df['Current Version Release Date'], ax=ax[3, 1], orient='h')\n",
        "\n",
        "    # maintaning_period distribution\n",
        "    sns.histplot(_df['maintaning_period'], ax=ax[4, 0])\n",
        "    sns.boxplot(_df['maintaning_period'], ax=ax[4, 1], orient='h')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Date outliers are legitimate data points that are worth keeping, they are not errors nor anomalies"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jJxiCYoXHvsr"
      },
      "source": [
        "### Languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-13T14:33:49.943309Z",
          "start_time": "2023-04-13T14:33:49.943309Z"
        },
        "id": "XULWiDzHHvsr"
      },
      "outputs": [],
      "source": [
        "def lang_analysis(_df):\n",
        "    _df['Languages'] = _df['Languages'].astype(str)\n",
        "    _df['Languages'] = _df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
        "\n",
        "    langs_counts = _df.explode('Languages').groupby('Languages').size().sort_values(ascending=False)\n",
        "    print(langs_counts[1:30])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "h-cVy1UEHvss"
      },
      "outputs": [],
      "source": [
        "# df.hist(figsize=(15, 15))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### General Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def analyze_column(dataframe, column_name, target_column=None):\n",
        "    # Check if the column exists in the dataframe\n",
        "    if column_name not in dataframe.columns:\n",
        "        return \"Column does not exist in the dataframe\"\n",
        "    \n",
        "    # Check if the target column exists in the dataframe\n",
        "    if target_column is not None and target_column not in dataframe.columns:\n",
        "        return \"Target column does not exist in the dataframe\"\n",
        "    \n",
        "    column_data = dataframe[column_name]\n",
        "    \n",
        "    # Handle missing values\n",
        "    if column_data.isnull().sum() > 0:\n",
        "        # If there are missing values, drop them or impute them based on the nature of your problem\n",
        "        column_data = column_data.dropna()\n",
        "    \n",
        "    # Check the datatype of the column\n",
        "    column_dtype = column_data.dtype\n",
        "    \n",
        "    if column_dtype == 'object':\n",
        "        # For categorical data\n",
        "        \n",
        "        # Calculate the frequency distribution\n",
        "        freq_table = pd.crosstab(index=column_data, columns='count')\n",
        "        \n",
        "        # Plot the frequency distribution\n",
        "        plt.figure(figsize=(10,5))\n",
        "        sns.countplot(x=column_name, data=dataframe)\n",
        "        plt.title(f\"{column_name} Frequency Distribution\")\n",
        "        plt.xlabel(column_name)\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.show()\n",
        "        \n",
        "        if target_column is not None:\n",
        "            # Calculate the mean target value for each category\n",
        "            target_mean = dataframe.groupby(column_name)[target_column].mean()\n",
        "            \n",
        "            # Plot the mean target value for each category\n",
        "            plt.figure(figsize=(10,5))\n",
        "            sns.barplot(x=target_mean.index, y=target_mean.values)\n",
        "            plt.title(f\"Mean {target_column} for each {column_name}\")\n",
        "            plt.xlabel(column_name)\n",
        "            plt.ylabel(f\"Mean {target_column}\")\n",
        "            plt.show()\n",
        "        \n",
        "    elif np.issubdtype(column_dtype, np.number):\n",
        "        # For numerical data\n",
        "        \n",
        "        # Check for outliers using box plot\n",
        "        plt.figure(figsize=(10,5))\n",
        "        sns.boxplot(x=column_data)\n",
        "        plt.title(f\"{column_name} Boxplot\")\n",
        "        plt.xlabel(column_name)\n",
        "        plt.show()\n",
        "        \n",
        "        # Calculate the summary statistics\n",
        "        summary_statistics = column_data.describe()\n",
        "        print(summary_statistics)\n",
        "        \n",
        "        if target_column is not None:\n",
        "            # Plot the scatter plot between the column and the target variable\n",
        "            plt.figure(figsize=(10,5))\n",
        "            sns.scatterplot(x=column_data, y=dataframe[target_column])\n",
        "            plt.title(f\"{column_name} vs {target_column}\")\n",
        "            plt.xlabel(column_name)\n",
        "            plt.ylabel(target_column)\n",
        "            plt.show()\n",
        "            \n",
        "    else:\n",
        "        return \"Data type not recognized for the column\"\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ov19V8buHvss"
      },
      "source": [
        "## Dates preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-14T10:36:05.737187Z",
          "start_time": "2023-04-14T10:36:05.699973Z"
        },
        "id": "OiCX3NwvHvss"
      },
      "outputs": [],
      "source": [
        "def date_preprocess(_df, test=False):\n",
        "    # Convert the datetime to ordinal\n",
        "    _df['Original Release Date'] = _df['Original Release Date'].apply(lambda x: x.toordinal())\n",
        "    _df['Current Version Release Date'] = _df['Current Version Release Date'].apply(lambda x: x.toordinal())\n",
        "    \n",
        "    # Impute missing values using simple imputer with median strategy\n",
        "    if test:\n",
        "        simple_imputer = pickle.load(open('imputers/date_simple.pkl', 'rb'))\n",
        "    else:\n",
        "        simple_imputer = SimpleImputer(strategy='median').fit(_df[['Original Release Date', 'Current Version Release Date']])\n",
        "        pickle.dump(simple_imputer, open('imputers/date_simple.pkl', 'wb'))\n",
        "    \n",
        "    _df[['Original Release Date', 'Current Version Release Date']] = simple_imputer.transform(_df[['Original Release Date', 'Current Version Release Date']])\n",
        "    \n",
        "    # Create a new column with the age of the game\n",
        "    _df['game_age'] = datetime.now().toordinal() - _df['Original Release Date']\n",
        "\n",
        "    # Create a new column with the time since the last update\n",
        "    _df['last_update'] = datetime.now().toordinal() - _df['Current Version Release Date']\n",
        "    \n",
        "    # Create a new column with the maintaning period\n",
        "    _df['maintaning_period'] = _df['game_age'] - _df['last_update']\n",
        "\n",
        "    return _df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "p6ocFra6Hvst"
      },
      "source": [
        "## Developer preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IJvJX4g3Hvst"
      },
      "source": [
        "### Target encoding approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fQhoy2JOHvst"
      },
      "outputs": [],
      "source": [
        "def dev_preprocess_target_enc(_df, test=False):\n",
        "    \"\"\"Preprocesses and encodes the 'Developer' column using target encoding.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): DataFrame (either train or test, depending on the value of test)\n",
        "        test (bool, optional): Boolean flag indicating whether the data is for testing (True) or training (False). Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: the preprocessed DataFrame with the new 'dev_avg' column\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert Developer column to string\n",
        "    _df['Developer'] = _df['Developer'].astype(str)\n",
        "    _df['Developer'] = _df['Developer'].str.replace(\"'\", \"\").str.strip('[]')  \n",
        "      \n",
        "    # Replace Developers with less than 2 games with 'Other'\n",
        "    if not test:\n",
        "        dev_counts = _df['Developer'].value_counts()\n",
        "        other = dev_counts[dev_counts < 2].index\n",
        "        _df['Developer'] = _df['Developer'].replace(other, np.nan)\n",
        "\n",
        "    # Perform target encoding on Developer column\n",
        "    if test:\n",
        "        te = pickle.load(open('encoders/dev_te.pkl', 'rb'))\n",
        "    else:\n",
        "        te = TargetEncoder(cols=['Developer'], smoothing=20, handle_missing='return_nan').fit(_df[['Developer']], _df['Average User Rating'])\n",
        "        pickle.dump(te, open('encoders/dev_te.pkl', 'wb'))\n",
        "\n",
        "    _df['dev_avg'] = te.transform(_df[['Developer']])\n",
        "    \n",
        "    # Impute missing values using KNN\n",
        "    if test:\n",
        "        knn = pickle.load(open('imputers/dev_knn.pkl', 'rb'))\n",
        "    else:\n",
        "        knn = KNNImputer(n_neighbors=5).fit(_df[['dev_avg']])\n",
        "        pickle.dump(knn, open('imputers/dev_knn.pkl', 'wb'))\n",
        "    \n",
        "    _df['dev_avg'] = knn.transform(_df[['dev_avg']])\n",
        "    _df['dev_avg'] = _df['dev_avg'].astype(float)\n",
        "    \n",
        "    return _df\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3eV4TBH1Hvsu"
      },
      "source": [
        "### Frequency encoding approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qRZuQNQCHvsu"
      },
      "outputs": [],
      "source": [
        "def dev_preprocess_freq_enc(_df, test):\n",
        "    # Convert to string\n",
        "    _df['Developer'] = _df['Developer'].astype(str)\n",
        "    _df['Developer'] = _df['Developer'].str.replace(\"'\", \"\").str.strip('[]')\n",
        "    \n",
        "    if not test:\n",
        "        ce = CountEncoder(cols=['Developer']).fit(_df[['Developer']])\n",
        "        pickle.dump(ce, open('encoders/dev_ce.pkl', 'wb'))\n",
        "    else:\n",
        "        ce = pickle.load(open('encoders/dev_ce.pkl', 'rb'))\n",
        "    \n",
        "    _df['dev_freq'] = ce.transform(_df[['Developer']])['Developer']\n",
        "        \n",
        "    return _df\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "EkJCyNcFHvsu"
      },
      "source": [
        "## Genres preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QMQ_ypD6Hvsu"
      },
      "source": [
        "### 1. NLP approach (Bag of Words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-14T10:35:21.245168Z",
          "start_time": "2023-04-14T10:35:20.885068Z"
        },
        "id": "ZhB6EiwAHvsv"
      },
      "outputs": [],
      "source": [
        "def genres_preprocess_bow(_df, test=False):\n",
        "    # Convert the genres column to a list of strings\n",
        "    _df['Genres'] = _df['Genres'].astype(str)\n",
        "    _df['Genres'] = _df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
        "\n",
        "    # drop Games, Strategy, Entertainment from the Genres column\n",
        "    _df['Genres'] = _df['Genres'].apply(lambda x: [genre for genre in x if genre not in ['Games', 'Strategy', 'Entertainment']])\n",
        "\n",
        "    # Join the list of genres into a single string\n",
        "    genres = _df['Genres'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    # Create a count Vectorizer and fit it to the genres\n",
        "    count_vec = CountVectorizer()\n",
        "    bow_genres = count_vec.fit_transform(genres)\n",
        "    \n",
        "    # Save the count vectorizer to be used on the test set\n",
        "    pickle.dump(count_vec, open('encoders/count_vec_genre.pkl', 'wb'))\n",
        "\n",
        "    # Apply principal component analysis to reduce the dimensionality\n",
        "    pca = PCA(n_components=10)\n",
        "    pca_genres = pca.fit_transform(bow_genres.toarray())\n",
        "    \n",
        "    # Save the pca to be used on the test set\n",
        "    pickle.dump(pca, open('encoders/genre_pca.pkl', 'wb'))\n",
        "\n",
        "    # Add the PCA-transformed genres to the original dataframe\n",
        "    for i in range(10):\n",
        "        _df[f'genreN_{i}'] = pca_genres[:, i]\n",
        "\n",
        "    return _df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6G-6R8lGHvsv"
      },
      "source": [
        "### 2. Dummy variables approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ca_dTu-8Hvs0"
      },
      "outputs": [],
      "source": [
        "def genres_preprocess_dummies(_df, test=False):\n",
        "    # Convert the genres column to a list of strings\n",
        "    _df['Genres'] = _df['Genres'].astype(str)\n",
        "    _df['Genres'] = _df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
        "    \n",
        "    # drop Games, Strategy, Entertainment from the Genres column\n",
        "    _df['Genres'] = _df['Genres'].apply(lambda x: [genre for genre in x if genre not in ['Games', 'Strategy', 'Entertainment']])\n",
        "    \n",
        "    if not test:\n",
        "        \n",
        "        # Replace genres with counts less than 100 with 'infrequent' as it would represent a very small percentage of the data (less than 2%)\n",
        "        threshold = _df.shape[0] * 0.02\n",
        "        \n",
        "        other = _df['Genres'].explode().value_counts()[_df['Genres'].explode().value_counts() < threshold].index\n",
        "        _df['Genres'] = _df['Genres'].apply(lambda x: [genre if genre not in other else 'infrequent' for genre in x])\n",
        "        \n",
        "        # Get dummy variables for the genres\n",
        "        genres = pd.get_dummies(_df['Genres'].apply(pd.Series).stack(), prefix=\"genre\", dummy_na=False).sum(level=0)\n",
        "        \n",
        "        # Save the genres dummies to be used on the test set\n",
        "        genres.to_csv('encoders/genres.csv', index=False)\n",
        "    \n",
        "    else:\n",
        "        # Load saved genres dummy variables\n",
        "        saved_dummies = pd.read_csv('encoders/genres.csv')\n",
        "\n",
        "        # Get the genres that are not in the saved dummy variables\n",
        "        other = [genre for genre in _df['Genres'].explode().unique() if genre not in saved_dummies.columns]\n",
        "\n",
        "        # Replace the genres that are not in the saved dummy variables with 'infrequent'\n",
        "        _df['Genres'] = _df['Genres'].apply(lambda x: ['infrequent' if genre in other else genre for genre in x])\n",
        "\n",
        "        # Preprocess test data using the saved dummy variables\n",
        "        genres = pd.get_dummies(_df['Genres'].apply(pd.Series).stack(), prefix=\"genre\", dummy_na=False).sum(level=0)\n",
        "        genres = genres.reindex(columns=saved_dummies.columns, fill_value=0)\n",
        "        \n",
        "        # Fill the dummy columns with 0 if nan\n",
        "        genres = genres.fillna(0)\n",
        "\n",
        "    \n",
        "    # Add the dummy variables to the original dataframe\n",
        "    _df = pd.concat([_df, genres], axis=1)\n",
        "    \n",
        "    # Fill the NaN values with 0\n",
        "    genre_cols = [col for col in _df.columns if col.startswith('genre')] # get all columns with prefix 'genre'\n",
        "    _df[genre_cols] = _df[genre_cols].fillna(0) # fill the NaN values with 0\n",
        "    \n",
        "    return _df\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "HXRGNv6yHvs2"
      },
      "source": [
        "## Languages preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3k4nxTCHvs2"
      },
      "source": [
        "### 1. NLP approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-14T10:35:38.695352Z",
          "start_time": "2023-04-14T10:35:38.599717Z"
        },
        "id": "CCjvulfAHvs2"
      },
      "outputs": [],
      "source": [
        "def lang_preprocessing_bow(_df):\n",
        "    # Convert the langs column to a list of strings\n",
        "    _df['Languages'] = _df['Languages'].astype(str)\n",
        "    _df['Languages'] = _df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
        "    \n",
        "    # drop English from the Languages column\n",
        "    _df['Languages'] = _df['Languages'].apply(lambda x: [lang for lang in x if lang not in ['EN']])\n",
        "    \n",
        "    # Join the list of langs into a single string\n",
        "    languages = _df['Languages'].apply(lambda x: ' '.join(x))\n",
        "    \n",
        "    # Create a count Vectorizer and fit it to the langs\n",
        "    count_vec = CountVectorizer()\n",
        "    bow_languages = count_vec.fit_transform(languages)\n",
        "    \n",
        "    # Save the count vectorizer to be used on the test set\n",
        "    pickle.dump(count_vec, open('encoders/count_vec_lang.pkl', 'wb'))\n",
        "    \n",
        "    # Apply principal component analysis to reduce the dimensionality\n",
        "    pca = PCA(n_components=10)\n",
        "    pca_languages = pca.fit_transform(bow_languages.toarray())\n",
        "    \n",
        "    # Save the pca to be used on the test set\n",
        "    pickle.dump(pca, open('encoders/pca_lang.pkl', 'wb'))\n",
        "    \n",
        "    # Add the PCA-transformed langs to the original dataframe\n",
        "    for i in range(len(pca_languages[0])):\n",
        "        _df[f'lang_{i}'] = pca_languages[:, i]\n",
        "        \n",
        "    return _df\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZERHEg_mHvs2"
      },
      "source": [
        "### 2. Dummy variables approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bH0iEpHMHvs2"
      },
      "outputs": [],
      "source": [
        "def langs_preprocess_dummies(_df, test = False):\n",
        "    # Convert the langs column to a list of strings\n",
        "    _df['Languages'] = _df['Languages'].astype(str)\n",
        "    _df['Languages'] = _df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
        "    \n",
        "    # Create a column with the number of languages supported\n",
        "    _df['langs_count'] = _df['Languages'].apply(lambda x: len(x)) \n",
        "    \n",
        "    # drop English from the Languages column\n",
        "    _df['Languages'] = _df['Languages'].apply(lambda x: [lang for lang in x if lang not in ['EN']])\n",
        "    \n",
        "    if not test:\n",
        "        # Replace languages with counts less than 100 with 'infrequent' as it would represent a very small percentage of the data (less than 10%)\n",
        "        threshold = _df.shape[0] * 0.1\n",
        "        \n",
        "        # Replace langs with counts less than 500 with 'infrequent_langs' as it would represent a very small percentage of the data (less than 10%)\n",
        "        other = _df['Languages'].explode().value_counts()[_df['Languages'].explode().value_counts() < threshold].index\n",
        "        _df['Languages'] = _df['Languages'].apply(lambda x: [lang if lang not in other else 'infrequent' for lang in x])\n",
        "\n",
        "        # Get dummy variables for the langs\n",
        "        langs = pd.get_dummies(_df['Languages'].apply(pd.Series).stack(), prefix='lang', dummy_na=False).sum(level=0)\n",
        "\n",
        "        langs.to_csv('encoders/langs.csv', index=False)\n",
        "    else:\n",
        "        saved_dummies = pd.read_csv('encoders/langs.csv')\n",
        "\n",
        "        # Get the languages that are not in the saved dummy variables\n",
        "        other = [lang for lang in _df['Languages'].explode().unique() if lang not in saved_dummies.columns]\n",
        "\n",
        "        # Replace the languages that are not in the saved dummy variables with 'infrequent'\n",
        "        _df['Languages'] = _df['Languages'].apply(lambda x: ['infrequent' if lang in other else lang for lang in x])\n",
        "\n",
        "        # Preprocess test data using the saved dummy variables\n",
        "        langs = pd.get_dummies(_df['Languages'].apply(pd.Series).stack(), prefix=\"lang\", dummy_na=False).sum(level=0)\n",
        "        langs = langs.reindex(columns=saved_dummies.columns, fill_value=0)\n",
        "\n",
        "        # Fill the dummy columns with 0 if nan\n",
        "        langs = langs.fillna(0)\n",
        "\n",
        "    # Add the dummy variables to the original dataframe\n",
        "    _df = pd.concat([_df, langs], axis=1)\n",
        "\n",
        "    # Fill NaN with 0\n",
        "    lang_cols = [col for col in _df.columns if col.startswith('lang')] # get all columns with prefix 'lang'\n",
        "    _df[lang_cols] = _df[lang_cols].fillna(0) # fill NaN with 0 for selected columns\n",
        "    \n",
        "    return _df\n",
        "    \n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Hl78fXBJHvs3"
      },
      "source": [
        "## In-app Purchases preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-14T10:35:52.935497Z",
          "start_time": "2023-04-14T10:35:52.881403Z"
        },
        "id": "FqdX_pJLHvs4"
      },
      "outputs": [],
      "source": [
        "# Free apps might skew the in-app purchases column,\n",
        "# so we might split the dataset into free and paid apps\n",
        "\n",
        "def purchases_preprocess(_df):\n",
        "    # Convert the In-app Purchases column to a list of floats\n",
        "    _df['In-app Purchases'] = _df['In-app Purchases'].astype(str)\n",
        "    _df['In-app Purchases'] = _df['In-app Purchases'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
        "\n",
        "    # Convert to float  \n",
        "    _df['In-app Purchases'] = _df['In-app Purchases'].apply(lambda x: [float(i) for i in x])\n",
        "\n",
        "    # Get the number of in-app purchases\n",
        "    _df['purchases_count'] = _df['In-app Purchases'].apply(lambda x: len(x))\n",
        "\n",
        "    # Get the lowest, highest and average purchase\n",
        "    _df['lowest_purchase'] = _df['In-app Purchases'].apply(lambda x: min(x) if len(x) > 0 else 0)\n",
        "    _df['highest_purchase'] = _df['In-app Purchases'].apply(lambda x: max(x) if len(x) > 0 else 0)\n",
        "    _df['average_purchase'] = _df['In-app Purchases'].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
        "\n",
        "    _df['lowest_purchase'] = _df['lowest_purchase'].fillna(0)\n",
        "    _df['highest_purchase'] = _df['highest_purchase'].fillna(0)\n",
        "    _df['average_purchase'] = _df['average_purchase'].fillna(0)\n",
        "    \n",
        "    return _df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "NusCKkJqHvs4"
      },
      "source": [
        "## Age Rating & Price preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-14T10:36:03.614263Z",
          "start_time": "2023-04-14T10:36:03.579637Z"
        },
        "id": "lQ5mruB9Hvs4"
      },
      "outputs": [],
      "source": [
        "def age_preprocess(_df, test=False):\n",
        "    # Convert to string\n",
        "    _df['Age Rating'] = _df['Age Rating'].astype(str)\n",
        "\n",
        "    # Remove the + sign\n",
        "    _df['Age Rating'] = _df['Age Rating'].str.replace('+', '')\n",
        "\n",
        "    # Convert to int\n",
        "    _df['Age Rating'] = _df['Age Rating'].astype(float)\n",
        "    \n",
        "    # Impute missing values using simple imputer with median strategy\n",
        "    if test:\n",
        "        simple_imputer = pickle.load(open('imputers/age_simple.pkl', 'rb'))\n",
        "    else:\n",
        "        simple_imputer = SimpleImputer(strategy='median').fit(_df[['Age Rating']])\n",
        "        pickle.dump(simple_imputer, open('imputers/age_simple.pkl', 'wb'))\n",
        "    \n",
        "    _df['Age Rating'] = simple_imputer.transform(_df[['Age Rating']])    \n",
        "    return _df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "DAMLBK5SHvs5"
      },
      "outputs": [],
      "source": [
        "def price_preprocess(_df):\n",
        "    # Convert to float\n",
        "    _df['Price'] = _df['Price'].astype(float)\n",
        "\n",
        "    # fill the missing values with 0 (free)\n",
        "    _df['Price'] = _df['Price'].fillna(0)\n",
        "    \n",
        "    return _df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gethQ-I0Hvs5"
      },
      "source": [
        "## NLP preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-14T10:43:13.386866Z",
          "start_time": "2023-04-14T10:43:13.386866Z"
        },
        "id": "NX5rEEDPHvs5"
      },
      "outputs": [],
      "source": [
        "def preprocess_nlp(_df, col):\n",
        "    # Convert to string\n",
        "    _df[col] = _df[col].astype(str)\n",
        "\n",
        "    # Remove URLs and email addresses\n",
        "    _df[col] = _df[col].apply(lambda x: re.sub(r'http\\S+|www.\\S+|\\S+@\\S+', '', x))\n",
        "\n",
        "    # Remove the punctuation, numbers, and convert to lowercase\n",
        "    _df[col] = _df[col].apply(lambda x: \" \".join(re.findall(r'\\w+', x.lower())))\n",
        "\n",
        "    # Remove the stopwords\n",
        "    _df[col] = _df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n",
        "\n",
        "    # Stemming\n",
        "    st = nltk.PorterStemmer()\n",
        "    _df[col] = _df[col].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "\n",
        "    # Lemmatization\n",
        "    lem = nltk.WordNetLemmatizer()\n",
        "    _df[col] = _df[col].apply(lambda x: \" \".join([lem.lemmatize(word) for word in x.split()]))\n",
        "\n",
        "    # Remove the frequent and rare words\n",
        "    freq = pd.Series(' '.join(_df[col]).split()).value_counts()\n",
        "    common_freq = list(freq[:10].index)\n",
        "    rare_freq = list(freq[-10:].index)\n",
        "    _df[col] = _df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in common_freq+rare_freq))\n",
        "\n",
        "    # Remove the whitespaces\n",
        "    _df[col] = _df[col].apply(lambda x: \" \".join(x.strip() for x in x.split()))\n",
        "\n",
        "    # Replace NaN values with empty string\n",
        "    _df[col] = _df[col].fillna('')\n",
        "\n",
        "    # Convert text data to bag-of-words representation\n",
        "    vectorizer = CountVectorizer()\n",
        "    BoW = vectorizer.fit_transform(_df[col])\n",
        "\n",
        "    # Apply principal component analysis to reduce the dimensionality\n",
        "    pca_ = PCA(n_components=2)\n",
        "    pca_col = pca_.fit_transform(BoW.toarray())\n",
        "    \n",
        "    # Save the vectorizer and pca for later use with the test data\n",
        "    pickle.dump(vectorizer, open(f'encoders/vectorizer_{col}.pkl', 'wb'))\n",
        "    pickle.dump(pca_, open(f'encoders/pca_{col}.pkl', 'wb'))\n",
        "\n",
        "    # Add the PCA-transformed col to the original dataframe\n",
        "    for feat in range(len(pca_col[0])):\n",
        "        _df[f'{col}_PCA_{feat}'] = pca_col[:, feat]\n",
        "        \n",
        "    return _df\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-_c08Rk-Hvs5"
      },
      "source": [
        "## Description, Name & Subtitle preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "T8uecIVbHvs6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_excitement_score(text, _sia):\n",
        "    \n",
        "    # compute the polarity scores for the given text\n",
        "    scores = _sia.polarity_scores(text)\n",
        "    \n",
        "    # compute the excitement score as the sum of the positive and negative polarity scores\n",
        "    excitement_score = scores['pos'] + abs(scores['neg'])\n",
        "    \n",
        "    return excitement_score\n",
        "\n",
        "# define a function to compute an attractive score from a given text\n",
        "def compute_attractive_score(text, tokenizer):\n",
        "    # define a list of keywords that might make a game attractive to users\n",
        "    attractive_keywords = ['graphics', 'gameplay', 'storyline', 'characters']\n",
        "    \n",
        "    # tokenize the text into words and count how many attractive keywords appear\n",
        "    words = tokenizer(text.lower())\n",
        "    \n",
        "    num_attractive_keywords = len([word for word in words if word in attractive_keywords])\n",
        "    \n",
        "    # compute the attractive score as the ratio of attractive keywords to total words\n",
        "    attractive_score = num_attractive_keywords / len(words) if len(words) > 0 else 0\n",
        "    \n",
        "    return attractive_score\n",
        "\n",
        "def desc_preprocess(_df, test=False):\n",
        "    _df['Description'] = _df['Description'].astype(str)\n",
        "    \n",
        "    # Create column for number of words in description\n",
        "    _df['desc_word_count'] = _df['Description'].apply(lambda x: len(x.split()))\n",
        "    \n",
        "    if not test:        \n",
        "        # load the Sentiment Intensity Analyzer model from NLTK\n",
        "        sia_desc = SentimentIntensityAnalyzer()\n",
        "        pickle.dump(sia_desc, open('encoders/sia_desc.pkl', 'wb'))\n",
        "\n",
        "        tokenizer = nltk.word_tokenize\n",
        "        pickle.dump(nltk.word_tokenize, open('encoders/desc_tokenizer.pkl', 'wb'))\n",
        "        \n",
        "    else:\n",
        "        sia_desc = pickle.load(open('encoders/sia_desc.pkl', 'rb'))\n",
        "        tokenizer = pickle.load(open('encoders/desc_tokenizer.pkl', 'rb'))\n",
        "    \n",
        "    _df['excitement_score'] = _df['Description'].apply(lambda x : compute_excitement_score(x, sia_desc))\n",
        "    _df['attractive_score'] = _df['Description'].apply(lambda x: compute_attractive_score(x, tokenizer))\n",
        "    \n",
        "    return _df\n",
        "\n",
        "def name_preprocess(_df, test=False):\n",
        "    _df['Name'] = _df['Name'].astype(str)\n",
        "    \n",
        "    # Create column for number of words in subtitle\n",
        "    _df['name_word_count'] = _df['Name'].apply(lambda x: len(str(x).split(\" \")))\n",
        "    \n",
        "    if not test:\n",
        "        sia_name = SentimentIntensityAnalyzer()\n",
        "        pickle.dump(sia_name, open('encoders/sia_name.pkl', 'wb'))\n",
        "    else:\n",
        "        sia_name = pickle.load(open('encoders/sia_name.pkl', 'rb'))\n",
        "    \n",
        "    _df['name_sia'] = _df['Name'].apply(lambda x : compute_excitement_score(x, sia_name))\n",
        "    \n",
        "    return _df\n",
        "\n",
        "def sub_preprocess(_df, test=False):\n",
        "    _df['Subtitle'] = _df['Subtitle'].astype(str)\n",
        "    \n",
        "    # Create column for number of words in subtitle\n",
        "    _df['sub_word_count'] = _df['Subtitle'].apply(lambda x: len(str(x).split(\" \")))\n",
        "    \n",
        "    if not test:\n",
        "        sia_sub = SentimentIntensityAnalyzer()\n",
        "        pickle.dump(sia_sub, open('encoders/sia_sub.pkl', 'wb'))\n",
        "    else:\n",
        "        sia_sub = pickle.load(open('encoders/sia_sub.pkl', 'rb'))\n",
        "    \n",
        "    _df['sub_sia'] = _df['Subtitle'].apply(lambda x : compute_excitement_score(x, sia_sub))\n",
        "    \n",
        "    return _df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "O_sh2eoIHvs6"
      },
      "source": [
        "## Icon preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_objects(image_path):\n",
        "    \"\"\"\n",
        "    Detect objects in an image and return the number of objects detected.\n",
        "    \n",
        "    https://medium.com/analytics-vidhya/opencv-findcontours-detailed-guide-692ee19eeb18\n",
        "    \n",
        "    Parameters:\n",
        "        image_path (str): The file path of the image to be analyzed.\n",
        "\n",
        "    Returns:\n",
        "        int: The number of objects detected in the image.\n",
        "    \"\"\"\n",
        "    # Load the image\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    # Convert the image to grayscale\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply edge detection to highlight the edges of objects in the image\n",
        "    edges = cv2.Canny(gray, 100, 200)\n",
        "\n",
        "    # Apply a threshold to convert the edge map to a binary image\n",
        "    _, thresh = cv2.threshold(edges, 0, 255, cv2.THRESH_BINARY_INV)\n",
        "\n",
        "    # Find contours in the binary image\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Return the number of objects detected\n",
        "    return len(contours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-04-14T10:52:48.299695Z",
          "start_time": "2023-04-14T10:52:13.578629Z"
        },
        "id": "sHunkTDXHvs6"
      },
      "outputs": [],
      "source": [
        "def preprocess_icon(img_path):\n",
        "    \n",
        "    # Load the game icon image\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (32, 32))\n",
        "\n",
        "    # Extract color features using color histograms\n",
        "    img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
        "    color_features = []\n",
        "    for i in range(3):\n",
        "        hist = cv2.calcHist([img_lab], [i], None, [256], [0, 256])\n",
        "        color_features.append(hist.ravel())\n",
        "    color_features = np.concatenate(color_features)\n",
        "\n",
        "    # Extract shape features using local binary patterns\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    lbp = local_binary_pattern(gray, 8, 1, method='uniform')\n",
        "    hist_lbp, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 10), range=(0, 9))\n",
        "    edge_features = hist_lbp.astype(float)\n",
        "\n",
        "    # Combine the color and shape features into a single feature vector\n",
        "    feature_vector = np.concatenate((color_features, edge_features))\n",
        "\n",
        "    # Normalize the feature vector to have unit length\n",
        "    normalized_feature_vector = feature_vector / np.linalg.norm(feature_vector)\n",
        "    \n",
        "    return normalized_feature_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "WO7-CjobHvs7"
      },
      "outputs": [],
      "source": [
        "def icons_preprocess(_df, test=False):\n",
        "\n",
        "    # Create a list to store the feature vectors\n",
        "    icon_features = []\n",
        "\n",
        "    _df['Icon URL'] = _df['Icon URL'].astype(str)\n",
        "    _df['icon_objects'] = np.nan\n",
        "\n",
        "    # Iterate over the images and extract the features\n",
        "    for i, row in tqdm(_df.iterrows(), total=_df.shape[0]):\n",
        "        _df.loc[i, 'icon_objects'] = detect_objects(row['Icon URL'])\n",
        "        feature_vec = preprocess_icon(row['Icon URL'])\n",
        "        icon_features.append((row['Icon URL'], feature_vec))\n",
        "        \n",
        "    if not test:\n",
        "        # Apply PCA to reduce the number of features\n",
        "        pca = PCA(n_components=8)\n",
        "        pca.fit([f[1] for f in icon_features])\n",
        "\n",
        "        # Save the pca instance for later use\n",
        "        pickle.dump(pca, open('encoders/icon_pca.pkl', 'wb'))\n",
        "    \n",
        "    else:\n",
        "        pca = pickle.load(open('encoders/icon_pca.pkl', 'rb'))\n",
        "    \n",
        "    reduced_features = pca.transform([f[1] for f in icon_features])\n",
        "\n",
        "    # Convert the reduced features to a dataframe\n",
        "        # Convert the reduced features to a dataframe\n",
        "    icon_features_df = pd.DataFrame({'Icon URL': [f[0] for f in icon_features],\n",
        "                                        'Icon1': reduced_features[:,0],\n",
        "                                        'Icon2': reduced_features[:,1],\n",
        "                                        'Icon3': reduced_features[:,2],\n",
        "                                        'Icon4': reduced_features[:,3],\n",
        "                                        'Icon5': reduced_features[:,4],\n",
        "                                        'Icon6': reduced_features[:,5],\n",
        "                                        'Icon7': reduced_features[:,6],\n",
        "                                        'Icon8': reduced_features[:,7],})\n",
        "\n",
        "    \n",
        "    # Merge the icon features with the original dataframe on the icon URL\n",
        "    _df = _df.merge(icon_features_df, on='Icon URL', how='left')\n",
        "    \n",
        "    return _df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reviews preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_outliers(reviews):\n",
        "    q1 = np.percentile(reviews, 25)\n",
        "    q3 = np.percentile(reviews, 75)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    return [review for review in reviews if review >= lower_bound and review <= upper_bound]\n",
        "\n",
        "\n",
        "def reviews_preprocess(data, test=False):\n",
        "  \n",
        "  # Apply sentiment_analysis\n",
        "  \n",
        "  if test == False:\n",
        "    sia_reviews = SentimentIntensityAnalyzer()\n",
        "    pickle.dump(sia_reviews, open('encoders/sia_reviews.pkl', 'wb'))\n",
        "  else:\n",
        "    sia_reviews = pickle.load(open('encoders/sia_reviews.pkl', 'rb'))\n",
        "  \n",
        "  # Only preprocess the reviews that are not null\n",
        "  data['Reviews'] = data['Reviews'].apply(lambda x: [sia_reviews.polarity_scores(review)['compound'] for review in x] if isinstance(x, list) and len(x)>0 else [])\n",
        "\n",
        "  # Get the lowest, highest and average Reviews\n",
        "  data['lowest_review'] = data['Reviews'].apply(lambda x: min(x) if len(x) > 0 else None)\n",
        "  data['highest_review'] = data['Reviews'].apply(lambda x: max(x) if len(x) > 0 else None)\n",
        "  \n",
        "  # Calculate the average review without the outliers via z-score\n",
        "  data['average_review'] = data['Reviews'].apply(lambda x: np.mean(remove_outliers(x)) if len(x) > 0 else None)\n",
        "  \n",
        "  # Impute missing values using KNN\n",
        "  if test:\n",
        "    knn_low = pickle.load(open('imputers/review_low_knn.pkl', 'rb'))\n",
        "    knn_high = pickle.load(open('imputers/review_high_knn.pkl', 'rb'))\n",
        "    knn_avg = pickle.load(open('imputers/review_avg_knn.pkl', 'rb'))\n",
        "  else:\n",
        "    knn_low = KNNImputer(n_neighbors=5).fit(data[['lowest_review']])\n",
        "    knn_high = KNNImputer(n_neighbors=5).fit(data[['highest_review']])\n",
        "    knn_avg = KNNImputer(n_neighbors=5).fit(data[['average_review']])\n",
        "\n",
        "    pickle.dump(knn_low, open('imputers/review_low_knn.pkl', 'wb'))\n",
        "    pickle.dump(knn_high, open('imputers/review_high_knn.pkl', 'wb'))\n",
        "    pickle.dump(knn_avg, open('imputers/review_avg_knn.pkl', 'wb'))\n",
        "\n",
        "  data['lowest_review'] = knn_low.transform(data[['lowest_review']])\n",
        "  data['highest_review'] = knn_high.transform(data[['highest_review']])\n",
        "  data['average_review'] = knn_avg.transform(data[['average_review']])\n",
        "  \n",
        "  return data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KFf6bh1WHvs7"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "zKXKoW2eHvs7"
      },
      "outputs": [],
      "source": [
        "def preprocess_pipeline(_df, test=False):\n",
        "    _df = _df.drop(['Primary Genre', 'ID', 'URL'], axis=1)\n",
        "    \n",
        "    _df = date_preprocess(_df, test)\n",
        "    _df = purchases_preprocess(_df)\n",
        "\n",
        "    _df = age_preprocess(_df, test)\n",
        "    _df = price_preprocess(_df)\n",
        "    \n",
        "    _df = name_preprocess(_df, test)\n",
        "    _df = sub_preprocess(_df, test)\n",
        "    _df = desc_preprocess(_df, test)\n",
        "    _df = icons_preprocess(_df, test)\n",
        "    \n",
        "    _df = _df.drop(['Name',\n",
        "                    'Subtitle',\n",
        "                    'Description',\n",
        "                    'Icon URL',\n",
        "                    'Languages',\n",
        "                    'Genres',\n",
        "                    'In-app Purchases',\n",
        "                    'Original Release Date',\n",
        "                    'Current Version Release Date'], axis=1)\n",
        "    \n",
        "    # Dropping all the columns that are already preprocessed as they are not needed anymore \n",
        "    # But more importantly for KNN imputation to work\n",
        "    \n",
        "    _df = dev_preprocess_freq_enc(_df, test)\n",
        "    _df = dev_preprocess_target_enc(_df, test)\n",
        "    \n",
        "    _df = _df.drop(['Developer'], axis=1)\n",
        "    \n",
        "    _df = reviews_preprocess(_df, test)\n",
        "    _df = _df.drop(['Reviews'], axis=1)\n",
        "    \n",
        "    if not test:\n",
        "        _df.to_csv('preprocessed_data.csv', index=False)\n",
        "    \n",
        "    return _df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OPW7EDf-Hvs8"
      },
      "source": [
        "## Scores Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "scChLmTBHvs8"
      },
      "outputs": [],
      "source": [
        "def plot_scores(mse_train, r2_train, mse_test, r2_test):\n",
        "    # Calculate the differences between the training and testing scores\n",
        "    mse_diff =  abs(mse_train - mse_test)\n",
        "    r2_diff = abs(r2_train - r2_test)\n",
        "\n",
        "    # Determine whether each score is better or worse than the other\n",
        "    mse_color = 'green' if mse_diff < 0.1 else 'red'\n",
        "    r2_color = 'green' if r2_diff < 0.1 else 'red'\n",
        "    \n",
        "    # Create a bar plot of the scores with colors based on the value\n",
        "    fig = go.Figure(data=[\n",
        "        go.Bar(name='MSE Train', x=['MSE'], y=[mse_train], marker_color='navy'),\n",
        "        go.Bar(name='MSE Test', x=['MSE'], y=[mse_test], marker_color='blue'),\n",
        "        go.Bar(name='MSE Difference', x=['MSE'], y=[mse_diff], marker_color=mse_color),\n",
        "        go.Bar(name='R2 Train', x=['R2'], y=[r2_train], marker_color='purple'),\n",
        "        go.Bar(name='R2 Test', x=['R2'], y=[r2_test], marker_color='pink'),\n",
        "        go.Bar(name='R2 Difference', x=['R2'], y=[r2_diff], marker_color=r2_color)\n",
        "    ])\n",
        "\n",
        "    # Add labels and title\n",
        "    fig.update_layout(title='Model Performance', xaxis_title='Score Type', yaxis_title='Score')\n",
        "\n",
        "    # Show the plot\n",
        "    fig.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YW1zAhY8Hvs9"
      },
      "source": [
        "## Feature Scaling and Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "IPuIVP0lHvs9"
      },
      "outputs": [],
      "source": [
        "# Scale the features\n",
        "\n",
        "def scale_data_std(_df, test=False):\n",
        "    cols = _df.columns\n",
        "    \n",
        "    if not test:\n",
        "        scaler = StandardScaler()\n",
        "        _df = scaler.fit_transform(_df)\n",
        "        \n",
        "        # Save the scaler\n",
        "        pickle.dump(scaler, open('scalers/std_scaler.pkl', 'wb'))\n",
        "    else:\n",
        "        scaler = pickle.load(open('scalers/std_scaler.pkl', 'rb'))\n",
        "        _df = scaler.transform(_df)\n",
        "    \n",
        "    _df = pd.DataFrame(_df, columns=cols)\n",
        "    return _df\n",
        "\n",
        "def scale_data_minmax(_df, test=False):\n",
        "    cols = _df.columns\n",
        "    \n",
        "    if not test:\n",
        "        scaler = MinMaxScaler()\n",
        "        _df = scaler.fit_transform(_df)\n",
        "        \n",
        "        # Save the scaler\n",
        "        pickle.dump(scaler, open('scalers/minmax_scaler.pkl', 'wb'))\n",
        "    \n",
        "    else:\n",
        "        scaler = pickle.load(open('scalers/minmax_scaler.pkl', 'rb'))\n",
        "        _df = scaler.transform(_df)\n",
        "    \n",
        "    _df = pd.DataFrame(_df, columns=cols)\n",
        "    return _df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "uG4yGb0YHvs-"
      },
      "outputs": [],
      "source": [
        "def select_features(_df_x, _df_y, num_features=8, test=False):\n",
        "    if not test:\n",
        "        selector = SelectKBest(f_regression, k=num_features)\n",
        "        selector.fit(_df_x, _df_y)\n",
        "        \n",
        "        # Save the selector\n",
        "        pickle.dump(selector, open('encoders/selector.pkl', 'wb'))\n",
        "        \n",
        "    else:\n",
        "        selector = pickle.load(open('encoders/selector.pkl', 'rb'))\n",
        "        \n",
        "    _df_x = selector.transform(_df_x)\n",
        "    \n",
        "    return _df_x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'imputers/date_simple.pkl'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[41], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m# Join the training data together\u001b[39;00m\n\u001b[0;32m     44\u001b[0m X_train \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([X_train, y_train], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m X_train \u001b[39m=\u001b[39m preprocess_pipeline(X_train)\n\u001b[0;32m     46\u001b[0m X_train \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mAverage User Rating\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     47\u001b[0m X_train \u001b[39m=\u001b[39m scale_data_std(X_train)\n",
            "Cell \u001b[1;32mIn[35], line 4\u001b[0m, in \u001b[0;36mpreprocess_pipeline\u001b[1;34m(_df, test)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_pipeline\u001b[39m(_df, test\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m      2\u001b[0m     _df \u001b[39m=\u001b[39m _df\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mPrimary Genre\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mID\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mURL\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     _df \u001b[39m=\u001b[39m date_preprocess(_df, test)\n\u001b[0;32m      6\u001b[0m     _df \u001b[39m=\u001b[39m purchases_preprocess(_df)\n\u001b[0;32m      7\u001b[0m     _df \u001b[39m=\u001b[39m _df\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mLanguages\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mGenres\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mIn-app Purchases\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
            "Cell \u001b[1;32mIn[19], line 11\u001b[0m, in \u001b[0;36mdate_preprocess\u001b[1;34m(_df, test)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     simple_imputer \u001b[39m=\u001b[39m SimpleImputer(strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmedian\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mfit(_df[[\u001b[39m'\u001b[39m\u001b[39mOriginal Release Date\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCurrent Version Release Date\u001b[39m\u001b[39m'\u001b[39m]])\n\u001b[1;32m---> 11\u001b[0m     pickle\u001b[39m.\u001b[39mdump(simple_imputer, \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mimputers/date_simple.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     13\u001b[0m _df[[\u001b[39m'\u001b[39m\u001b[39mOriginal Release Date\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCurrent Version Release Date\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m simple_imputer\u001b[39m.\u001b[39mtransform(_df[[\u001b[39m'\u001b[39m\u001b[39mOriginal Release Date\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCurrent Version Release Date\u001b[39m\u001b[39m'\u001b[39m]])\n\u001b[0;32m     15\u001b[0m \u001b[39m# Create a new column with the age of the game\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Yusuf\\.conda\\envs\\mcadams\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'imputers/date_simple.pkl'"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import numpy as np\n",
        "\n",
        "# Define the number of folds for cross-validation\n",
        "n_folds = 5\n",
        "\n",
        "# Load your data and target variables into X and y arrays\n",
        "# df_origin = df_origin.dropna(subset=['Reviews'])\n",
        "X, y = df_origin.drop('Average User Rating', axis=1), df_origin['Average User Rating']\n",
        "\n",
        "# Instantiate the CatBoost model object\n",
        "model = CatBoostRegressor(iterations=1000,\n",
        "                            learning_rate=0.05,\n",
        "                            depth=3,\n",
        "                            l2_leaf_reg=1,\n",
        "                            border_count=32,\n",
        "                            bagging_temperature=1,\n",
        "                            fold_permutation_block=1,\n",
        "                            boosting_type='Plain',\n",
        "                            random_seed=42,\n",
        "                            subsample=1.0,\n",
        "                            colsample_bylevel=0.5,\n",
        "                            early_stopping_rounds=5,\n",
        "                            loss_function='RMSE', eval_metric='RMSE',\n",
        "                          verbose=50)\n",
        "\n",
        "# Create a K-fold cross-validator object\n",
        "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Use a loop to train and test the model on each fold of the data\n",
        "fold_scores = []\n",
        "for train_index, test_index in tqdm(kf.split(X, y), total=n_folds):\n",
        "    # Get the training and testing sets for this fold\n",
        "    _X_train, _X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    \n",
        "    X_train, X_test = _X_train.copy(), _X_test.copy()\n",
        "    \n",
        "    # Join the training data together\n",
        "    X_train = pd.concat([X_train, y_train], axis=1)\n",
        "    X_train = preprocess_pipeline(X_train)\n",
        "    X_train = X_train.drop('Average User Rating', axis=1)\n",
        "    X_train = scale_data_std(X_train)\n",
        "    X_train = select_features(X_train, y_train)\n",
        "    \n",
        "    # Join the testing data together\n",
        "    X_test = pd.concat([X_test, y_test], axis=1)\n",
        "    X_test = preprocess_pipeline(X_test, test=True)\n",
        "    X_test = X_test.drop('Average User Rating', axis=1)\n",
        "    X_test = scale_data_std(X_test, test=True)\n",
        "    X_test = select_features(X_test, y_test, test=True)    \n",
        "    \n",
        "    # Fit the model to the training data\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate the model on the testing data and store the results\n",
        "    fold_score = model.score(X_test, y_test)\n",
        "    print(fold_score)\n",
        "    print(confusion_matrix(y_test, model.predict(X_test)))\n",
        "    print('----------------')\n",
        "    fold_scores.append(fold_score)\n",
        "\n",
        "# Calculate the average score across all folds\n",
        "print(np.mean(fold_scores))\n",
        "fold_scores\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XoNTBxJ1HvtH"
      },
      "source": [
        "## Lazy Predict & Advanced Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2195/2195 [00:49<00:00, 44.43it/s]\n",
            "100%|██████████| 549/549 [00:11<00:00, 47.36it/s]\n"
          ]
        }
      ],
      "source": [
        "# drop the rows with no reviews\n",
        "df_wo_reviews = df_origin.dropna(subset=['Reviews'])\n",
        "\n",
        "df, df_test = train_test_split(df_wo_reviews, test_size=0.2, random_state=42)\n",
        "\n",
        "df = preprocess_pipeline(df)\n",
        "df_test = preprocess_pipeline(df_test, test=True)\n",
        "\n",
        "df_x, df_y = df.drop('Average User Rating', axis=1), df['Average User Rating']\n",
        "df_test_x, df_test_y = df_test.drop('Average User Rating', axis=1), df_test['Average User Rating']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot correlation matrix between features and target\n",
        "\n",
        "corr = df_x.corrwith(df_y)\n",
        "corr = corr.sort_values(ascending=False)\n",
        "\n",
        "corr = corr[abs(corr) > 0.1]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.barplot(x=corr.values, y=corr.index)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale the features\n",
        "df_x_scaled = scale_data_minmax(df_x)\n",
        "df_test_x_scaled = scale_data_minmax(df_test_x, test=True)\n",
        "\n",
        "# Select the features\n",
        "df_x_select = select_features(df_x_scaled, df_y)\n",
        "df_test_x_select = select_features(df_test_x_scaled, df_test_y, test=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "game_age\n",
            "last_update\n",
            "purchases_count\n",
            "sub_word_count\n",
            "dev_avg\n",
            "lowest_review\n",
            "highest_review\n",
            "average_review\n"
          ]
        }
      ],
      "source": [
        "selector = pickle.load(open('encoders/selector.pkl', 'rb'))\n",
        "# Print the selected features\n",
        "for i in range(len(selector.get_support())): \n",
        "    if selector.get_support()[i]:\n",
        "        print(df_x.columns[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x14Faa_f856r",
        "outputId": "a722617a-49ca-4b5c-832d-f04293e1c56e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 42/42 [01:16<00:00,  1.83s/it]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Adjusted R-Squared</th>\n",
              "      <th>R-Squared</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>Time Taken</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>SVR</th>\n",
              "      <td>0.27</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NuSVR</th>\n",
              "      <td>0.27</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GradientBoostingRegressor</th>\n",
              "      <td>0.26</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BaggingRegressor</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandomForestRegressor</th>\n",
              "      <td>0.24</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.58</td>\n",
              "      <td>1.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ExtraTreesRegressor</th>\n",
              "      <td>0.24</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MLPRegressor</th>\n",
              "      <td>0.24</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.58</td>\n",
              "      <td>1.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>OrthogonalMatchingPursuitCV</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SGDRegressor</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BayesianRidge</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RidgeCV</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ElasticNetCV</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LassoCV</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ridge</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LarsCV</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LassoLarsIC</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LinearRegression</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LassoLarsCV</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lars</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TransformedTargetRegressor</th>\n",
              "      <td>0.23</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PoissonRegressor</th>\n",
              "      <td>0.22</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LinearSVR</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HuberRegressor</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HistGradientBoostingRegressor</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GammaRegressor</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TweedieRegressor</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LGBMRegressor</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBRegressor</th>\n",
              "      <td>0.18</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNeighborsRegressor</th>\n",
              "      <td>0.17</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>OrthogonalMatchingPursuit</th>\n",
              "      <td>0.09</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RANSACRegressor</th>\n",
              "      <td>0.07</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AdaBoostRegressor</th>\n",
              "      <td>0.05</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LassoLars</th>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lasso</th>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ElasticNet</th>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DummyRegressor</th>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PassiveAggressiveRegressor</th>\n",
              "      <td>-0.25</td>\n",
              "      <td>-0.23</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ExtraTreeRegressor</th>\n",
              "      <td>-0.28</td>\n",
              "      <td>-0.26</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>QuantileRegressor</th>\n",
              "      <td>-0.32</td>\n",
              "      <td>-0.30</td>\n",
              "      <td>0.77</td>\n",
              "      <td>69.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTreeRegressor</th>\n",
              "      <td>-0.42</td>\n",
              "      <td>-0.40</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GaussianProcessRegressor</th>\n",
              "      <td>-15.78</td>\n",
              "      <td>-15.54</td>\n",
              "      <td>2.73</td>\n",
              "      <td>0.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KernelRidge</th>\n",
              "      <td>-38.63</td>\n",
              "      <td>-38.05</td>\n",
              "      <td>4.20</td>\n",
              "      <td>0.16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               Adjusted R-Squared  R-Squared  RMSE  Time Taken\n",
              "Model                                                                         \n",
              "SVR                                          0.27       0.28  0.57        0.41\n",
              "NuSVR                                        0.27       0.28  0.57        0.32\n",
              "GradientBoostingRegressor                    0.26       0.27  0.57        0.49\n",
              "BaggingRegressor                             0.25       0.26  0.58        0.17\n",
              "RandomForestRegressor                        0.24       0.25  0.58        1.58\n",
              "ExtraTreesRegressor                          0.24       0.25  0.58        0.55\n",
              "MLPRegressor                                 0.24       0.25  0.58        1.66\n",
              "OrthogonalMatchingPursuitCV                  0.23       0.24  0.59        0.02\n",
              "SGDRegressor                                 0.23       0.24  0.59        0.02\n",
              "BayesianRidge                                0.23       0.24  0.59        0.01\n",
              "RidgeCV                                      0.23       0.24  0.59        0.01\n",
              "ElasticNetCV                                 0.23       0.24  0.59        0.09\n",
              "LassoCV                                      0.23       0.24  0.59        0.09\n",
              "Ridge                                        0.23       0.24  0.59        0.01\n",
              "LarsCV                                       0.23       0.24  0.59        0.03\n",
              "LassoLarsIC                                  0.23       0.24  0.59        0.01\n",
              "LinearRegression                             0.23       0.24  0.59        0.01\n",
              "LassoLarsCV                                  0.23       0.24  0.59        0.02\n",
              "Lars                                         0.23       0.24  0.59        0.01\n",
              "TransformedTargetRegressor                   0.23       0.24  0.59        0.01\n",
              "PoissonRegressor                             0.22       0.23  0.59        0.01\n",
              "LinearSVR                                    0.21       0.22  0.59        0.03\n",
              "HuberRegressor                               0.21       0.22  0.59        0.03\n",
              "HistGradientBoostingRegressor                0.21       0.22  0.59        0.57\n",
              "GammaRegressor                               0.20       0.21  0.60        0.02\n",
              "TweedieRegressor                             0.20       0.21  0.60        0.02\n",
              "LGBMRegressor                                0.20       0.21  0.60        0.12\n",
              "XGBRegressor                                 0.18       0.19  0.60        0.20\n",
              "KNeighborsRegressor                          0.17       0.19  0.61        0.03\n",
              "OrthogonalMatchingPursuit                    0.09       0.10  0.64        0.01\n",
              "RANSACRegressor                              0.07       0.08  0.64        0.18\n",
              "AdaBoostRegressor                            0.05       0.06  0.65        0.15\n",
              "LassoLars                                   -0.01      -0.00  0.67        0.01\n",
              "Lasso                                       -0.01      -0.00  0.67        0.01\n",
              "ElasticNet                                  -0.01      -0.00  0.67        0.01\n",
              "DummyRegressor                              -0.01      -0.00  0.67        0.01\n",
              "PassiveAggressiveRegressor                  -0.25      -0.23  0.75        0.01\n",
              "ExtraTreeRegressor                          -0.28      -0.26  0.75        0.02\n",
              "QuantileRegressor                           -0.32      -0.30  0.77       69.23\n",
              "DecisionTreeRegressor                       -0.42      -0.40  0.79        0.03\n",
              "GaussianProcessRegressor                   -15.78     -15.54  2.73        0.51\n",
              "KernelRidge                                -38.63     -38.05  4.20        0.16"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reg = LazyRegressor(ignore_warnings=True, custom_metric=None)\n",
        "models, predictions = reg.fit(df_x_select, df_test_x_select, df_y, df_test_y)\n",
        "\n",
        "models = models.sort_values(by='RMSE', ascending=True)\n",
        "models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a linear regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(df_x_select, df_y)\n",
        "\n",
        "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
        "mse_train = mean_squared_error(df_y, model.predict(df_x_select))\n",
        "r2_train = r2_score(df_y, model.predict(df_x_select))\n",
        "mse_test = mean_squared_error(df_test_y, model.predict(df_test_x_select))\n",
        "r2_test = r2_score(df_test_y, model.predict(df_test_x_select))\n",
        "\n",
        "# Print the training MSE and R2 score\n",
        "print('Training MSE: ', mse_train)\n",
        "print('Training R2: ', r2_train)\n",
        "\n",
        "# Print the testing MSE and R2 score\n",
        "print('Testing MSE: ', mse_test)\n",
        "print('Testing R2: ', r2_test)\n",
        "\n",
        "# Display the scores\n",
        "plot_scores(mse_train, r2_train, mse_test, r2_test)\n",
        "\n",
        "# Save the model\n",
        "pickle.dump(model, open('models/LR_model.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an elastic net regression model\n",
        "model = ElasticNet(alpha=0.001)\n",
        "\n",
        "# Train the model\n",
        "model.fit(df_x_select, df_y)\n",
        "\n",
        "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
        "mse_train = mean_squared_error(df_y, model.predict(df_x_select))\n",
        "r2_train = r2_score(df_y, model.predict(df_x_select))\n",
        "mse_test = mean_squared_error(df_test_y, model.predict(df_test_x_select))\n",
        "r2_test = r2_score(df_test_y, model.predict(df_test_x_select))\n",
        "\n",
        "# Print the training MSE and R2 score\n",
        "print('Training MSE: ', mse_train)\n",
        "print('Training R2: ', r2_train)\n",
        "\n",
        "# Print the testing MSE and R2 score\n",
        "print('Testing MSE: ', mse_test)\n",
        "print('Testing R2: ', r2_test)\n",
        "\n",
        "# Display the scores\n",
        "plot_scores(mse_train, r2_train, mse_test, r2_test)\n",
        "\n",
        "# Save the model\n",
        "pickle.dump(model, open('models/ElasticNet_model.pkl', 'wb'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a polynomial regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Generate the polynomial features\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "df_x_poly = poly.fit_transform(df_x_select)\n",
        "df_test_x_poly = poly.transform(df_test_x_select)\n",
        "\n",
        "# Save the polynomial features\n",
        "pickle.dump(poly, open('encoders/poly.pkl', 'wb'))\n",
        "\n",
        "# Train the model\n",
        "model.fit(df_x_poly, df_y)\n",
        "\n",
        "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
        "mse_train = mean_squared_error(df_y, model.predict(df_x_poly))\n",
        "r2_train = r2_score(df_y, model.predict(df_x_poly))\n",
        "mse_test = mean_squared_error(df_test_y, model.predict(df_test_x_poly))\n",
        "r2_test = r2_score(df_test_y, model.predict(df_test_x_poly))\n",
        "\n",
        "# Print the training MSE and R2 score\n",
        "print('Training MSE: ', mse_train)\n",
        "print('Training R2: ', r2_train)\n",
        "\n",
        "# Print the testing MSE and R2 score\n",
        "print('Testing MSE: ', mse_test)\n",
        "print('Testing R2: ', r2_test)\n",
        "\n",
        "# Display the scores\n",
        "plot_scores(mse_train, r2_train, mse_test, r2_test)\n",
        "# Save the model\n",
        "pickle.dump(model, open('models/Polynomial_model.pkl', 'wb'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an XGBoost model\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "model = xgb.XGBRegressor(subsample= 0.8,\n",
        "                         reg_lambda= 0.1,\n",
        "                         reg_alpha= 0.1,\n",
        "                         n_estimators=500,\n",
        "                         min_child_weight=1,\n",
        "                         max_depth= 4,\n",
        "                         learning_rate= 0.01,\n",
        "                         gamma =0.1,\n",
        "                         colsample_bytree= 0.8)\n",
        "\n",
        "# Train the model\n",
        "model.fit(df_x_select, df_y)\n",
        "\n",
        "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
        "mse_train = mean_squared_error(df_y, model.predict(df_x_select))\n",
        "r2_train = r2_score(df_y, model.predict(df_x_select))\n",
        "mse_test = mean_squared_error(df_test_y, model.predict(df_test_x_select))\n",
        "r2_test = r2_score(df_test_y, model.predict(df_test_x_select))\n",
        "\n",
        "# Print the training MSE and R2 score\n",
        "print('Training MSE: ', mse_train)\n",
        "print('Training R2: ', r2_train)\n",
        "\n",
        "# Print the testing MSE and R2 score\n",
        "print('Testing MSE: ', mse_test)\n",
        "print('Testing R2: ', r2_test)\n",
        "\n",
        "# Display the scores\n",
        "plot_scores(mse_train, r2_train, mse_test, r2_test)\n",
        "\n",
        "# Save the model\n",
        "pickle.dump(model, open('models/XGBoost_model.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a Gradient Boosting model\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "model = GradientBoostingRegressor(learning_rate=0.01, \n",
        "                                  max_depth=5, \n",
        "                                  max_features='sqrt', \n",
        "                                  min_samples_leaf=1, \n",
        "                                  min_samples_split=2, \n",
        "                                  n_estimators=500, \n",
        "                                  subsample=0.8)\n",
        "\n",
        "# Train the model\n",
        "model.fit(df_x_select, df_y)\n",
        "\n",
        "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
        "mse_train = mean_squared_error(df_y, model.predict(df_x_select))\n",
        "r2_train = r2_score(df_y, model.predict(df_x_select))\n",
        "mse_test = mean_squared_error(df_test_y, model.predict(df_test_x_select))\n",
        "r2_test = r2_score(df_test_y, model.predict(df_test_x_select))\n",
        "\n",
        "# Print the training MSE and R2 score\n",
        "print('Training MSE: ', mse_train)\n",
        "print('Training R2: ', r2_train)\n",
        "\n",
        "# Print the testing MSE and R2 score\n",
        "print('Testing MSE: ', mse_test)\n",
        "print('Testing R2: ', r2_test)\n",
        "\n",
        "# Display the scores\n",
        "plot_scores(mse_train, r2_train, mse_test, r2_test)\n",
        "\n",
        "# Save the model\n",
        "pickle.dump(model, open('models/GradientBoosting_model.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a Random Forest Regressor model\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor(max_depth=5, \n",
        "                              n_estimators=500, \n",
        "                              n_jobs=-1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(df_x_select, df_y)\n",
        "\n",
        "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
        "mse_train = mean_squared_error(df_y, model.predict(df_x_select))\n",
        "r2_train = r2_score(df_y, model.predict(df_x_select))\n",
        "mse_test = mean_squared_error(df_test_y, model.predict(df_test_x_select))\n",
        "r2_test = r2_score(df_test_y, model.predict(df_test_x_select))\n",
        "\n",
        "# Print the training MSE and R2 score\n",
        "print('Training MSE: ', mse_train)\n",
        "print('Training R2: ', r2_train)\n",
        "\n",
        "# Print the testing MSE and R2 score\n",
        "print('Testing MSE: ', mse_test)\n",
        "print('Testing R2: ', r2_test)\n",
        "\n",
        "# Display the scores\n",
        "plot_scores(mse_train, r2_train, mse_test, r2_test)\n",
        "# Save the model\n",
        "pickle.dump(model, open('models/RandomForest_model.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create CatBoost Regressor model\n",
        "\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "model = CatBoostRegressor(iterations=1000,\n",
        "                            learning_rate=0.05,\n",
        "                            depth=3,\n",
        "                            l2_leaf_reg=1,\n",
        "                            border_count=32,\n",
        "                            bagging_temperature=1,\n",
        "                            fold_permutation_block=1,\n",
        "                            boosting_type='Plain',\n",
        "                            random_seed=42,\n",
        "                            subsample=1.0,\n",
        "                            colsample_bylevel=0.5,\n",
        "                            early_stopping_rounds=5,\n",
        "                            loss_function='RMSE', eval_metric='RMSE',\n",
        "                          verbose=50)\n",
        "\n",
        "# Train the model\n",
        "model.fit(df_x_select, df_y)\n",
        "\n",
        "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
        "mse_train = mean_squared_error(df_y, model.predict(df_x_select))\n",
        "r2_train = r2_score(df_y, model.predict(df_x_select))\n",
        "mse_test = mean_squared_error(df_test_y, model.predict(df_test_x_select))\n",
        "r2_test = r2_score(df_test_y, model.predict(df_test_x_select))\n",
        "\n",
        "# Print the training MSE and R2 score\n",
        "print('Training MSE: ', mse_train)\n",
        "print('Training R2: ', r2_train)\n",
        "\n",
        "# Print the testing MSE and R2 score\n",
        "print('Testing MSE: ', mse_test)\n",
        "print('Testing R2: ', r2_test)\n",
        "\n",
        "# print catboost weights with the feature names and biases\n",
        "print(model.get_feature_importance(prettified=True)) \n",
        "\n",
        "# Display the scores\n",
        "plot_scores(mse_train, r2_train, mse_test, r2_test)\n",
        "\n",
        "# Save the model\n",
        "pickle.dump(model, open('models/CatBoost_model.pkl', 'wb'))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from catboost import CatBoostRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'iterations': [500],\n",
        "    'learning_rate': [0.01],\n",
        "    'depth': [5],\n",
        "    'l2_leaf_reg': [1],\n",
        "    'border_count': [32],\n",
        "    'bagging_temperature': [1],\n",
        "    'fold_permutation_block': [1],\n",
        "    'boosting_type': ['Plain'],\n",
        "    'random_seed': [42],\n",
        "    'subsample': [1.0],\n",
        "    'colsample_bylevel': [0.7],\n",
        "    'early_stopping_rounds': [5],\n",
        "}\n",
        "\n",
        "# Create a CatBoost regressor\n",
        "clf = CatBoostRegressor(loss_function='RMSE', eval_metric='RMSE', verbose=250)\n",
        "\n",
        "# Grid Search with cross-validation\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(df_x_select, df_y)\n",
        "\n",
        "# Print best hyperparameters and corresponding score\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best score:\", grid_search.best_score_)\n",
        "\n",
        "# Train final model with best hyperparameters on entire dataset\n",
        "best_clf = CatBoostRegressor(loss_function='RMSE', eval_metric='RMSE', logging_level='Silent', **grid_search.best_params_)\n",
        "best_clf.fit(df_x_select, df_y, eval_set=(df_test_x_select, df_test_y))\n",
        "\n",
        "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
        "mse_train = mean_squared_error(df_y, best_clf.predict(df_x_select))\n",
        "r2_train = r2_score(df_y, best_clf.predict(df_x_select))\n",
        "mse_test = mean_squared_error(df_test_y, best_clf.predict(df_test_x_select))\n",
        "r2_test = r2_score(df_test_y, best_clf.predict(df_test_x_select))\n",
        "\n",
        "# Print the training MSE and R2 score\n",
        "print('Training MSE: ', mse_train)\n",
        "print('Training R2: ', r2_train)\n",
        "\n",
        "# Print the testing MSE and R2 score\n",
        "print('Testing MSE: ', mse_test)\n",
        "print('Testing R2: ', r2_test)\n",
        "\n",
        "# Display the scores\n",
        "plot_scores(mse_train, r2_train, mse_test, r2_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradient Boosting with RandomizedSearchCV on GPU\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define the Gradient Boosting regressor\n",
        "gb_model = GradientBoostingRegressor()\n",
        "\n",
        "# Define the hyperparameter grid for RandomizedSearchCV\n",
        "params = {\n",
        "    'learning_rate': [0.001, 0.01],\n",
        "    'max_depth': [3, 4],\n",
        "    'subsample': [0.8],\n",
        "    'n_estimators': [500],\n",
        "    'min_samples_split': [2, 3],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Define the RandomizedSearchCV object\n",
        "rs_cv = RandomizedSearchCV(\n",
        "    estimator=gb_model,\n",
        "    param_distributions=params,\n",
        "    n_iter=10,\n",
        "    cv=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the data\n",
        "rs_cv.fit(df_x_select, df_y)\n",
        " \n",
        "# Print the best hyperparameters\n",
        "print(rs_cv.best_params_)\n",
        "\n",
        "# Use the best hyperparameters to train the final model\n",
        "final_gb_model = GradientBoostingRegressor(**rs_cv.best_params_)\n",
        "final_gb_model.fit(df_x_select, df_y)\n",
        "\n",
        "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
        "mse_train = mean_squared_error(df_y, final_gb_model.predict(df_x_select))\n",
        "r2_train = r2_score(df_y, final_gb_model.predict(df_x_select))\n",
        "mse_test = mean_squared_error(df_test_y, final_gb_model.predict(df_test_x_select))\n",
        "r2_test = r2_score(df_test_y, final_gb_model.predict(df_test_x_select))\n",
        "\n",
        "# Print the training MSE and R2 score\n",
        "print('Training MSE: ', mse_train)\n",
        "print('Training R2: ', r2_train)\n",
        "\n",
        "# Print the testing MSE and R2 score\n",
        "print('Testing MSE: ', mse_test)\n",
        "print('Testing R2: ', r2_test)\n",
        " \n",
        "# Display the scores\n",
        "plot_scores(mse_train, r2_train, mse_test, r2_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Random Forest with GridSearchCV on GPU\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the Random Forest regressor\n",
        "rf_model = RandomForestRegressor()\n",
        "\n",
        "# Define the hyperparameter grid for GridSearchCV\n",
        "params = {\n",
        "    'max_depth': [4, 5],\n",
        "    'n_estimators': [500, 1000],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'random_state': [42],\n",
        "    'verbose': [1],\n",
        "    'warm_start': [True],\n",
        "    'bootstrap': [True],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'min_samples_split': [2, 3],\n",
        "    'criterion': ['squared_error'],\n",
        "    'oob_score': [True]\n",
        "}\n",
        "\n",
        "# Define the GridSearchCV object\n",
        "gs_cv = GridSearchCV(\n",
        "    estimator=rf_model,\n",
        "    param_grid=params,\n",
        "    cv=5,\n",
        "    n_jobs=6,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    return_train_score=True,\n",
        "    refit=True,\n",
        ")\n",
        "\n",
        "# Fit the GridSearchCV object to the data\n",
        "gs_cv.fit(df_x_select, df_y)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(gs_cv.best_params_)\n",
        "\n",
        "# Use the best hyperparameters to train the final model\n",
        "final_rf_model = RandomForestRegressor(**gs_cv.best_params_)\n",
        "\n",
        "final_rf_model.fit(df_x_select, df_y)\n",
        "\n",
        "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
        "mse_train = mean_squared_error(df_y, final_rf_model.predict(df_x_select))\n",
        "r2_train = r2_score(df_y, final_rf_model.predict(df_x_select))\n",
        "mse_test = mean_squared_error(df_test_y, final_rf_model.predict(df_test_x_select))\n",
        "r2_test = r2_score(df_test_y, final_rf_model.predict(df_test_x_select))\n",
        "\n",
        "# Print the training MSE and R2 score\n",
        "print('Training MSE: ', mse_train)\n",
        "print('Training R2: ', r2_train)\n",
        "\n",
        "# Print the testing MSE and R2 score\n",
        "print('Testing MSE: ', mse_test)\n",
        "print('Testing R2: ', r2_test)\n",
        "\n",
        "# Display the scores\n",
        "plot_scores(mse_train, r2_train, mse_test, r2_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define the XGBoost regressor\n",
        "xgb_model = xgb.XGBRegressor()\n",
        "\n",
        "# Define the hyperparameter grid for RandomizedSearchCV\n",
        "params = {\n",
        "    'learning_rate': [0.01],\n",
        "    'max_depth': [3, 4],\n",
        "    'subsample': [0.8],\n",
        "    'colsample_bytree': [0.8],\n",
        "    'n_estimators': [500, 1000],\n",
        "    'min_child_weight': [1],\n",
        "    'gamma': [0.1],\n",
        "    'reg_alpha': [0.1],\n",
        "    'reg_lambda': [0.1]\n",
        "}\n",
        "\n",
        "# Define the RandomizedSearchCV object\n",
        "rs_cv = RandomizedSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_distributions=params,\n",
        "    n_iter=10,\n",
        "    cv=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the data\n",
        "rs_cv.fit(df_x_select, df_y)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(rs_cv.best_params_)\n",
        "\n",
        "# Use the best hyperparameters to train the final model\n",
        "final_xgb_model = xgb.XGBRegressor(**rs_cv.best_params_)\n",
        "final_xgb_model.fit(df_x_select, df_y)\n",
        "\n",
        "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
        "mse_train = mean_squared_error(df_y, final_xgb_model.predict(df_x_select))\n",
        "r2_train = r2_score(df_y, final_xgb_model.predict(df_x_select))\n",
        "mse_test = mean_squared_error(df_test_y, final_xgb_model.predict(df_test_x_select))\n",
        "r2_test = r2_score(df_test_y, final_xgb_model.predict(df_test_x_select))\n",
        "\n",
        "# Print the training MSE and R2 score\n",
        "print('Training MSE: ', mse_train)\n",
        "print('Training R2: ', r2_train)\n",
        "\n",
        "# Print the testing MSE and R2 score\n",
        "print('Testing MSE: ', mse_test)\n",
        "print('Testing R2: ', r2_test)\n",
        "\n",
        "# Display the scores\n",
        "plot_scores(mse_train, r2_train, mse_test, r2_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# SVM hyperparameters\n",
        "svm_params = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "    'degree': [2, 3, 4],\n",
        "    'gamma': ['scale', 'auto'] + list(np.logspace(-3, 3, 7)),\n",
        "    'coef0': [-1, 0, 1]\n",
        "}\n",
        "\n",
        "# Define the SVM model\n",
        "svm_model = SVR()\n",
        "\n",
        "# Set up grid search with cross-validation\n",
        "grid_search = GridSearchCV(svm_model, svm_params, cv=5, n_jobs=2)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(df_x_select, df_y)\n",
        "\n",
        "# Print the best hyperparameters found by the grid search\n",
        "print(\"Best SVM hyperparameters: \", grid_search.best_params_)\n",
        "\n",
        "\n",
        "# Use the best hyperparameters to train the final model\n",
        "final_svr_model = SVR(**rs_cv.best_params_)\n",
        "final_svr_model.fit(df_x_select, df_y)\n",
        "\n",
        "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
        "mse_train = mean_squared_error(df_y, final_svr_model.predict(df_x_select))\n",
        "r2_train = r2_score(df_y, final_svr_model.predict(df_x_select))\n",
        "mse_test = mean_squared_error(df_test_y, final_svr_model.predict(df_test_x_select))\n",
        "r2_test = r2_score(df_test_y, final_svr_model.predict(df_test_x_select))\n",
        "\n",
        "# Print the training MSE and R2 score\n",
        "print('Training MSE: ', mse_train)\n",
        "print('Training R2: ', r2_train)\n",
        "\n",
        "# Print the testing MSE and R2 score\n",
        "print('Testing MSE: ', mse_test)\n",
        "print('Testing R2: ', r2_test)\n",
        "\n",
        "# Display the scores\n",
        "plot_scores(mse_train, r2_train, mse_test, r2_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "57bc2b6ce032b5f0e93daa91901b7ea38a856826ef43aa9e95b6d3999f5310df"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
