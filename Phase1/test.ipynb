{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test & Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import cv2\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origin = pd.read_csv('df_test.csv', parse_dates=['Original Release Date' , 'Current Version Release Date'])\n",
    "df_old = pd.read_csv('preprocessed_data.csv')\n",
    "df_old = df_old.drop(['Average User Rating'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Primary Genre\n",
    "df_origin.drop(['Primary Genre', 'ID', 'URL'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the icons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, filename):\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(filename, 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "\n",
    "def download_icons(df):\n",
    "    # Convert to string\n",
    "    df_origin['Icon URL'] = df_origin['Icon URL'].astype(str)\n",
    "\n",
    "    # Create a folder to store the images\n",
    "    if not os.path.exists('icons_test'):\n",
    "        os.makedirs('icons_test')\n",
    "\n",
    "    # Download the images\n",
    "    for i, row in tqdm(df_origin.iterrows(), total=df_origin.shape[0]):\n",
    "        if not os.path.exists(f'icons_test/{i}.png'):\n",
    "            download_image(row['Icon URL'], f'icons_test/{i}.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_reviews = pickle.load(open('encoders/sia_reviews.pkl', 'rb'))\n",
    "\n",
    "def reviews_preprocess(data):\n",
    "  data['Reviews'] = data['Reviews'].astype(str)\n",
    "  data['Reviews'] = data['Reviews'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "  \n",
    "  # Apply sentiment analysis for each review in the list\n",
    "  data['Reviews'] = data['Reviews'].apply(lambda x: [sia_reviews.polarity_scores(review)['compound'] for review in x])\n",
    "  \n",
    "  \n",
    "  # Get the lowest, highest and average Reviews\n",
    "  data['lowest_review'] = data['Reviews'].apply(lambda x: min(x) if len(x) > 0 else 0)\n",
    "  data['highest_review'] = data['Reviews'].apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "  data['average_review'] = data['Reviews'].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
    "  \n",
    "  ## Drop nulls of sentiment\n",
    "  # data = data.dropna(subset=['lowest_review','highest_review','average_review'])\n",
    "  return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates, Developer, Age Rating, Purchases and Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dates\n",
    "\n",
    "def date_preprocess(_df):\n",
    "    # Convert the datetime to ordinal\n",
    "    _df['Original Release Date'] = _df['Original Release Date'].apply(lambda x: x.toordinal())\n",
    "    _df['Current Version Release Date'] = _df['Current Version Release Date'].apply(lambda x: x.toordinal())\n",
    "    \n",
    "    # Fill the missing values with the median\n",
    "    _df['Original Release Date'] = _df['Original Release Date'].fillna(df_old['Original Release Date'].median())\n",
    "    _df['Current Version Release Date'] = _df['Current Version Release Date'].fillna(df_old['Current Version Release Date'].median())\n",
    "    \n",
    "    # Create a new column with the age of the game\n",
    "    _df['game_age'] = datetime.now().toordinal() - _df['Original Release Date']\n",
    "\n",
    "    # Create a new column with the time since the last update\n",
    "    _df['last_update'] = datetime.now().toordinal() - _df['Current Version Release Date']\n",
    "    \n",
    "    # Create a new column with the maintaning period\n",
    "    _df['maintaning_period'] = _df['last_update'] - _df['game_age']\n",
    "\n",
    "    return _df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Developer\n",
    "\n",
    "def dev_preprocess_target_enc(_df):\n",
    "    # Convert to string\n",
    "    _df['Developer'] = _df['Developer'].astype(str)\n",
    "    _df['Developer'] = _df['Developer'].str.replace(\"'\", \"\").str.strip('[]')\n",
    "    \n",
    "    dev_df = pd.read_csv('encoders/dev_avg.csv')\n",
    "    dev_df['Average User Rating'] = dev_df['Average User Rating'].astype(float)\n",
    "    dev_df['Developer'] = dev_df['Developer'].astype(str)\n",
    "\n",
    "    # Get developers that are unseen in the test set\n",
    "    unseen_dev = _df['Developer'][~_df['Developer'].isin(dev_df['Developer'])].unique()\n",
    "    \n",
    "    # Replace the unseen developers with the 'Other'\n",
    "    _df['Developer'] = _df['Developer'].replace(unseen_dev, 'Other')\n",
    "\n",
    "    # Replace the developer with the rating from dev_df\n",
    "    _df['dev_avg'] = _df['Developer'].replace(dev_df.set_index('Developer')['Average User Rating'])\n",
    "    \n",
    "    return _df\n",
    "\n",
    "\n",
    "def dev_preprocess_freq_enc(_df):\n",
    "    # Convert to string\n",
    "    _df['Developer'] = _df['Developer'].astype(str)\n",
    "    _df['Developer'] = _df['Developer'].str.replace(\"'\", \"\").str.strip('[]')\n",
    "    \n",
    "    # Get the frequency of each developer\n",
    "    dev_freqs = pd.read_csv('encoders/dev_freq.csv')\n",
    "    \n",
    "    # Create a dictionary mapping developers to frequencies\n",
    "    dev_freq_dict = dict(zip(dev_freqs['Developer'], dev_freqs['Frequency']))\n",
    "\n",
    "    # Create a new column with the frequency of each developer\n",
    "    _df['dev_freq'] = _df['Developer'].map(dev_freq_dict)\n",
    "    \n",
    "    # Fill the missing values with 1 (the minimum frequency)\n",
    "    _df['dev_freq'] = _df['dev_freq'].fillna(1)\n",
    "        \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Age Rating\n",
    "\n",
    "def age_preprocess(_df):\n",
    "    # Convert to string\n",
    "    _df['Age Rating'] = _df['Age Rating'].astype(str)\n",
    "\n",
    "    # Remove the + sign\n",
    "    _df['Age Rating'] = _df['Age Rating'].str.replace('+', '')\n",
    "\n",
    "    # Convert to int\n",
    "    _df['Age Rating'] = _df['Age Rating'].astype(float)\n",
    "\n",
    "    # fill the missing values with the median\n",
    "    _df['Age Rating'] = _df['Age Rating'].fillna(df_old['Age Rating'].median())\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Purchases\n",
    "\n",
    "# Free apps might skew the in-app purchases column,\n",
    "# so we might split the dataset into free and paid apps\n",
    "\n",
    "def purchases_preprocess(_df):\n",
    "    # Convert to string\n",
    "\n",
    "    _df['In-app Purchases'] = _df['In-app Purchases'].astype(str)\n",
    "    _df['In-app Purchases'] = _df['In-app Purchases'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "    # Convert to float\n",
    "    _df['In-app Purchases'] = _df['In-app Purchases'].apply(lambda x: [float(i) for i in x])\n",
    "\n",
    "    # Get the number of in-app purchases\n",
    "    _df['purchases_count'] = _df['In-app Purchases'].apply(lambda x: len(x))\n",
    "\n",
    "    # Get the lowest, highest and average purchase\n",
    "    _df['lowest_purchase'] = _df['In-app Purchases'].apply(lambda x: min(x) if len(x) > 0 else 0)\n",
    "    _df['highest_purchase'] = _df['In-app Purchases'].apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "    _df['average_purchase'] = _df['In-app Purchases'].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
    "\n",
    "    _df['lowest_purchase'] = _df['lowest_purchase'].fillna(0)\n",
    "    _df['highest_purchase'] = _df['highest_purchase'].fillna(0)\n",
    "    _df['average_purchase'] = _df['average_purchase'].fillna(0)\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Price\n",
    "\n",
    "def price_preprocess(_df):\n",
    "    # Convert to float\n",
    "    _df['Price'] = _df['Price'].astype(float)\n",
    "\n",
    "    # fill the missing values with 0 (free)\n",
    "    _df['Price'] = _df['Price'].fillna(0)\n",
    "    \n",
    "    return _df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genres & Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genres_preprocess_dummies(_df):\n",
    "    # Convert to string\n",
    "    _df['Genres'] = _df['Genres'].astype(str)\n",
    "    _df['Genres'] = _df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "    # drop Games, Strategy, Entertainment from the Genres column\n",
    "    _df['Genres'] = _df['Genres'].apply(lambda x: [genre for genre in x if genre not in ['Games', 'Strategy', 'Entertainment']])\n",
    "\n",
    "    # Load saved genres dummy variables\n",
    "    saved_dummies = pd.read_csv('encoders/genres.csv')\n",
    "\n",
    "    # Get the genres that are not in the saved dummy variables\n",
    "    other = [genre for genre in _df['Genres'].explode().unique() if genre not in saved_dummies.columns]\n",
    "\n",
    "    # Replace the genres that are not in the saved dummy variables with 'infrequent'\n",
    "    _df['Genres'] = _df['Genres'].apply(lambda x: ['infrequent' if genre in other else genre for genre in x])\n",
    "\n",
    "    # Preprocess test data using the saved dummy variables\n",
    "    test_dummies = pd.get_dummies(_df['Genres'].apply(pd.Series).stack(), prefix=\"genre\", dummy_na=False).sum(level=0)\n",
    "    test_dummies = test_dummies.reindex(columns=saved_dummies.columns, fill_value=0)\n",
    "    \n",
    "    # Fill the dummy columns with 0 if nan\n",
    "    test_dummies = test_dummies.fillna(0)\n",
    "\n",
    "    # Add the new dummy variables to the test data\n",
    "    _df = pd.concat([_df, test_dummies], axis=1)\n",
    "    \n",
    "    # Fill NaN with 0\n",
    "    genre_cols = [col for col in _df.columns if col.startswith('genre')] # get all columns with prefix 'genre'\n",
    "    _df[genre_cols] = _df[genre_cols].fillna(0) # fill NaN with 0 for selected columns\n",
    "    \n",
    "    return _df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langs_preprocess_dummies(_df):\n",
    "    # Convert the langs column to a list of strings\n",
    "    _df['Languages'] = _df['Languages'].astype(str)\n",
    "    _df['Languages'] = _df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "    # Create a column with the number of languages supported\n",
    "    _df['langs_count'] = _df['Languages'].apply(lambda x: len(x))\n",
    "\n",
    "    # Drop the English language from the Languages column (it is the most common language and would dominate the model)\n",
    "    _df['Languages'] = _df['Languages'].apply(lambda x: [lang for lang in x if lang not in ['EN']])\n",
    "\n",
    "    # Load saved languages dummy variables\n",
    "    saved_dummies = pd.read_csv('encoders/langs.csv')\n",
    "\n",
    "    # Get the languages that are not in the saved dummy variables\n",
    "    other = [lang for lang in _df['Languages'].explode().unique() if lang not in saved_dummies.columns]\n",
    "\n",
    "    # Replace the languages that are not in the saved dummy variables with 'infrequent'\n",
    "    _df['Languages'] = _df['Languages'].apply(lambda x: ['infrequent' if lang in other else lang for lang in x])\n",
    "\n",
    "    # Preprocess test data using the saved dummy variables\n",
    "    test_dummies = pd.get_dummies(_df['Languages'].apply(pd.Series).stack(), prefix=\"lang\", dummy_na=False).sum(level=0)\n",
    "    test_dummies = test_dummies.reindex(columns=saved_dummies.columns, fill_value=0)\n",
    "\n",
    "    # Fill the dummy columns with 0 if nan\n",
    "    test_dummies = test_dummies.fillna(0)\n",
    "\n",
    "    # Concatenate dummies to original DataFrame\n",
    "    _df = pd.concat([_df, test_dummies], axis=1)\n",
    "\n",
    "    # Fill NaN with 0\n",
    "    lang_cols = [col for col in _df.columns if col.startswith('lang')] # get all columns with prefix 'lang'\n",
    "    _df[lang_cols] = _df[lang_cols].fillna(0) # fill NaN with 0 for selected columns\n",
    "    \n",
    "    return _df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "def preprocess_nlp(_df, col):\n",
    "    _df[col] = _df[col].astype(str)\n",
    "\n",
    "    # Remove URLs and email addresses\n",
    "    _df[col] = _df[col].apply(lambda x: re.sub(r'http\\S+|www.\\S+|\\S+@\\S+', '', x))\n",
    "\n",
    "    # Remove the punctuation, numbers, and convert to lowercase\n",
    "    _df[col] = _df[col].apply(lambda x: \" \".join(re.findall(r'\\w+', x.lower())))\n",
    "\n",
    "    # Remove the stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    _df[col] = _df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "    # Stemming\n",
    "    st = nltk.PorterStemmer()\n",
    "    _df[col] = _df[col].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "    # Lemmatization\n",
    "    lem = nltk.WordNetLemmatizer()\n",
    "    _df[col] = _df[col].apply(lambda x: \" \".join([lem.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "    # Remove the frequent and rare words\n",
    "    freq = pd.Series(' '.join(_df[col]).split()).value_counts()\n",
    "    common_freq = list(freq[:10].index)\n",
    "    rare_freq = list(freq[-10:].index)\n",
    "    _df[col] = _df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in common_freq+rare_freq))\n",
    "\n",
    "    # Remove the whitespaces\n",
    "    _df[col] = _df[col].apply(lambda x: \" \".join(x.strip() for x in x.split()))\n",
    "\n",
    "    # Replace NaN values with empty string\n",
    "    _df[col] = _df[col].fillna('')\n",
    "\n",
    "    # Convert text data to bag-of-words representation\n",
    "    vectorizer = pickle.load(open(f'encoders/vectorizer_{col}.pkl', 'rb'))\n",
    "    BoW = vectorizer.transform(_df[col])\n",
    "\n",
    "    # Apply principal component analysis to reduce the dimensionality\n",
    "    pca_ = pickle.load(open(f'encoders/pca_{col}.pkl', 'rb'))\n",
    "    pca_col = pca_.transform(BoW.toarray())\n",
    "\n",
    "    # Add the PCA-transformed col to the original dataframe\n",
    "    for feat in range(len(pca_col[0])):\n",
    "        _df[f'{col}_PCA_{feat}'] = pca_col[:, feat]\n",
    "        \n",
    "    return _df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Icon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_icon(img_path):\n",
    "    \n",
    "        # Load the game icon image\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "\n",
    "    # Extract color features using color histograms\n",
    "    img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    color_features = []\n",
    "    for i in range(3):\n",
    "        hist = cv2.calcHist([img_lab], [i], None, [256], [0, 256])\n",
    "        color_features.append(hist.ravel())\n",
    "    color_features = np.concatenate(color_features)\n",
    "\n",
    "    # Extract shape features using local binary patterns\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    lbp = local_binary_pattern(gray, 8, 1, method='uniform')\n",
    "    hist_lbp, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 10), range=(0, 9))\n",
    "    edge_features = hist_lbp.astype(float)\n",
    "\n",
    "    # Combine the color and shape features into a single feature vector\n",
    "    feature_vector = np.concatenate((color_features, edge_features))\n",
    "\n",
    "    # Normalize the feature vector to have unit length\n",
    "    normalized_feature_vector = feature_vector / np.linalg.norm(feature_vector)\n",
    "    \n",
    "    return normalized_feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def icons_preprocess(_df):\n",
    "    # Create a list to store the feature vectors\n",
    "    icon_features = []\n",
    "\n",
    "    _df['Icon URL'] = _df['Icon URL'].astype(str)\n",
    "\n",
    "    # Iterate over the images and extract the features\n",
    "    for i, row in tqdm(_df.iterrows(), total=_df.shape[0]):\n",
    "        feature_vec = preprocess_icon(row['Icon URL'])\n",
    "        icon_features.append((row['Icon URL'], feature_vec))\n",
    "        \n",
    "    # Apply PCA to reduce the number of features\n",
    "    pca = pickle.load(open('encoders/icon_pca.pkl', 'rb'))\n",
    "\n",
    "    reduced_features = pca.transform([f[1] for f in icon_features])\n",
    "\n",
    "    # Convert the reduced features to a dataframe\n",
    "    icon_features_df = pd.DataFrame({'Icon URL': [f[0] for f in icon_features],\n",
    "                                        'Icon1': reduced_features[:,0],\n",
    "                                        'Icon2': reduced_features[:,1],\n",
    "                                        'Icon3': reduced_features[:,2],\n",
    "                                        'Icon4': reduced_features[:,3]})\n",
    "\n",
    "    # Merge the icon features with the original dataframe on the icon URL\n",
    "    _df = _df.merge(icon_features_df, on='Icon URL', how='left')\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Sentiment Intensity Analyzer models\n",
    "sia_desc = pickle.load(open('encoders/sia_desc.pkl', 'rb'))\n",
    "sia_name = pickle.load(open('encoders/sia_name.pkl', 'rb'))\n",
    "sia_sub = pickle.load(open('encoders/sia_sub.pkl', 'rb'))\n",
    "\n",
    "# tokenize the text into words and count how many attractive keywords appear\n",
    "tokenizer = pickle.load(open('encoders/nltk_word_tokenize.pkl', 'rb'))\n",
    "\n",
    "def compute_excitement_score(text, _sia):\n",
    "    \n",
    "    # compute the polarity scores for the given text\n",
    "    scores = _sia.polarity_scores(text)\n",
    "    \n",
    "    # compute the excitement score as the sum of the positive and negative polarity scores\n",
    "    excitement_score = scores['pos'] + abs(scores['neg'])\n",
    "    \n",
    "    return excitement_score\n",
    "\n",
    "# define a function to compute an attractive score from a given text\n",
    "def compute_attractive_score(text):\n",
    "    # define a list of keywords that might make a game attractive to users\n",
    "    attractive_keywords = ['graphics', 'gameplay', 'storyline', 'characters']\n",
    "    \n",
    "    words = tokenizer(text.lower())\n",
    "    num_attractive_keywords = len([word for word in words if word in attractive_keywords])\n",
    "    \n",
    "    # compute the attractive score as the ratio of attractive keywords to total words\n",
    "    attractive_score = num_attractive_keywords / len(words) if len(words) > 0 else 0\n",
    "    \n",
    "    return attractive_score\n",
    "\n",
    "def desc_preprocess(_df):\n",
    "    _df['Description'] = _df['Description'].astype(str)\n",
    "    \n",
    "    _df['excitement_score'] = _df['Description'].apply(lambda x : compute_excitement_score(x, sia_desc))\n",
    "    _df['attractive_score'] = _df['Description'].apply(compute_attractive_score)\n",
    "    \n",
    "    return _df\n",
    "\n",
    "def name_preprocess(_df):\n",
    "    _df['Name'] = _df['Name'].astype(str)\n",
    "    _df['name_sia'] = _df['Name'].apply(lambda x : compute_excitement_score(x, sia_name))\n",
    "    \n",
    "    return _df\n",
    "\n",
    "def sub_preprocess(_df):\n",
    "    _df['Subtitle'] = _df['Subtitle'].astype(str)\n",
    "    _df['sub_sia'] = _df['Subtitle'].apply(lambda x : compute_excitement_score(x, sia_sub))\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_origin.drop(['Average User Rating'], axis=1)\n",
    "df_y = df_origin['Average User Rating']\n",
    "\n",
    "df = date_preprocess(df)\n",
    "\n",
    "df = dev_preprocess_freq_enc(df)\n",
    "\n",
    "df = dev_preprocess_target_enc(df)\n",
    "\n",
    "df = genres_preprocess_dummies(df)\n",
    "\n",
    "df = langs_preprocess_dummies(df)\n",
    "\n",
    "df = purchases_preprocess(df)\n",
    "\n",
    "df = age_preprocess(df)\n",
    "\n",
    "df = price_preprocess(df)\n",
    "\n",
    "df = preprocess_nlp(df, 'Description')\n",
    "df = preprocess_nlp(df, 'Subtitle')\n",
    "df = preprocess_nlp(df, 'Name')\n",
    "\n",
    "df = desc_preprocess(df)\n",
    "df = name_preprocess(df)\n",
    "df = sub_preprocess(df)\n",
    "\n",
    "df = reviews_preprocess(df)\n",
    "\n",
    "df = icons_preprocess(df)\n",
    "\n",
    "df = df.drop(['Developer',\n",
    "              'Original Release Date',\n",
    "              'Current Version Release Date',\n",
    "              'Genres',\n",
    "              'Languages',\n",
    "              'Reviews',\n",
    "              'In-app Purchases',\n",
    "              'Description',\n",
    "              'Subtitle',\n",
    "              'Name', \n",
    "              'Icon URL'], axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reindex(columns=df_old.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns\n",
    "\n",
    "scaler = pickle.load(open('scalers/std_scaler.pkl', 'rb'))\n",
    "df = scaler.transform(df)\n",
    "\n",
    "df = pd.DataFrame(df, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation matrix between features and target\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr = df.corrwith(df_y)\n",
    "corr = corr.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x=corr.values, y=corr.index)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = pickle.load(open('encoders/selector.pkl', 'rb'))\n",
    "df = selector.transform(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear Regression\n",
    "\n",
    "lr_model = pickle.load(open('models/LR_model.pkl', 'rb'))\n",
    "lr_pred = lr_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "lr_mse = mean_squared_error(df_y, lr_pred)\n",
    "lr_r2 = r2_score(df_y, lr_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Linear Regression MSE: {lr_mse}')\n",
    "print(f'Linear Regression R2: {lr_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lasso Regression\n",
    "\n",
    "lasso_model = pickle.load(open('models/Lasso_model.pkl', 'rb'))\n",
    "lasso_pred = lasso_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "lasso_mse = mean_squared_error(df_y, lasso_pred)\n",
    "lasso_r2 = r2_score(df_y, lasso_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Lasso Regression MSE: {lasso_mse}')\n",
    "print(f'Lasso Regression R2: {lasso_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ridge Regression\n",
    "\n",
    "ridge_model = pickle.load(open('models/Ridge_model.pkl', 'rb'))\n",
    "ridge_pred = ridge_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "ridge_mse = mean_squared_error(df_y, ridge_pred)\n",
    "ridge_r2 = r2_score(df_y, ridge_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Ridge Regression MSE: {ridge_mse}')\n",
    "print(f'Ridge Regression R2: {ridge_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ElasticNet Regression\n",
    "\n",
    "en_model = pickle.load(open('models/ElasticNet_model.pkl', 'rb'))\n",
    "en_pred = en_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "en_mse = mean_squared_error(df_y, en_pred)\n",
    "en_r2 = r2_score(df_y, en_pred)\n",
    " \n",
    "# Print the results\n",
    "print(f'ElasticNet Regression MSE: {en_mse}')\n",
    "print(f'ElasticNet Regression R2: {en_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "poly_model = pickle.load(open('models/Polynomial_model.pkl', 'rb'))\n",
    "poly_features = pickle.load(open('encoders/poly.pkl', 'rb'))\n",
    "\n",
    "poly_pred = poly_model.predict(poly_features.transform(df))\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "poly_mse = mean_squared_error(df_y, poly_pred)\n",
    "poly_r2 = r2_score(df_y, poly_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Polynomial Regression MSE: {poly_mse}')\n",
    "print(f'Polynomial Regression R2: {poly_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest Regression\n",
    "\n",
    "rfg_model = pickle.load(open('models/RandomForest_model.pkl', 'rb'))\n",
    "\n",
    "rfg_pred = rfg_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "rfg_mse = mean_squared_error(df_y, rfg_pred)\n",
    "rfg_r2 = r2_score(df_y, rfg_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Random Forest Regression MSE: {rfg_mse}')\n",
    "print(f'Random Forest Regression R2: {rfg_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### XGBoost Regression\n",
    "\n",
    "xgb_model = pickle.load(open('models/XGBoost_model.pkl', 'rb'))\n",
    "\n",
    "xgb_pred = xgb_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "xgb_mse = mean_squared_error(df_y, xgb_pred)\n",
    "xgb_r2 = r2_score(df_y, xgb_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'XGBoost Regression MSE: {xgb_mse}')\n",
    "print(f'XGBoost Regression R2: {xgb_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gradient Boosting Regression\n",
    "\n",
    "gb_model = pickle.load(open('models/GradientBoosting_model.pkl', 'rb'))\n",
    "\n",
    "gb_pred = gb_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "gb_mse = mean_squared_error(df_y, gb_pred)\n",
    "gb_r2 = r2_score(df_y, gb_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Gradient Boosting Regression MSE: {gb_mse}')\n",
    "print(f'Gradient Boosting Regression R2: {gb_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CatBoost Regression\n",
    "\n",
    "cb_model = pickle.load(open('models/CatBoost_model.pkl', 'rb'))\n",
    "\n",
    "cb_pred = cb_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "cb_mse = mean_squared_error(df_y, cb_pred)\n",
    "cb_r2 = r2_score(df_y, cb_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'CatBoost Regression MSE: {cb_mse}')\n",
    "print(f'CatBoost Regression R2: {cb_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Regression\n",
    "\n",
    "ada_model = pickle.load(open('models/AdaBoost_model.pkl', 'rb'))\n",
    "\n",
    "ada_pred = ada_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "ada_mse = mean_squared_error(df_y, ada_pred)\n",
    "ada_r2 = r2_score(df_y, ada_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'AdaBoost Regression MSE: {ada_mse}')\n",
    "print(f'AdaBoost Regression R2: {ada_r2}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcadams",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
