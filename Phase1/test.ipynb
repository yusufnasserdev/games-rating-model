{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test & Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "dateparse = lambda x: datetime.strptime(x, '%d/%m/%Y')\n",
    "\n",
    "df_origin = pd.read_csv('df_test.csv', parse_dates=['Original Release Date' , 'Current Version Release Date'], date_parser=dateparse)\n",
    "df_old = pd.read_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Primary Genre\n",
    "df_origin.drop(['Primary Genre', 'ID', 'URL'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the icons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to string\n",
    "df_origin['Icon URL'] = df_origin['Icon URL'].astype(str)\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def download_image(url, filename):\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(filename, 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "\n",
    "# Create a folder to store the images\n",
    "if not os.path.exists('icons_test'):\n",
    "    os.makedirs('icons_test')\n",
    "\n",
    "# Download the images\n",
    "for i, row in df_origin.iterrows():\n",
    "    download_image(row['Icon URL'], f'icons_test/{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the URL with the icon filename which is the index of the row\n",
    "df_origin['Icon URL'] = df_origin.apply(lambda row : f'icons_test/{row.name}.png', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_origin.drop(['Average User Rating'], axis=1)\n",
    "df_y = df_origin['Average User Rating']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates, Developer, Age Rating, Purchases and Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dates\n",
    "\n",
    "# Convert the datetime to ordinal\n",
    "df['Original Release Date'] = df['Original Release Date'].apply(lambda x: x.toordinal())\n",
    "df['Current Version Release Date'] = df['Current Version Release Date'].apply(lambda x: x.toordinal())\n",
    "\n",
    "df['Original Release Date'] = df['Original Release Date'].fillna(df_old['Original Release Date'].median())\n",
    "df['Current Version Release Date'] = df['Current Version Release Date'].fillna(df_old['Current Version Release Date'].median())\n",
    "\n",
    "# Create a new column with the age of the game\n",
    "df['game_age'] = df['Current Version Release Date'] - df['Original Release Date']\n",
    "\n",
    "# Create a new column with the time since the last update\n",
    "df['last_update'] = datetime.now().toordinal() - df['Current Version Release Date'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Developer\n",
    "\n",
    "dev_df = pd.read_csv('dev_df.csv')\n",
    "\n",
    "# Replace the developer names with the average user rating from dev_df\n",
    "df['Developer'] = df['Developer'].replace(dev_df.index, dev_df['Average User Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Age Rating\n",
    "\n",
    "# Convert to string\n",
    "df['Age Rating'] = df['Age Rating'].astype(str)\n",
    "\n",
    "# Remove the + sign\n",
    "df['Age Rating'] = df['Age Rating'].str.replace('+', '')\n",
    "\n",
    "# Convert to int\n",
    "df['Age Rating'] = df['Age Rating'].astype(float)\n",
    "\n",
    "# fill the missing values with the median\n",
    "df['Age Rating'] = df['Age Rating'].fillna(df_old['Age Rating'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Purchases\n",
    "\n",
    "# Free apps might skew the in-app purchases column,\n",
    "# so we might split the dataset into free and paid apps\n",
    "\n",
    "df['In-app Purchases'] = df['In-app Purchases'].astype(str)\n",
    "df['In-app Purchases'] = df['In-app Purchases'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# Convert to float\n",
    "df['In-app Purchases'] = df['In-app Purchases'].apply(lambda x: [float(i) for i in x])\n",
    "\n",
    "# Get the number of in-app purchases\n",
    "df['purchases_count'] = df['In-app Purchases'].apply(lambda x: len(x))\n",
    "\n",
    "# Get the lowest, highest and average purchase\n",
    "df['lowest_purchase'] = df['In-app Purchases'].apply(lambda x: min(x) if len(x) > 0 else 0)\n",
    "df['highest_purchase'] = df['In-app Purchases'].apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "df['average_purchase'] = df['In-app Purchases'].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
    "\n",
    "# Drop the original column\n",
    "df = df.drop(['In-app Purchases'], axis=1)\n",
    "\n",
    "df['lowest_purchase'] = df['lowest_purchase'].fillna(0)\n",
    "df['highest_purchase'] = df['highest_purchase'].fillna(0)\n",
    "df['average_purchase'] = df['average_purchase'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Price\n",
    "df['Price'] = df['Price'].astype(float)\n",
    "df['Price'] = df['Price'].fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genres & Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the genres column to a list of strings\n",
    "df['Genres'] = df['Genres'].astype(str)\n",
    "df['Genres'] = df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# drop Games, Strategy, Entertainment from the Genres column\n",
    "df['Genres'] = df['Genres'].apply(lambda x: [genre for genre in x if genre not in ['Games', 'Strategy', 'Entertainment']])\n",
    "\n",
    "# Load saved genres dummy variables\n",
    "saved_dummies = pd.read_csv('encoders/genres.csv')\n",
    "\n",
    "# Get the genres that are not in the saved dummy variables\n",
    "other = [genre for genre in df['Genres'].explode().unique() if genre not in saved_dummies.columns]\n",
    "\n",
    "# Replace the genres that are not in the saved dummy variables with 'infrequent'\n",
    "df['Genres'] = df['Genres'].apply(lambda x: ['infrequent' if genre in other else genre for genre in x])\n",
    "\n",
    "# Preprocess test data using the saved dummy variables\n",
    "test_dummies = pd.get_dummies(df['Genres'].apply(pd.Series).stack(), prefix=\"genre\", dummy_na=False).sum(level=0)\n",
    "test_dummies = test_dummies.reindex(columns=saved_dummies.columns, fill_value=0)\n",
    "\n",
    "# Fill the dummy columns with 0 if nan\n",
    "test_dummies = test_dummies.fillna(0)\n",
    "\n",
    "# Concatenate dummies to original DataFrame\n",
    "df = pd.concat([df, test_dummies], axis=1)\n",
    "\n",
    "# Fill NaN with 0\n",
    "genre_cols = [col for col in df.columns if col.startswith('genre')] # get all columns with prefix 'genre'\n",
    "df[genre_cols] = df[genre_cols].fillna(0) # fill NaN with 0 for selected columns\n",
    "\n",
    "# Drop the original Genres column\n",
    "df = df.drop('Genres', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the langs column to a list of strings\n",
    "df['Languages'] = df['Languages'].astype(str)\n",
    "df['Languages'] = df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# Drop the English language from the Languages column (it is the most common language and would dominate the model)\n",
    "df['Languages'] = df['Languages'].apply(lambda x: [lang for lang in x if lang not in ['EN']])\n",
    "\n",
    "# Load saved languages dummy variables\n",
    "saved_dummies = pd.read_csv('encoders/langs.csv')\n",
    "\n",
    "# Get the languages that are not in the saved dummy variables\n",
    "other = [lang for lang in df['Languages'].explode().unique() if lang not in saved_dummies.columns]\n",
    "\n",
    "# Replace the languages that are not in the saved dummy variables with 'infrequent'\n",
    "df['Languages'] = df['Languages'].apply(lambda x: ['infrequent' if lang in other else lang for lang in x])\n",
    "\n",
    "# Preprocess test data using the saved dummy variables\n",
    "test_dummies = pd.get_dummies(df['Languages'].apply(pd.Series).stack(), prefix=\"lang\", dummy_na=False).sum(level=0)\n",
    "test_dummies = test_dummies.reindex(columns=saved_dummies.columns, fill_value=0)\n",
    "\n",
    "# Fill the dummy columns with 0 if nan\n",
    "test_dummies = test_dummies.fillna(0)\n",
    "\n",
    "# Fill NaN with 0\n",
    "lang_cols = [col for col in df.columns if col.startswith('lang')] # get all columns with prefix 'lang'\n",
    "df[lang_cols] = df[lang_cols].fillna(0) # fill NaN with 0 for selected columns\n",
    "\n",
    "# Concatenate dummies to original DataFrame\n",
    "df = pd.concat([df, test_dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))\n",
    "\n",
    "# print the features weights\n",
    "for i in range(len(model.coef_)):\n",
    "    print(f'Feature {selected_features[i]}: {model.coef_[i]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "def preprocess_nlp(col):\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "    # Remove URLs and email addresses\n",
    "    df[col] = df[col].apply(lambda x: re.sub(r'http\\S+|www.\\S+|\\S+@\\S+', '', x))\n",
    "\n",
    "    # Remove the punctuation, numbers, and convert to lowercase\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(re.findall(r'\\w+', x.lower())))\n",
    "\n",
    "    # Remove the stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "    # Stemming\n",
    "    st = nltk.PorterStemmer()\n",
    "    df[col] = df[col].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "    # Lemmatization\n",
    "    lem = nltk.WordNetLemmatizer()\n",
    "    df[col] = df[col].apply(lambda x: \" \".join([lem.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "    # Remove the frequent and rare words\n",
    "    freq = pd.Series(' '.join(df[col]).split()).value_counts()\n",
    "    common_freq = list(freq[:10].index)\n",
    "    rare_freq = list(freq[-10:].index)\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in common_freq+rare_freq))\n",
    "\n",
    "    # Remove the whitespaces\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(x.strip() for x in x.split()))\n",
    "\n",
    "    # Replace NaN values with empty string\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "    # Convert text data to bag-of-words representation\n",
    "    vectorizer = pickle.load(open(f'encoders/vectorizer_{col}.pkl', 'rb'))\n",
    "    BoW = vectorizer.transform(df[col])\n",
    "\n",
    "    # Apply principal component analysis to reduce the dimensionality\n",
    "    pca_ = pickle.load(open(f'encoders/pca_{col}.pkl', 'rb'))\n",
    "    pca_col = pca_.transform(BoW.toarray())\n",
    "\n",
    "    # Add the PCA-transformed col to the original dataframe\n",
    "    for feat in range(len(pca_col[0])):\n",
    "        df[f'{col}_PCA_{feat}'] = pca_col[:, feat]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_nlp('Description')\n",
    "preprocess_nlp('Subtitle')\n",
    "preprocess_nlp('Name')\n",
    "\n",
    "df = df.drop(['Description', 'Subtitle', 'Name'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Icon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess_icon(img_path):\n",
    "    # Load the game icon image\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (100, 100))\n",
    "\n",
    "    # Extract color features using color histograms\n",
    "    colors = ('b', 'g', 'r')\n",
    "    color_features = []\n",
    "    for k, col in enumerate(colors):\n",
    "        hist = cv2.calcHist([img], [k], None, [256], [0, 256])\n",
    "        color_features.append(hist)\n",
    "\n",
    "    # Reshape the color features to have a single dimension\n",
    "    color_features = np.concatenate(color_features).ravel()\n",
    "\n",
    "    # Extract shape features using edge detection\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "    edge_features = np.array(edges).flatten()\n",
    "\n",
    "    # Combine the color and shape features into a single feature vector\n",
    "    feature_vector = np.concatenate((color_features, edge_features))\n",
    "\n",
    "    # Normalize the feature vector to have unit length\n",
    "    normalized_feature_vector = feature_vector / np.linalg.norm(feature_vector)\n",
    "    \n",
    "    return normalized_feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Create a list to store the feature vectors\n",
    "icon_features = []\n",
    "\n",
    "df['Icon URL'] = df['Icon URL'].astype(str)\n",
    "\n",
    "# Iterate over the images and extract the features\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    feature_vec = preprocess_icon(row['Icon URL'])\n",
    "    icon_features.append((row['Icon URL'], feature_vec))\n",
    "    \n",
    "# Apply PCA to reduce the number of features\n",
    "pca = pickle.load(open('encoders/icon_pca.pkl', 'rb'))\n",
    "\n",
    "reduced_features = pca.transform([f[1] for f in icon_features])\n",
    "\n",
    "# Convert the reduced features to a dataframe\n",
    "icon_features_df = pd.DataFrame({'Icon URL': [f[0] for f in icon_features],\n",
    "                                    'Icon1': reduced_features[:,0],\n",
    "                                    'Icon2': reduced_features[:,1],\n",
    "                                    'Icon3': reduced_features[:,2],\n",
    "                                    'Icon4': reduced_features[:,3]})\n",
    "\n",
    "# Merge the icon features with the original dataframe on the icon URL\n",
    "df = df.merge(icon_features_df, on='Icon URL', how='left')\n",
    "\n",
    "# Drop the icon URL column\n",
    "df = df.drop(['Icon URL'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns\n",
    "\n",
    "scaler = pickle.load(open('scalers/std_scaler.pkl', 'rb'))\n",
    "df = scaler.transform(df)\n",
    "\n",
    "df = pd.DataFrame(df, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = pickle.load(open('encoders/selector.pkl', 'rb'))\n",
    "df = selector.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Models\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#### Linear Regression\n",
    "lr_model = pickle.load(open('models/LR_model.pkl', 'rb'))\n",
    "lr_pred = lr_model.predict(df)\n",
    "\n",
    "# Calculate the mean squared error and coefficient of determination\n",
    "print('Linear Regression')\n",
    "print('Mean squared error: %.2f' % mean_squared_error(df_y, lr_pred))\n",
    "print('Coefficient of determination: %.2f' % r2_score(df_y, lr_pred))\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "#### Ridge Regression\n",
    "ridge_model = pickle.load(open('models/Ridge_model.pkl', 'rb'))\n",
    "ridge_pred = ridge_model.predict(df)\n",
    "\n",
    "# Calculate the mean squared error and coefficient of determination\n",
    "print('Ridge Regression')\n",
    "print('Mean squared error: %.2f' % mean_squared_error(df_y, ridge_pred))\n",
    "print('Coefficient of determination: %.2f' % r2_score(df_y, ridge_pred))\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "#### Lasso Regression\n",
    "lasso_model = pickle.load(open('models/Lasso_model.pkl', 'rb'))\n",
    "lasso_pred = lasso_model.predict(df)\n",
    "\n",
    "# Calculate the mean squared error and coefficient of determination\n",
    "print('Lasso Regression')\n",
    "print('Mean squared error: %.2f' % mean_squared_error(df_y, lasso_pred))\n",
    "print('Coefficient of determination: %.2f' % r2_score(df_y, lasso_pred))\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "#### ElasticNet Regression\n",
    "elastic_model = pickle.load(open('models/ElasticNet_model.pkl', 'rb'))\n",
    "elastic_pred = elastic_model.predict(df)\n",
    "\n",
    "# Calculate the mean squared error and coefficient of determination\n",
    "print('ElasticNet Regression')\n",
    "print('Mean squared error: %.2f' % mean_squared_error(df_y, elastic_pred))\n",
    "print('Coefficient of determination: %.2f' % r2_score(df_y, elastic_pred))\n",
    "print('------------------------------------------------------------')\n",
    "\n",
    "#### Polynomial Regression\n",
    "poly_model = pickle.load(open('models/Polynomial_model.pkl', 'rb'))\n",
    "poly_features = pickle.load(open('encoders/poly.pkl', 'rb'))\n",
    "\n",
    "df_poly = poly_features.transform(df)\n",
    "poly_pred = poly_model.predict(df_poly)\n",
    "\n",
    "# Calculate the mean squared error and coefficient of determination\n",
    "print('Polynomial Regression')\n",
    "print('Mean squared error: %.2f' % mean_squared_error(df_y, poly_pred))\n",
    "print('Coefficient of determination: %.2f' % r2_score(df_y, poly_pred))\n",
    "print('------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcadams",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
