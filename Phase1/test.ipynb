{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test & Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "df_origin = pd.read_csv('df_test.csv', parse_dates=['Original Release Date' , 'Current Version Release Date'])\n",
    "df_old = pd.read_csv('preprocessed_data.csv')\n",
    "df_old = df_old.drop(['Average User Rating'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Primary Genre\n",
    "df_origin.drop(['Primary Genre', 'ID', 'URL'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the icons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to string\n",
    "df_origin['Icon URL'] = df_origin['Icon URL'].astype(str)\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def download_image(url, filename):\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(filename, 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "\n",
    "# Create a folder to store the images\n",
    "if not os.path.exists('icons_test'):\n",
    "    os.makedirs('icons_test')\n",
    "\n",
    "# Download the images\n",
    "for i, row in df_origin.iterrows():\n",
    "    download_image(row['Icon URL'], f'icons_test/{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the URL with the icon filename which is the index of the row\n",
    "df_origin['Icon URL'] = df_origin.apply(lambda row : f'icons/{row.name}.png', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_origin.drop(['Average User Rating'], axis=1)\n",
    "df_y = df_origin['Average User Rating']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates, Developer, Age Rating, Purchases and Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dates\n",
    "\n",
    "# Convert the datetime to ordinal\n",
    "df['Original Release Date'] = df['Original Release Date'].apply(lambda x: x.toordinal())\n",
    "df['Current Version Release Date'] = df['Current Version Release Date'].apply(lambda x: x.toordinal())\n",
    "\n",
    "df['Original Release Date'] = df['Original Release Date'].fillna(df_old['Original Release Date'].median())\n",
    "df['Current Version Release Date'] = df['Current Version Release Date'].fillna(df_old['Current Version Release Date'].median())\n",
    "\n",
    "# Create a new column with the age of the game\n",
    "df['game_age'] = df['Current Version Release Date'] - df['Original Release Date']\n",
    "\n",
    "# Create a new column with the time since the last update\n",
    "df['last_update'] = datetime.now().toordinal() - df['Current Version Release Date'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Developer\n",
    "\n",
    "# Convert to string\n",
    "df['Developer'] = df['Developer'].astype(str)\n",
    "df['Developer'] = df['Developer'].str.replace(\"'\", \"\").str.strip('[]')\n",
    "\n",
    "dev_df = pd.read_csv('encoders/dev_df.csv')\n",
    "dev_df['Average User Rating'] = dev_df['Average User Rating'].astype(float)\n",
    "dev_df['Developer'] = dev_df['Developer'].astype(str)\n",
    "\n",
    "# Get developers that are unseen in the test set\n",
    "unseen_dev = df['Developer'][~df['Developer'].isin(dev_df['Developer'])].unique()\n",
    "\n",
    "# Replace the unseen developers with the 'Other'\n",
    "df['Developer'] = df['Developer'].replace(unseen_dev, 'Other')\n",
    "\n",
    "# Replace the developer with the rating from dev_df\n",
    "df['Developer'] = df['Developer'].replace(dev_df.set_index('Developer')['Average User Rating'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Age Rating\n",
    "\n",
    "# Convert to string\n",
    "df['Age Rating'] = df['Age Rating'].astype(str)\n",
    "\n",
    "# Remove the + sign\n",
    "df['Age Rating'] = df['Age Rating'].str.replace('+', '')\n",
    "\n",
    "# Convert to int\n",
    "df['Age Rating'] = df['Age Rating'].astype(float)\n",
    "\n",
    "# fill the missing values with the median\n",
    "df['Age Rating'] = df['Age Rating'].fillna(df_old['Age Rating'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Purchases\n",
    "\n",
    "# Free apps might skew the in-app purchases column,\n",
    "# so we might split the dataset into free and paid apps\n",
    "\n",
    "df['In-app Purchases'] = df['In-app Purchases'].astype(str)\n",
    "df['In-app Purchases'] = df['In-app Purchases'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# Convert to float\n",
    "df['In-app Purchases'] = df['In-app Purchases'].apply(lambda x: [float(i) for i in x])\n",
    "\n",
    "# Get the number of in-app purchases\n",
    "df['purchases_count'] = df['In-app Purchases'].apply(lambda x: len(x))\n",
    "\n",
    "# Get the lowest, highest and average purchase\n",
    "df['lowest_purchase'] = df['In-app Purchases'].apply(lambda x: min(x) if len(x) > 0 else 0)\n",
    "df['highest_purchase'] = df['In-app Purchases'].apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "df['average_purchase'] = df['In-app Purchases'].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
    "\n",
    "# Drop the original column\n",
    "df = df.drop(['In-app Purchases'], axis=1)\n",
    "\n",
    "df['lowest_purchase'] = df['lowest_purchase'].fillna(0)\n",
    "df['highest_purchase'] = df['highest_purchase'].fillna(0)\n",
    "df['average_purchase'] = df['average_purchase'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Price\n",
    "df['Price'] = df['Price'].astype(float)\n",
    "df['Price'] = df['Price'].fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genres & Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the genres column to a list of strings\n",
    "df['Genres'] = df['Genres'].astype(str)\n",
    "df['Genres'] = df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# drop Games, Strategy, Entertainment from the Genres column\n",
    "df['Genres'] = df['Genres'].apply(lambda x: [genre for genre in x if genre not in ['Games', 'Strategy', 'Entertainment']])\n",
    "\n",
    "# Load saved genres dummy variables\n",
    "saved_dummies = pd.read_csv('encoders/genres.csv')\n",
    "\n",
    "# Get the genres that are not in the saved dummy variables\n",
    "other = [genre for genre in df['Genres'].explode().unique() if genre not in saved_dummies.columns]\n",
    "\n",
    "# Replace the genres that are not in the saved dummy variables with 'infrequent'\n",
    "df['Genres'] = df['Genres'].apply(lambda x: ['infrequent' if genre in other else genre for genre in x])\n",
    "\n",
    "# Preprocess test data using the saved dummy variables\n",
    "test_dummies = pd.get_dummies(df['Genres'].apply(pd.Series).stack(), prefix=\"genre\", dummy_na=False).sum(level=0)\n",
    "test_dummies = test_dummies.reindex(columns=saved_dummies.columns, fill_value=0)\n",
    "\n",
    "# Fill the dummy columns with 0 if nan\n",
    "test_dummies = test_dummies.fillna(0)\n",
    "\n",
    "# Concatenate dummies to original DataFrame\n",
    "df = pd.concat([df, test_dummies], axis=1)\n",
    "\n",
    "# Fill NaN with 0\n",
    "genre_cols = [col for col in df.columns if col.startswith('genre')] # get all columns with prefix 'genre'\n",
    "df[genre_cols] = df[genre_cols].fillna(0) # fill NaN with 0 for selected columns\n",
    "\n",
    "# Drop the original Genres column\n",
    "df = df.drop('Genres', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the langs column to a list of strings\n",
    "df['Languages'] = df['Languages'].astype(str)\n",
    "df['Languages'] = df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# Create a column with the number of languages supported\n",
    "df['langs_count'] = df['Languages'].apply(lambda x: len(x))\n",
    "\n",
    "# Drop the English language from the Languages column (it is the most common language and would dominate the model)\n",
    "df['Languages'] = df['Languages'].apply(lambda x: [lang for lang in x if lang not in ['EN']])\n",
    "\n",
    "# Load saved languages dummy variables\n",
    "saved_dummies = pd.read_csv('encoders/langs.csv')\n",
    "\n",
    "# Get the languages that are not in the saved dummy variables\n",
    "other = [lang for lang in df['Languages'].explode().unique() if lang not in saved_dummies.columns]\n",
    "\n",
    "# Replace the languages that are not in the saved dummy variables with 'infrequent'\n",
    "df['Languages'] = df['Languages'].apply(lambda x: ['infrequent' if lang in other else lang for lang in x])\n",
    "\n",
    "# Preprocess test data using the saved dummy variables\n",
    "test_dummies = pd.get_dummies(df['Languages'].apply(pd.Series).stack(), prefix=\"lang\", dummy_na=False).sum(level=0)\n",
    "test_dummies = test_dummies.reindex(columns=saved_dummies.columns, fill_value=0)\n",
    "\n",
    "# Fill the dummy columns with 0 if nan\n",
    "test_dummies = test_dummies.fillna(0)\n",
    "\n",
    "# Concatenate dummies to original DataFrame\n",
    "df = pd.concat([df, test_dummies], axis=1)\n",
    "\n",
    "# Fill NaN with 0\n",
    "lang_cols = [col for col in df.columns if col.startswith('lang')] # get all columns with prefix 'lang'\n",
    "df[lang_cols] = df[lang_cols].fillna(0) # fill NaN with 0 for selected columns\n",
    "\n",
    "# Drop the original Languages column\n",
    "df = df.drop('Languages', axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "def preprocess_nlp(col):\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "    # Remove URLs and email addresses\n",
    "    df[col] = df[col].apply(lambda x: re.sub(r'http\\S+|www.\\S+|\\S+@\\S+', '', x))\n",
    "\n",
    "    # Remove the punctuation, numbers, and convert to lowercase\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(re.findall(r'\\w+', x.lower())))\n",
    "\n",
    "    # Remove the stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "    # Stemming\n",
    "    st = nltk.PorterStemmer()\n",
    "    df[col] = df[col].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "    # Lemmatization\n",
    "    lem = nltk.WordNetLemmatizer()\n",
    "    df[col] = df[col].apply(lambda x: \" \".join([lem.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "    # Remove the frequent and rare words\n",
    "    freq = pd.Series(' '.join(df[col]).split()).value_counts()\n",
    "    common_freq = list(freq[:10].index)\n",
    "    rare_freq = list(freq[-10:].index)\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in common_freq+rare_freq))\n",
    "\n",
    "    # Remove the whitespaces\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(x.strip() for x in x.split()))\n",
    "\n",
    "    # Replace NaN values with empty string\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "    # Convert text data to bag-of-words representation\n",
    "    vectorizer = pickle.load(open(f'encoders/vectorizer_{col}.pkl', 'rb'))\n",
    "    BoW = vectorizer.transform(df[col])\n",
    "\n",
    "    # Apply principal component analysis to reduce the dimensionality\n",
    "    pca_ = pickle.load(open(f'encoders/pca_{col}.pkl', 'rb'))\n",
    "    pca_col = pca_.transform(BoW.toarray())\n",
    "\n",
    "    # Add the PCA-transformed col to the original dataframe\n",
    "    for feat in range(len(pca_col[0])):\n",
    "        df[f'{col}_PCA_{feat}'] = pca_col[:, feat]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_nlp('Description')\n",
    "preprocess_nlp('Subtitle')\n",
    "preprocess_nlp('Name')\n",
    "\n",
    "df = df.drop(['Description', 'Subtitle', 'Name'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Icon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess_icon(img_path):\n",
    "    # Load the game icon image\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (100, 100))\n",
    "\n",
    "    # Extract color features using color histograms\n",
    "    colors = ('b', 'g', 'r')\n",
    "    color_features = []\n",
    "    for k, col in enumerate(colors):\n",
    "        hist = cv2.calcHist([img], [k], None, [256], [0, 256])\n",
    "        color_features.append(hist)\n",
    "\n",
    "    # Reshape the color features to have a single dimension\n",
    "    color_features = np.concatenate(color_features).ravel()\n",
    "\n",
    "    # Extract shape features using edge detection\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "    edge_features = np.array(edges).flatten()\n",
    "\n",
    "    # Combine the color and shape features into a single feature vector\n",
    "    feature_vector = np.concatenate((color_features, edge_features))\n",
    "\n",
    "    # Normalize the feature vector to have unit length\n",
    "    normalized_feature_vector = feature_vector / np.linalg.norm(feature_vector)\n",
    "    \n",
    "    return normalized_feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1043/1043 [00:04<00:00, 239.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Create a list to store the feature vectors\n",
    "icon_features = []\n",
    "\n",
    "df['Icon URL'] = df['Icon URL'].astype(str)\n",
    "\n",
    "# Iterate over the images and extract the features\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    feature_vec = preprocess_icon(row['Icon URL'])\n",
    "    icon_features.append((row['Icon URL'], feature_vec))\n",
    "    \n",
    "# Apply PCA to reduce the number of features\n",
    "pca = pickle.load(open('encoders/icon_pca.pkl', 'rb'))\n",
    "\n",
    "reduced_features = pca.transform([f[1] for f in icon_features])\n",
    "\n",
    "# Convert the reduced features to a dataframe\n",
    "icon_features_df = pd.DataFrame({'Icon URL': [f[0] for f in icon_features],\n",
    "                                    'Icon1': reduced_features[:,0],\n",
    "                                    'Icon2': reduced_features[:,1],\n",
    "                                    'Icon3': reduced_features[:,2],\n",
    "                                    'Icon4': reduced_features[:,3]})\n",
    "\n",
    "# Merge the icon features with the original dataframe on the icon URL\n",
    "df = df.merge(icon_features_df, on='Icon URL', how='left')\n",
    "\n",
    "# Drop the icon URL column\n",
    "df = df.drop(['Icon URL'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reindex(columns=df_old.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1043, 46)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print any cells with string values in column Developer\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if type(row['Developer']) == str:\n",
    "        print(row['Developer'])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns\n",
    "\n",
    "scaler = pickle.load(open('scalers/min_max_scaler.pkl', 'rb'))\n",
    "df = scaler.transform(df)\n",
    "\n",
    "df = pd.DataFrame(df, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = pickle.load(open('encoders/selector.pkl', 'rb'))\n",
    "df = selector.transform(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 0.50194758035364\n",
      "Linear Regression R2: 0.12548579212530697\n"
     ]
    }
   ],
   "source": [
    "### Linear Regression\n",
    "\n",
    "lr_model = pickle.load(open('models/LR_model.pkl', 'rb'))\n",
    "lr_pred = lr_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "lr_mse = mean_squared_error(df_y, lr_pred)\n",
    "lr_r2 = r2_score(df_y, lr_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Linear Regression MSE: {lr_mse}')\n",
    "print(f'Linear Regression R2: {lr_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression MSE: 0.5021389545791592\n",
      "Lasso Regression R2: 0.1251523718922226\n"
     ]
    }
   ],
   "source": [
    "### Lasso Regression\n",
    "\n",
    "lasso_model = pickle.load(open('models/Lasso_model.pkl', 'rb'))\n",
    "lasso_pred = lasso_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "lasso_mse = mean_squared_error(df_y, lasso_pred)\n",
    "lasso_r2 = r2_score(df_y, lasso_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Lasso Regression MSE: {lasso_mse}')\n",
    "print(f'Lasso Regression R2: {lasso_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression MSE: 0.4973109952191694\n",
      "Ridge Regression R2: 0.13356384595965032\n"
     ]
    }
   ],
   "source": [
    "### Ridge Regression\n",
    "\n",
    "ridge_model = pickle.load(open('models/Ridge_model.pkl', 'rb'))\n",
    "ridge_pred = ridge_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "ridge_mse = mean_squared_error(df_y, ridge_pred)\n",
    "ridge_r2 = r2_score(df_y, ridge_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Ridge Regression MSE: {ridge_mse}')\n",
    "print(f'Ridge Regression R2: {ridge_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet Regression MSE: 0.5004474693266845\n",
      "ElasticNet Regression R2: 0.1280993487152955\n"
     ]
    }
   ],
   "source": [
    "### ElasticNet Regression\n",
    "\n",
    "en_model = pickle.load(open('models/ElasticNet_model.pkl', 'rb'))\n",
    "en_pred = en_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "en_mse = mean_squared_error(df_y, en_pred)\n",
    "en_r2 = r2_score(df_y, en_pred)\n",
    " \n",
    "# Print the results\n",
    "print(f'ElasticNet Regression MSE: {en_mse}')\n",
    "print(f'ElasticNet Regression R2: {en_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression MSE: 0.4918949057706136\n",
      "Polynomial Regression R2: 0.14299998502928235\n"
     ]
    }
   ],
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "poly_model = pickle.load(open('models/Polynomial_model.pkl', 'rb'))\n",
    "poly_features = pickle.load(open('encoders/poly.pkl', 'rb'))\n",
    "\n",
    "poly_pred = poly_model.predict(poly_features.transform(df))\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "poly_mse = mean_squared_error(df_y, poly_pred)\n",
    "poly_r2 = r2_score(df_y, poly_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Polynomial Regression MSE: {poly_mse}')\n",
    "print(f'Polynomial Regression R2: {poly_r2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcadams",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
