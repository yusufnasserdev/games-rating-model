{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test & Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import cv2\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origin = pd.read_csv('df_test.csv', parse_dates=['Original Release Date' , 'Current Version Release Date'])\n",
    "df_old = pd.read_csv('preprocessed_data.csv')\n",
    "df_old = df_old.drop(['Average User Rating'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop Primary Genre\n",
    "df_origin.drop(['Primary Genre', 'ID', 'URL'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the icons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to string\n",
    "df_origin['Icon URL'] = df_origin['Icon URL'].astype(str)\n",
    "\n",
    "def download_image(url, filename):\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(filename, 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "\n",
    "# Create a folder to store the images\n",
    "if not os.path.exists('icons_test'):\n",
    "    os.makedirs('icons_test')\n",
    "\n",
    "# Download the images\n",
    "for i, row in tqdm(df_origin.iterrows(), total=df_origin.shape[0]):\n",
    "    if not os.path.exists(f'icons_test/{i}.png'):\n",
    "        download_image(row['Icon URL'], f'icons_test/{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the URL with the icon filename which is the index of the row\n",
    "df_origin['Icon URL'] = df_origin.apply(lambda row : f'icons/{row.name}.png', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia_reviews = pickle.load(open('encoders/sia_sub.pkl', 'rb'))\n",
    "\n",
    "def sentiment_analysis(data):\n",
    "  \n",
    "  # Apply sentiment analysis for each review in the list\n",
    "  data['Reviews'] = data['Reviews'].apply(lambda x: [sia_reviews.polarity_scores(review)['compound'] for review in x])\n",
    "  \n",
    "  return data\n",
    "\n",
    "\n",
    "def reviews_preprocess(data):\n",
    "  \n",
    "  # Apply sentiment_analysis\n",
    "  data = sentiment_analysis(data)\n",
    "  \n",
    "  # Get the lowest, highest and average Reviews\n",
    "  data['lowest_review'] = data['Reviews'].apply(lambda x: min(x) if len(x) > 0 else 0)\n",
    "  data['highest_review'] = data['Reviews'].apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "  data['average_review'] = data['Reviews'].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
    "  \n",
    "  ## Drop nulls of sentiment\n",
    "  # data = data.dropna(subset=['lowest_review','highest_review','average_review'])\n",
    "  return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates, Developer, Age Rating, Purchases and Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dates\n",
    "\n",
    "def date_preprocess(_df):\n",
    "    # Convert the datetime to ordinal\n",
    "    _df['Original Release Date'] = _df['Original Release Date'].apply(lambda x: x.toordinal())\n",
    "    _df['Current Version Release Date'] = _df['Current Version Release Date'].apply(lambda x: x.toordinal())\n",
    "    \n",
    "    # Fill the missing values with the median\n",
    "    _df['Original Release Date'] = _df['Original Release Date'].fillna(_df['Original Release Date'].median())\n",
    "    _df['Current Version Release Date'] = _df['Current Version Release Date'].fillna(_df['Current Version Release Date'].median())\n",
    "    \n",
    "    # Create a new column with the age of the game\n",
    "    _df['game_age'] = datetime.now().toordinal() - _df['Original Release Date']\n",
    "\n",
    "    # Create a new column with the time since the last update\n",
    "    _df['last_update'] = datetime.now().toordinal() - _df['Current Version Release Date']\n",
    "    \n",
    "    # Create a new column with the maintaning period\n",
    "    _df['maintaning_period'] = _df['last_update'] - _df['game_age']\n",
    "\n",
    "    return _df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Developer\n",
    "\n",
    "def dev_preprocess_target_enc(_df):\n",
    "    # Convert to string\n",
    "    _df['Developer'] = _df['Developer'].astype(str)\n",
    "    _df['Developer'] = _df['Developer'].str.replace(\"'\", \"\").str.strip('[]')\n",
    "    \n",
    "    dev_df = pd.read_csv('encoders/dev_avg.csv')\n",
    "    dev_df['Average User Rating'] = dev_df['Average User Rating'].astype(float)\n",
    "    dev_df['Developer'] = dev_df['Developer'].astype(str)\n",
    "\n",
    "    # Get developers that are unseen in the test set\n",
    "    unseen_dev = _df['Developer'][~_df['Developer'].isin(dev_df['Developer'])].unique()\n",
    "    \n",
    "    # Replace the unseen developers with the 'Other'\n",
    "    _df['Developer'] = _df['Developer'].replace(unseen_dev, 'Other')\n",
    "\n",
    "    # Replace the developer with the rating from dev_df\n",
    "    _df['dev_avg'] = _df['Developer'].replace(dev_df.set_index('Developer')['Average User Rating'])\n",
    "    \n",
    "    return _df\n",
    "\n",
    "\n",
    "def dev_preprocess_freq_enc(_df):\n",
    "    # Convert to string\n",
    "    _df['Developer'] = _df['Developer'].astype(str)\n",
    "    _df['Developer'] = _df['Developer'].str.replace(\"'\", \"\").str.strip('[]')\n",
    "    \n",
    "    # Get the frequency of each developer\n",
    "    dev_freqs = pd.read_csv('encoders/dev_freq.csv')\n",
    "    \n",
    "    # Create a dictionary mapping developers to frequencies\n",
    "    dev_freq_dict = dict(zip(dev_freqs['Developer'], dev_freqs['Frequency']))\n",
    "\n",
    "    # Create a new column with the frequency of each developer\n",
    "    _df['dev_freq'] = _df['Developer'].map(dev_freq_dict)\n",
    "    \n",
    "    # Fill the missing values with 1 (the minimum frequency)\n",
    "    _df['dev_freq'] = _df['dev_freq'].fillna(1)\n",
    "        \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Age Rating\n",
    "\n",
    "def age_preprocess(_df):\n",
    "    # Convert to string\n",
    "    _df['Age Rating'] = _df['Age Rating'].astype(str)\n",
    "\n",
    "    # Remove the + sign\n",
    "    _df['Age Rating'] = _df['Age Rating'].str.replace('+', '')\n",
    "\n",
    "    # Convert to int\n",
    "    _df['Age Rating'] = _df['Age Rating'].astype(float)\n",
    "\n",
    "    # fill the missing values with the median\n",
    "    _df['Age Rating'] = _df['Age Rating'].fillna(df_old['Age Rating'].median())\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Purchases\n",
    "\n",
    "# Free apps might skew the in-app purchases column,\n",
    "# so we might split the dataset into free and paid apps\n",
    "\n",
    "def purchases_preprocess(_df):\n",
    "    # Convert to string\n",
    "\n",
    "    _df['In-app Purchases'] = _df['In-app Purchases'].astype(str)\n",
    "    _df['In-app Purchases'] = _df['In-app Purchases'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "    # Convert to float\n",
    "    _df['In-app Purchases'] = _df['In-app Purchases'].apply(lambda x: [float(i) for i in x])\n",
    "\n",
    "    # Get the number of in-app purchases\n",
    "    _df['purchases_count'] = _df['In-app Purchases'].apply(lambda x: len(x))\n",
    "\n",
    "    # Get the lowest, highest and average purchase\n",
    "    _df['lowest_purchase'] = _df['In-app Purchases'].apply(lambda x: min(x) if len(x) > 0 else 0)\n",
    "    _df['highest_purchase'] = _df['In-app Purchases'].apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "    _df['average_purchase'] = _df['In-app Purchases'].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
    "\n",
    "    _df['lowest_purchase'] = _df['lowest_purchase'].fillna(0)\n",
    "    _df['highest_purchase'] = _df['highest_purchase'].fillna(0)\n",
    "    _df['average_purchase'] = _df['average_purchase'].fillna(0)\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Price\n",
    "\n",
    "def price_preprocess(_df):\n",
    "    # Convert to float\n",
    "    _df['Price'] = _df['Price'].astype(float)\n",
    "\n",
    "    # fill the missing values with 0 (free)\n",
    "    _df['Price'] = _df['Price'].fillna(0)\n",
    "    \n",
    "    return _df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genres & Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genres_preprocess_dummies(_df):\n",
    "    # Convert to string\n",
    "    _df['Genres'] = _df['Genres'].astype(str)\n",
    "    _df['Genres'] = _df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "    # drop Games, Strategy, Entertainment from the Genres column\n",
    "    _df['Genres'] = _df['Genres'].apply(lambda x: [genre for genre in x if genre not in ['Games', 'Strategy', 'Entertainment']])\n",
    "\n",
    "    # Load saved genres dummy variables\n",
    "    saved_dummies = pd.read_csv('encoders/genres.csv')\n",
    "\n",
    "    # Get the genres that are not in the saved dummy variables\n",
    "    other = [genre for genre in _df['Genres'].explode().unique() if genre not in saved_dummies.columns]\n",
    "\n",
    "    # Replace the genres that are not in the saved dummy variables with 'infrequent'\n",
    "    _df['Genres'] = _df['Genres'].apply(lambda x: ['infrequent' if genre in other else genre for genre in x])\n",
    "\n",
    "    # Preprocess test data using the saved dummy variables\n",
    "    test_dummies = pd.get_dummies(_df['Genres'].apply(pd.Series).stack(), prefix=\"genre\", dummy_na=False).sum(level=0)\n",
    "    test_dummies = test_dummies.reindex(columns=saved_dummies.columns, fill_value=0)\n",
    "    \n",
    "    # Fill the dummy columns with 0 if nan\n",
    "    test_dummies = test_dummies.fillna(0)\n",
    "\n",
    "    # Add the new dummy variables to the test data\n",
    "    _df = pd.concat([_df, test_dummies], axis=1)\n",
    "    \n",
    "    # Fill NaN with 0\n",
    "    genre_cols = [col for col in _df.columns if col.startswith('genre')] # get all columns with prefix 'genre'\n",
    "    _df[genre_cols] = _df[genre_cols].fillna(0) # fill NaN with 0 for selected columns\n",
    "    \n",
    "    return _df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langs_preprocess_dummies(_df):\n",
    "    # Convert the langs column to a list of strings\n",
    "    _df['Languages'] = _df['Languages'].astype(str)\n",
    "    _df['Languages'] = _df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "    # Create a column with the number of languages supported\n",
    "    _df['langs_count'] = _df['Languages'].apply(lambda x: len(x))\n",
    "\n",
    "    # Drop the English language from the Languages column (it is the most common language and would dominate the model)\n",
    "    _df['Languages'] = _df['Languages'].apply(lambda x: [lang for lang in x if lang not in ['EN']])\n",
    "\n",
    "    # Load saved languages dummy variables\n",
    "    saved_dummies = pd.read_csv('encoders/langs.csv')\n",
    "\n",
    "    # Get the languages that are not in the saved dummy variables\n",
    "    other = [lang for lang in _df['Languages'].explode().unique() if lang not in saved_dummies.columns]\n",
    "\n",
    "    # Replace the languages that are not in the saved dummy variables with 'infrequent'\n",
    "    _df['Languages'] = _df['Languages'].apply(lambda x: ['infrequent' if lang in other else lang for lang in x])\n",
    "\n",
    "    # Preprocess test data using the saved dummy variables\n",
    "    test_dummies = pd.get_dummies(_df['Languages'].apply(pd.Series).stack(), prefix=\"lang\", dummy_na=False).sum(level=0)\n",
    "    test_dummies = test_dummies.reindex(columns=saved_dummies.columns, fill_value=0)\n",
    "\n",
    "    # Fill the dummy columns with 0 if nan\n",
    "    test_dummies = test_dummies.fillna(0)\n",
    "\n",
    "    # Concatenate dummies to original DataFrame\n",
    "    _df = pd.concat([_df, test_dummies], axis=1)\n",
    "\n",
    "    # Fill NaN with 0\n",
    "    lang_cols = [col for col in _df.columns if col.startswith('lang')] # get all columns with prefix 'lang'\n",
    "    _df[lang_cols] = _df[lang_cols].fillna(0) # fill NaN with 0 for selected columns\n",
    "    \n",
    "    return _df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "def preprocess_nlp(_df, col):\n",
    "    _df[col] = _df[col].astype(str)\n",
    "\n",
    "    # Remove URLs and email addresses\n",
    "    _df[col] = _df[col].apply(lambda x: re.sub(r'http\\S+|www.\\S+|\\S+@\\S+', '', x))\n",
    "\n",
    "    # Remove the punctuation, numbers, and convert to lowercase\n",
    "    _df[col] = _df[col].apply(lambda x: \" \".join(re.findall(r'\\w+', x.lower())))\n",
    "\n",
    "    # Remove the stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    _df[col] = _df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "    # Stemming\n",
    "    st = nltk.PorterStemmer()\n",
    "    _df[col] = _df[col].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "    # Lemmatization\n",
    "    lem = nltk.WordNetLemmatizer()\n",
    "    _df[col] = _df[col].apply(lambda x: \" \".join([lem.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "    # Remove the frequent and rare words\n",
    "    freq = pd.Series(' '.join(_df[col]).split()).value_counts()\n",
    "    common_freq = list(freq[:10].index)\n",
    "    rare_freq = list(freq[-10:].index)\n",
    "    _df[col] = _df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in common_freq+rare_freq))\n",
    "\n",
    "    # Remove the whitespaces\n",
    "    _df[col] = _df[col].apply(lambda x: \" \".join(x.strip() for x in x.split()))\n",
    "\n",
    "    # Replace NaN values with empty string\n",
    "    _df[col] = _df[col].fillna('')\n",
    "\n",
    "    # Convert text data to bag-of-words representation\n",
    "    vectorizer = pickle.load(open(f'encoders/vectorizer_{col}.pkl', 'rb'))\n",
    "    BoW = vectorizer.transform(_df[col])\n",
    "\n",
    "    # Apply principal component analysis to reduce the dimensionality\n",
    "    pca_ = pickle.load(open(f'encoders/pca_{col}.pkl', 'rb'))\n",
    "    pca_col = pca_.transform(BoW.toarray())\n",
    "\n",
    "    # Add the PCA-transformed col to the original dataframe\n",
    "    for feat in range(len(pca_col[0])):\n",
    "        _df[f'{col}_PCA_{feat}'] = pca_col[:, feat]\n",
    "        \n",
    "    return _df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Icon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_icon(img_path):\n",
    "    # Load the game icon image\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (100, 100))\n",
    "\n",
    "    # Extract color features using color histograms\n",
    "    colors = ('b', 'g', 'r')\n",
    "    color_features = []\n",
    "    for k, col in enumerate(colors):\n",
    "        hist = cv2.calcHist([img], [k], None, [256], [0, 256])\n",
    "        color_features.append(hist)\n",
    "\n",
    "    # Reshape the color features to have a single dimension\n",
    "    color_features = np.concatenate(color_features).ravel()\n",
    "\n",
    "    # Extract shape features using edge detection\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "    edge_features = np.array(edges).flatten()\n",
    "\n",
    "    # Combine the color and shape features into a single feature vector\n",
    "    feature_vector = np.concatenate((color_features, edge_features))\n",
    "\n",
    "    # Normalize the feature vector to have unit length\n",
    "    normalized_feature_vector = feature_vector / np.linalg.norm(feature_vector)\n",
    "    \n",
    "    return normalized_feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def icons_preprocess(_df):\n",
    "    # Create a list to store the feature vectors\n",
    "    icon_features = []\n",
    "\n",
    "    _df['Icon URL'] = _df['Icon URL'].astype(str)\n",
    "\n",
    "    # Iterate over the images and extract the features\n",
    "    for i, row in tqdm(_df.iterrows(), total=_df.shape[0]):\n",
    "        feature_vec = preprocess_icon(row['Icon URL'])\n",
    "        icon_features.append((row['Icon URL'], feature_vec))\n",
    "        \n",
    "    # Apply PCA to reduce the number of features\n",
    "    pca = pickle.load(open('encoders/icon_pca.pkl', 'rb'))\n",
    "\n",
    "    reduced_features = pca.transform([f[1] for f in icon_features])\n",
    "\n",
    "    # Convert the reduced features to a dataframe\n",
    "    icon_features_df = pd.DataFrame({'Icon URL': [f[0] for f in icon_features],\n",
    "                                        'Icon1': reduced_features[:,0],\n",
    "                                        'Icon2': reduced_features[:,1],\n",
    "                                        'Icon3': reduced_features[:,2],\n",
    "                                        'Icon4': reduced_features[:,3]})\n",
    "\n",
    "    # Merge the icon features with the original dataframe on the icon URL\n",
    "    _df = _df.merge(icon_features_df, on='Icon URL', how='left')\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Sentiment Intensity Analyzer models\n",
    "sia_desc = pickle.load(open('encoders/sia_desc.pkl', 'rb'))\n",
    "sia_name = pickle.load(open('encoders/sia_name.pkl', 'rb'))\n",
    "sia_sub = pickle.load(open('encoders/sia_sub.pkl', 'rb'))\n",
    "\n",
    "# tokenize the text into words and count how many attractive keywords appear\n",
    "tokenizer = pickle.load(open('encoders/nltk_word_tokenize.pkl', 'rb'))\n",
    "\n",
    "def compute_excitement_score(text, _sia):\n",
    "    \n",
    "    # compute the polarity scores for the given text\n",
    "    scores = _sia.polarity_scores(text)\n",
    "    \n",
    "    # compute the excitement score as the sum of the positive and negative polarity scores\n",
    "    excitement_score = scores['pos'] + abs(scores['neg'])\n",
    "    \n",
    "    return excitement_score\n",
    "\n",
    "# define a function to compute an attractive score from a given text\n",
    "def compute_attractive_score(text):\n",
    "    # define a list of keywords that might make a game attractive to users\n",
    "    attractive_keywords = ['graphics', 'gameplay', 'storyline', 'characters']\n",
    "    \n",
    "    words = tokenizer(text.lower())\n",
    "    num_attractive_keywords = len([word for word in words if word in attractive_keywords])\n",
    "    \n",
    "    # compute the attractive score as the ratio of attractive keywords to total words\n",
    "    attractive_score = num_attractive_keywords / len(words) if len(words) > 0 else 0\n",
    "    \n",
    "    return attractive_score\n",
    "\n",
    "def desc_preprocess(_df):\n",
    "    _df['Description'] = _df['Description'].astype(str)\n",
    "    \n",
    "    _df['excitement_score'] = _df['Description'].apply(lambda x : compute_excitement_score(x, sia_desc))\n",
    "    _df['attractive_score'] = _df['Description'].apply(compute_attractive_score)\n",
    "    \n",
    "    return _df\n",
    "\n",
    "def name_preprocess(_df):\n",
    "    _df['Name'] = _df['Name'].astype(str)\n",
    "    _df['name_sia'] = _df['Name'].apply(lambda x : compute_excitement_score(x, sia_name))\n",
    "    \n",
    "    return _df\n",
    "\n",
    "def sub_preprocess(_df):\n",
    "    _df['Subtitle'] = _df['Subtitle'].astype(str)\n",
    "    _df['sub_sia'] = _df['Subtitle'].apply(lambda x : compute_excitement_score(x, sia_sub))\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_origin.drop(['Average User Rating'], axis=1)\n",
    "df_y = df_origin['Average User Rating']\n",
    "\n",
    "df = date_preprocess(df)\n",
    "\n",
    "df = dev_preprocess_freq_enc(df)\n",
    "\n",
    "df = dev_preprocess_target_enc(df)\n",
    "\n",
    "df = genres_preprocess_dummies(df)\n",
    "\n",
    "df = langs_preprocess_dummies(df)\n",
    "\n",
    "df = purchases_preprocess(df)\n",
    "\n",
    "df = age_preprocess(df)\n",
    "\n",
    "df = price_preprocess(df)\n",
    "\n",
    "df = preprocess_nlp(df, 'Description')\n",
    "df = preprocess_nlp(df, 'Subtitle')\n",
    "df = preprocess_nlp(df, 'Name')\n",
    "\n",
    "df = desc_preprocess(df)\n",
    "df = name_preprocess(df)\n",
    "df = sub_preprocess(df)\n",
    "\n",
    "df = reviews_preprocess(df)\n",
    "\n",
    "df = icons_preprocess(df)\n",
    "\n",
    "df = df.drop(['Developer',\n",
    "              'Original Release Date',\n",
    "              'Current Version Release Date',\n",
    "              'Genres',\n",
    "              'Languages',\n",
    "              'Reviews',\n",
    "              'In-app Purchases',\n",
    "              'Description',\n",
    "              'Subtitle',\n",
    "              'Name', \n",
    "              'Icon URL'], axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reindex(columns=df_old.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(549, 55)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns\n",
    "\n",
    "scaler = pickle.load(open('scalers/std_scaler.pkl', 'rb'))\n",
    "df = scaler.transform(df)\n",
    "\n",
    "df = pd.DataFrame(df, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = pickle.load(open('encoders/selector.pkl', 'rb'))\n",
    "df = selector.transform(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 0.5367574028409572\n",
      "Linear Regression R2: -0.18909841805829664\n"
     ]
    }
   ],
   "source": [
    "### Linear Regression\n",
    "\n",
    "lr_model = pickle.load(open('models/LR_model.pkl', 'rb'))\n",
    "lr_pred = lr_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "lr_mse = mean_squared_error(df_y, lr_pred)\n",
    "lr_r2 = r2_score(df_y, lr_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Linear Regression MSE: {lr_mse}')\n",
    "print(f'Linear Regression R2: {lr_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression MSE: 0.5437670571497714\n",
      "Lasso Regression R2: -0.20462716308468987\n"
     ]
    }
   ],
   "source": [
    "### Lasso Regression\n",
    "\n",
    "lasso_model = pickle.load(open('models/Lasso_model.pkl', 'rb'))\n",
    "lasso_pred = lasso_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "lasso_mse = mean_squared_error(df_y, lasso_pred)\n",
    "lasso_r2 = r2_score(df_y, lasso_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Lasso Regression MSE: {lasso_mse}')\n",
    "print(f'Lasso Regression R2: {lasso_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression MSE: 0.5405566788225264\n",
      "Ridge Regression R2: -0.19751509388901534\n"
     ]
    }
   ],
   "source": [
    "### Ridge Regression\n",
    "\n",
    "ridge_model = pickle.load(open('models/Ridge_model.pkl', 'rb'))\n",
    "ridge_pred = ridge_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "ridge_mse = mean_squared_error(df_y, ridge_pred)\n",
    "ridge_r2 = r2_score(df_y, ridge_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Ridge Regression MSE: {ridge_mse}')\n",
    "print(f'Ridge Regression R2: {ridge_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet Regression MSE: 0.5404826865236979\n",
      "ElasticNet Regression R2: -0.1973511760277622\n"
     ]
    }
   ],
   "source": [
    "### ElasticNet Regression\n",
    "\n",
    "en_model = pickle.load(open('models/ElasticNet_model.pkl', 'rb'))\n",
    "en_pred = en_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "en_mse = mean_squared_error(df_y, en_pred)\n",
    "en_r2 = r2_score(df_y, en_pred)\n",
    " \n",
    "# Print the results\n",
    "print(f'ElasticNet Regression MSE: {en_mse}')\n",
    "print(f'ElasticNet Regression R2: {en_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression MSE: 0.539405004504007\n",
      "Polynomial Regression R2: -0.19496374740916855\n"
     ]
    }
   ],
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "poly_model = pickle.load(open('models/Polynomial_model.pkl', 'rb'))\n",
    "poly_features = pickle.load(open('encoders/poly.pkl', 'rb'))\n",
    "\n",
    "poly_pred = poly_model.predict(poly_features.transform(df))\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "poly_mse = mean_squared_error(df_y, poly_pred)\n",
    "poly_r2 = r2_score(df_y, poly_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Polynomial Regression MSE: {poly_mse}')\n",
    "print(f'Polynomial Regression R2: {poly_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regression MSE: 0.5700345333838793\n",
      "Random Forest Regression R2: -0.26281846938254927\n"
     ]
    }
   ],
   "source": [
    "### Random Forest Regression\n",
    "\n",
    "rfg_model = pickle.load(open('models/RandomForest_model.pkl', 'rb'))\n",
    "\n",
    "rfg_pred = rfg_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "rfg_mse = mean_squared_error(df_y, rfg_pred)\n",
    "rfg_r2 = r2_score(df_y, rfg_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Random Forest Regression MSE: {rfg_mse}')\n",
    "print(f'Random Forest Regression R2: {rfg_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Regression MSE: 0.468663502121082\n",
      "XGBoost Regression R2: -0.038247495095965034\n"
     ]
    }
   ],
   "source": [
    "### XGBoost Regression\n",
    "\n",
    "xgb_model = pickle.load(open('models/XGBoost_model.pkl', 'rb'))\n",
    "\n",
    "xgb_pred = xgb_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "xgb_mse = mean_squared_error(df_y, xgb_pred)\n",
    "xgb_r2 = r2_score(df_y, xgb_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'XGBoost Regression MSE: {xgb_mse}')\n",
    "print(f'XGBoost Regression R2: {xgb_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Regression MSE: 0.5108188773887918\n",
      "Gradient Boosting Regression R2: -0.13163584852746935\n"
     ]
    }
   ],
   "source": [
    "### Gradient Boosting Regression\n",
    "\n",
    "gb_model = pickle.load(open('models/GradientBoosting_model.pkl', 'rb'))\n",
    "\n",
    "gb_pred = gb_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "gb_mse = mean_squared_error(df_y, gb_pred)\n",
    "gb_r2 = r2_score(df_y, gb_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Gradient Boosting Regression MSE: {gb_mse}')\n",
    "print(f'Gradient Boosting Regression R2: {gb_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Regression MSE: 0.6143977790463164\n",
      "CatBoost Regression R2: -0.36109799931157816\n"
     ]
    }
   ],
   "source": [
    "### CatBoost Regression\n",
    "\n",
    "cb_model = pickle.load(open('models/CatBoost_model.pkl', 'rb'))\n",
    "\n",
    "cb_pred = cb_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "cb_mse = mean_squared_error(df_y, cb_pred)\n",
    "cb_r2 = r2_score(df_y, cb_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'CatBoost Regression MSE: {cb_mse}')\n",
    "print(f'CatBoost Regression R2: {cb_r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Regression MSE: 0.608133123655274\n",
      "AdaBoost Regression R2: -0.3472196777910157\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost Regression\n",
    "\n",
    "ada_model = pickle.load(open('models/AdaBoost_model.pkl', 'rb'))\n",
    "\n",
    "ada_pred = ada_model.predict(df)\n",
    "\n",
    "# Calculate the MSE and coefficient of determination\n",
    "ada_mse = mean_squared_error(df_y, ada_pred)\n",
    "ada_r2 = r2_score(df_y, ada_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'AdaBoost Regression MSE: {ada_mse}')\n",
    "print(f'AdaBoost Regression R2: {ada_r2}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcadams",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
