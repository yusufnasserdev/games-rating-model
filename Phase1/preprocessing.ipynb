{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T14:19:38.839351Z",
     "start_time": "2023-04-14T14:19:38.815268Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:33:46.390000Z",
     "start_time": "2023-04-14T10:33:46.279076Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('games-regression-dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Setting data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:07.218975Z",
     "start_time": "2023-04-14T10:35:07.199405Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop Primary Genre\n",
    "df.drop(['Primary Genre', 'ID', 'URL', 'Icon URL'], axis=1, inplace=True)\n",
    "\n",
    "df['Original Release Date'] = pd.to_datetime(df['Original Release Date'], format='%d/%m/%Y')\n",
    "df['Current Version Release Date'] = pd.to_datetime(df['Current Version Release Date'], format='%d/%m/%Y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T16:07:00.183708Z",
     "start_time": "2023-04-13T16:07:00.127616Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:05.138536Z",
     "start_time": "2023-04-13T14:33:05.138536Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:07.893814Z",
     "start_time": "2023-04-13T14:33:07.893814Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:21.139604Z",
     "start_time": "2023-04-13T14:33:21.139604Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Genres'] = df['Genres'].astype(str)\n",
    "df['Genres'] = df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "genre_counts = df.explode('Genres').groupby('Genres').size().sort_values(ascending=False)\n",
    "genre_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:43.637371Z",
     "start_time": "2023-04-13T14:33:43.637371Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Developer'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:47.598084Z",
     "start_time": "2023-04-13T14:33:47.598084Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Developer'].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:49.943309Z",
     "start_time": "2023-04-13T14:33:49.943309Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Languages'] = df['Languages'].astype(str)\n",
    "\n",
    "df['Languages'] = df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "langs_counts = df.explode('Languages').groupby('Languages').size().sort_values(ascending=False)\n",
    "langs_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Developer preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:14.110217Z",
     "start_time": "2023-04-14T10:35:13.123201Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to string\n",
    "df['Developer'] = df['Developer'].astype(str)\n",
    "df['Developer'] = df['Developer'].str.replace(\"'\", \"\").str.strip('[]')\n",
    "\n",
    "dev_counts = df['Developer'].value_counts()\n",
    "other = dev_counts[dev_counts < 5].index\n",
    "df['Developer'] = df['Developer'].replace(other, 'Other')\n",
    "\n",
    "dev_df = df[['Developer', 'Average User Rating']].groupby('Developer').mean()\n",
    "dev_df['Count'] = df['Developer'].value_counts()\n",
    "\n",
    "dev_df = dev_df.sort_values(by='Count', ascending=False)\n",
    "dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:16.888246Z",
     "start_time": "2023-04-14T10:35:16.822234Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace the developer names with the average user rating from dev_df\n",
    "df['Developer'] = df['Developer'].replace(dev_df.index, dev_df['Average User Rating'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Genres preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:21.245168Z",
     "start_time": "2023-04-14T10:35:20.885068Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Convert the genres column to a list of strings\n",
    "df['Genres'] = df['Genres'].astype(str)\n",
    "df['Genres'] = df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# drop Games, Strategy, Entertainment from the Genres column\n",
    "df['Genres'] = df['Genres'].apply(lambda x: [genre for genre in x if genre not in ['Games', 'Strategy', 'Entertainment']])\n",
    "\n",
    "# Join the list of genres into a single string\n",
    "genres = df['Genres'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Create a count Vectorizer and fit it to the genres\n",
    "count_vec = CountVectorizer()\n",
    "bow_genres = count_vec.fit_transform(genres)\n",
    "\n",
    "# Apply principal component analysis to reduce the dimensionality\n",
    "pca = PCA(n_components=10)\n",
    "pca_genres = pca.fit_transform(bow_genres.toarray())\n",
    "\n",
    "# Add the PCA-transformed genres to the original dataframe\n",
    "for i in range(10):\n",
    "    df[f'Genre_PCA_{i}'] = pca_genres[:, i]\n",
    "\n",
    "# Drop the original column\n",
    "df = df.drop(['Genres'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Languages preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:38.695352Z",
     "start_time": "2023-04-14T10:35:38.599717Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the genres column to a list of strings\n",
    "df['Languages'] = df['Languages'].astype(str)\n",
    "df['Languages'] = df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# drop Games, Strategy, Entertainment from the Genres column\n",
    "df['Languages'] = df['Languages'].apply(lambda x: [lang for lang in x if lang not in ['En']])\n",
    "\n",
    "# Join the list of genres into a single string\n",
    "languages = df['Languages'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Create a count Vectorizer and fit it to the genres\n",
    "count_vec = CountVectorizer()\n",
    "bow_languages = count_vec.fit_transform(languages)\n",
    "\n",
    "# Apply principal component analysis to reduce the dimensionality\n",
    "pca = PCA(n_components=10)\n",
    "pca_languages = pca.fit_transform(bow_languages.toarray())\n",
    "\n",
    "# Add the PCA-transformed genres to the original dataframe\n",
    "for i in range(10):\n",
    "    df[f'Languages_PCA_{i}'] = pca_languages[:, i]\n",
    "\n",
    "# Drop the original column\n",
    "df = df.drop(['Languages'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## In-app Purchases preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:52.935497Z",
     "start_time": "2023-04-14T10:35:52.881403Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Free apps might skew the in-app purchases column,\n",
    "# so we might split the dataset into free and paid apps\n",
    "\n",
    "df['In-app Purchases'] = df['In-app Purchases'].astype(str)\n",
    "df['In-app Purchases'] = df['In-app Purchases'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:54.565885Z",
     "start_time": "2023-04-14T10:35:54.456681Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to float\n",
    "df['In-app Purchases'] = df['In-app Purchases'].apply(lambda x: [float(i) for i in x])\n",
    "\n",
    "# Get the lowest, highest and average purchase\n",
    "df['Lowest Purchase'] = df['In-app Purchases'].apply(lambda x: min(x) if len(x) > 0 else 0)\n",
    "df['Highest Purchase'] = df['In-app Purchases'].apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "df['Average Purchase'] = df['In-app Purchases'].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:36:00.624093Z",
     "start_time": "2023-04-14T10:36:00.576764Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop the original column\n",
    "df = df.drop(['In-app Purchases'], axis=1)\n",
    "\n",
    "df['Lowest Purchase'] = df['Lowest Purchase'].fillna(0)\n",
    "df['Highest Purchase'] = df['Highest Purchase'].fillna(0)\n",
    "df['Average Purchase'] = df['Average Purchase'].fillna(0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Age Rating preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:36:03.614263Z",
     "start_time": "2023-04-14T10:36:03.579637Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to string\n",
    "df['Age Rating'] = df['Age Rating'].astype(str)\n",
    "\n",
    "# Remove the + sign\n",
    "df['Age Rating'] = df['Age Rating'].str.replace('+', '')\n",
    "\n",
    "# Convert to int\n",
    "df['Age Rating'] = df['Age Rating'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dates preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:36:05.737187Z",
     "start_time": "2023-04-14T10:36:05.699973Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the datetime to ordinal\n",
    "df['Original Release Date'] = df['Original Release Date'].apply(lambda x: x.toordinal())\n",
    "df['Current Version Release Date'] = df['Current Version Release Date'].apply(lambda x: x.toordinal())\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NLP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:43:13.386866Z",
     "start_time": "2023-04-14T10:43:13.386866Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "def preprocess_nlp(col):\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "    # Remove URLs and email addresses\n",
    "    df[col] = df[col].apply(lambda x: re.sub(r'http\\S+|www.\\S+|\\S+@\\S+', '', x))\n",
    "\n",
    "    # Remove the punctuation, numbers, and convert to lowercase\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(re.findall(r'\\w+', x.lower())))\n",
    "\n",
    "    # Remove the stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "    # Stemming\n",
    "    st = nltk.PorterStemmer()\n",
    "    df[col] = df[col].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "    # Lemmatization\n",
    "    lem = nltk.WordNetLemmatizer()\n",
    "    df[col] = df[col].apply(lambda x: \" \".join([lem.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "    # Remove the frequent and rare words\n",
    "    freq = pd.Series(' '.join(df[col]).split()).value_counts()\n",
    "    common_freq = list(freq[:10].index)\n",
    "    rare_freq = list(freq[-10:].index)\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in common_freq+rare_freq))\n",
    "\n",
    "    # Remove the whitespaces\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(x.strip() for x in x.split()))\n",
    "\n",
    "    # Replace NaN values with empty string\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "    # Convert text data to bag-of-words representation\n",
    "    vectorizer = CountVectorizer()\n",
    "    BoW = vectorizer.fit_transform(df[col])\n",
    "\n",
    "    # Apply principal component analysis to reduce the dimensionality\n",
    "    pca_ = PCA(n_components=10)\n",
    "    pca_col = pca_.fit_transform(BoW.toarray())\n",
    "\n",
    "    # Add the PCA-transformed genres to the original dataframe\n",
    "    for feat in range(10):\n",
    "        df[f'{col}_PCA_{feat}'] = pca_col[:, feat]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:43:49.505557Z",
     "start_time": "2023-04-14T10:43:49.505557Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocess_nlp('Description')\n",
    "preprocess_nlp('Subtitle')\n",
    "preprocess_nlp('Name')\n",
    "\n",
    "df = df.drop(['Description', 'Subtitle', 'Name'], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Icon preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Download the icons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T15:23:14.248477Z",
     "start_time": "2023-04-13T15:23:14.247478Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to string\n",
    "df['Icon URL'] = df['Icon URL'].astype(str)\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def download_image(url, filename):\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(filename, 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "\n",
    "# Create a folder to store the images\n",
    "if not os.path.exists('icons'):\n",
    "    os.makedirs('icons')\n",
    "\n",
    "# Download the images\n",
    "for i, row in df.iterrows():\n",
    "    download_image(row['Icon URL'], f'icons/{i}.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Extract features from the icons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:52:48.299695Z",
     "start_time": "2023-04-14T10:52:13.578629Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_icon(img_path):\n",
    "    # Load the game icon image\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (100, 100))\n",
    "\n",
    "    # Extract color features using color histograms\n",
    "    colors = ('b', 'g', 'r')\n",
    "    color_features = []\n",
    "    for k, col in enumerate(colors):\n",
    "        hist = cv2.calcHist([img], [k], None, [256], [0, 256])\n",
    "        color_features.append(hist)\n",
    "\n",
    "    # Reshape the color features to have a single dimension\n",
    "    color_features = np.concatenate(color_features).ravel()\n",
    "\n",
    "    # Extract shape features using edge detection\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "    edge_features = np.array(edges).flatten()\n",
    "\n",
    "    # Combine the color and shape features into a single feature vector\n",
    "    feature_vector = np.concatenate((color_features, edge_features))\n",
    "\n",
    "    # Normalize the feature vector to have unit length\n",
    "    normalized_feature_vector = feature_vector / np.linalg.norm(feature_vector)\n",
    "\n",
    "    return normalized_feature_vector\n",
    "\n",
    "# Create a list to store the feature vectors\n",
    "icon_features = []\n",
    "\n",
    "# Iterate over the images and extract the features\n",
    "for i, row in df.iterrows():\n",
    "    icon_features.append(preprocess_icon(f'icons/{i}.png'))\n",
    "\n",
    "# Apply PCA to reduce the number of features\n",
    "pca = PCA(n_components=10)\n",
    "icon_features_pca = pca.fit_transform(icon_features)\n",
    "\n",
    "icon_features_df = pd.DataFrame(icon_features_pca, columns=[f'icon_{i}' for i in range(icon_features_pca.shape[1])])\n",
    "icon_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Add the icon features to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:53:05.530442Z",
     "start_time": "2023-04-14T10:53:04.975260Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Concatenate the icon features with the other features\n",
    "df = pd.concat([df, icon_features_df], axis=1)\n",
    "\n",
    "# Save the updated dataset\n",
    "df.to_csv('games_with_icon_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:53:31.380503Z",
     "start_time": "2023-04-14T10:53:31.354875Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T14:42:19.976831Z",
     "start_time": "2023-04-14T14:42:19.786259Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  5  6 10 28 29 36 41 52 56]\n",
      "Index(['Developer', 'Original Release Date', 'Current Version Release Date',\n",
      "       'Genre_PCA_3', 'Highest Purchase', 'Average Purchase',\n",
      "       'Description_PCA_6', 'Subtitle_PCA_1', 'Name_PCA_2', 'Name_PCA_6'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('games_with_icon_features.csv')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "y = df['Average User Rating']\n",
    "features = df.drop(['Average User Rating'], axis=1)\n",
    "\n",
    "# Feature selection\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "selector = SelectKBest(f_regression, k=10)\n",
    "X = selector.fit_transform(features, y)\n",
    "\n",
    "# print the selected features\n",
    "print(selector.get_support(indices=True))\n",
    "seleted_features = features.columns[selector.get_support(indices=True)]\n",
    "print(seleted_features)\n",
    "\n",
    "# Scale the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T14:40:09.683179Z",
     "start_time": "2023-04-14T14:40:09.629559Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.49\n",
      "Coefficient of determination: 0.15\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39m# print the features weights\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(model\u001b[39m.\u001b[39mcoef_)):\n\u001b[1;32m---> 20\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mX\u001b[39m.\u001b[39mcolumns[i]\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39mcoef_[i]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "import pickle\n",
    "pickle.dump(model, open('models/LR_model.pkl', 'wb'))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))\n",
    "\n",
    "# print the features weights\n",
    "for i in range(len(model.coef_)):\n",
    "    print(f'Feature {selected_features[i]}: {model.coef_[i]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T11:00:32.712100Z",
     "start_time": "2023-04-14T11:00:32.642347Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a ridge regression model\n",
    "model = Ridge(alpha=0.5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/Ridge_model.pkl', 'wb'))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T11:00:35.138470Z",
     "start_time": "2023-04-14T11:00:35.102309Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a lasso regression model\n",
    "model = Lasso(alpha=0.5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/Lasso_model.pkl', 'wb'))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T11:00:37.282470Z",
     "start_time": "2023-04-14T11:00:37.226872Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an elastic net regression model\n",
    "model = ElasticNet(alpha=0.5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/ElasticNet_model.pkl', 'wb'))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T11:02:11.403793Z",
     "start_time": "2023-04-14T11:00:55.155863Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a polynomial regression model\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.fit_transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/Polynomial_model.pkl', 'wb'))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_poly)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
