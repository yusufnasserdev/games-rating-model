{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T14:19:38.839351Z",
     "start_time": "2023-04-14T14:19:38.815268Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:33:46.390000Z",
     "start_time": "2023-04-14T10:33:46.279076Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "dateparse = lambda x: datetime.strptime(x, '%d/%m/%Y')\n",
    "\n",
    "df_origin = pd.read_csv('games-regression-dataset.csv', parse_dates=['Original Release Date' , 'Current Version Release Date'], date_parser=dateparse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Download the icons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to string\n",
    "df_origin['Icon URL'] = df_origin['Icon URL'].astype(str)\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def download_image(url, filename):\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(filename, 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "\n",
    "# Create a folder to store the images\n",
    "if not os.path.exists('icons'):\n",
    "    os.makedirs('icons')\n",
    "\n",
    "# Download the images\n",
    "for i, row in df_origin.iterrows():\n",
    "    download_image(row['Icon URL'], f'icons/{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the URL with the icon filename which is the index of the row\n",
    "df_origin['Icon URL'] = df_origin.apply(lambda row : f'icons/{row.name}.png', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df, df_test = train_test_split(df_origin, test_size=0.2, random_state=42)\n",
    "df_test.to_csv('df_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dropping unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:07.218975Z",
     "start_time": "2023-04-14T10:35:07.199405Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop Primary Genre\n",
    "df.drop(['Primary Genre', 'ID', 'URL'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dates preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:36:05.737187Z",
     "start_time": "2023-04-14T10:36:05.699973Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Release Date</th>\n",
       "      <th>Current Version Release Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2173</th>\n",
       "      <td>735920</td>\n",
       "      <td>736359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>733705</td>\n",
       "      <td>736758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>735558</td>\n",
       "      <td>735977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>736684</td>\n",
       "      <td>737228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>736536</td>\n",
       "      <td>736661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Original Release Date  Current Version Release Date\n",
       "2173                 735920                        736359\n",
       "927                  733705                        736758\n",
       "2499                 735558                        735977\n",
       "45                   736684                        737228\n",
       "1741                 736536                        736661"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the datetime to ordinal\n",
    "\n",
    "df['Original Release Date'] = df['Original Release Date'].apply(lambda x: x.toordinal())\n",
    "df['Current Version Release Date'] = df['Current Version Release Date'].apply(lambda x: x.toordinal())\n",
    "df[['Original Release Date', 'Current Version Release Date']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_age</th>\n",
       "      <th>last_update</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2173</th>\n",
       "      <td>439</td>\n",
       "      <td>2270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>3053</td>\n",
       "      <td>1871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>419</td>\n",
       "      <td>2652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>544</td>\n",
       "      <td>1401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>125</td>\n",
       "      <td>1968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      game_age  last_update\n",
       "2173       439         2270\n",
       "927       3053         1871\n",
       "2499       419         2652\n",
       "45         544         1401\n",
       "1741       125         1968"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new column with the age of the game\n",
    "df['game_age'] = df['Current Version Release Date'] - df['Original Release Date']\n",
    "\n",
    "# Create a new column with the time since the last update\n",
    "df['last_update'] = datetime.now().toordinal() - df['Current Version Release Date'] \n",
    "\n",
    "df[['game_age', 'last_update']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T16:07:00.183708Z",
     "start_time": "2023-04-13T16:07:00.127616Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:05.138536Z",
     "start_time": "2023-04-13T14:33:05.138536Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:07.893814Z",
     "start_time": "2023-04-13T14:33:07.893814Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:21.139604Z",
     "start_time": "2023-04-13T14:33:21.139604Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Genres'] = df['Genres'].astype(str)\n",
    "df['Genres'] = df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "genre_counts = df.explode('Genres').groupby('Genres').size().sort_values(ascending=False)\n",
    "genre_counts\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:43.637371Z",
     "start_time": "2023-04-13T14:33:43.637371Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Developer'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:47.598084Z",
     "start_time": "2023-04-13T14:33:47.598084Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Developer'].unique().size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:49.943309Z",
     "start_time": "2023-04-13T14:33:49.943309Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Languages'] = df['Languages'].astype(str)\n",
    "\n",
    "df['Languages'] = df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "langs_counts = df.explode('Languages').groupby('Languages').size().sort_values(ascending=False)\n",
    "print(langs_counts[1:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Developer preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:14.110217Z",
     "start_time": "2023-04-14T10:35:13.123201Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to string\n",
    "df['Developer'] = df['Developer'].astype(str)\n",
    "df['Developer'] = df['Developer'].str.replace(\"'\", \"\").str.strip('[]')\n",
    "\n",
    "# Replace the developer names with less than 3 games with 'Other'\n",
    "dev_counts = df['Developer'].value_counts()\n",
    "other = dev_counts[dev_counts < 3].index\n",
    "df['Developer'] = df['Developer'].replace(other, 'Other')\n",
    "\n",
    "dev_df = df[['Developer', 'Average User Rating']].groupby('Developer').mean()\n",
    "\n",
    "# Save dev_df to be used on the test set\n",
    "dev_df.to_csv('encoders/dev_df.csv')\n",
    "\n",
    "# Replace the developer names with the average user rating from dev_df\n",
    "df['Developer'] = df['Developer'].replace(dev_df.index, dev_df['Average User Rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Genres preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NLP approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:21.245168Z",
     "start_time": "2023-04-14T10:35:20.885068Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the genres column to a list of strings\n",
    "df['Genres'] = df['Genres'].astype(str)\n",
    "df['Genres'] = df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# drop Games, Strategy, Entertainment from the Genres column\n",
    "df['Genres'] = df['Genres'].apply(lambda x: [genre for genre in x if genre not in ['Games', 'Strategy', 'Entertainment']])\n",
    "\n",
    "# Join the list of genres into a single string\n",
    "genres = df['Genres'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Create a count Vectorizer and fit it to the genres\n",
    "count_vec = CountVectorizer()\n",
    "bow_genres = count_vec.fit_transform(genres)\n",
    "\n",
    "# Apply principal component analysis to reduce the dimensionality\n",
    "pca = PCA(n_components=10)\n",
    "pca_genres = pca.fit_transform(bow_genres.toarray())\n",
    "\n",
    "# Add the PCA-transformed genres to the original dataframe\n",
    "for i in range(len(pca_genres[0])):\n",
    "    df[f'Genre_PCA_{i}'] = pca_genres[:, i]\n",
    "\n",
    "# Drop the original column\n",
    "df = df.drop(['Genres'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dummy variables approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the genres column to a list of strings\n",
    "df['Genres'] = df['Genres'].astype(str)\n",
    "df['Genres'] = df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# drop Games, Strategy, Entertainment from the Genres column\n",
    "df['Genres'] = df['Genres'].apply(lambda x: [genre for genre in x if genre not in ['Games', 'Strategy', 'Entertainment']])\n",
    "\n",
    "# Replace genres with counts less than 100 with 'infrequent' as it would represent a very small percentage of the data (less than 2%)\n",
    "other = df['Genres'].explode().value_counts()[df['Genres'].explode().value_counts() < 100].index\n",
    "df['Genres'] = df['Genres'].apply(lambda x: [genre if genre not in other else 'infrequent' for genre in x])\n",
    "\n",
    "# Get dummy variables for the genres\n",
    "genres = pd.get_dummies(df['Genres'].apply(pd.Series).stack(), prefix=\"genre\", dummy_na=False).sum(level=0)\n",
    "\n",
    "# Save the genres dummies to be used on the test set\n",
    "genres.to_csv('encoders/genres.csv', index=False)\n",
    "\n",
    "# Add the dummy variables to the original dataframe\n",
    "df = pd.concat([df, genres], axis=1)\n",
    "\n",
    "# Drop the original column\n",
    "df = df.drop(['Genres'], axis=1)\n",
    "\n",
    "# Fill NaN with 0\n",
    "genre_cols = [col for col in df.columns if col.startswith('genre')] # get all columns with prefix 'genre'\n",
    "df[genre_cols] = df[genre_cols].fillna(0) # fill NaN with 0 for selected columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi-label binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the genres column to a list of strings\n",
    "df['Genres'] = df['Genres'].astype(str)\n",
    "df['Genres'] = df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# drop Games, Strategy, Entertainment from the Genres column\n",
    "df['Genres'] = df['Genres'].apply(lambda x: [genre for genre in x if genre not in ['Games', 'Strategy', 'Entertainment']])\n",
    "\n",
    "# Replace genres with counts less than 100 with 'infrequent' as it would represent a very small percentage of the data (less than 2%)\n",
    "other = df['Genres'].explode().value_counts()[df['Genres'].explode().value_counts() < 100].index\n",
    "df['Genres'] = df['Genres'].apply(lambda x: [genre if genre not in other else 'infrequent_genre' for genre in x])\n",
    "\n",
    "# Instantiate the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit the MultiLabelBinarizer to the genres\n",
    "mlb.fit(df['Genres'])\n",
    "\n",
    "# Drop nan from the classes\n",
    "mlb.classes_ = np.delete(mlb.classes_, np.where(mlb.classes_ == 'nan'))\n",
    "\n",
    "# Save the mlb for later use with the test data\n",
    "pickle.dump(mlb, open('encoders/mlb_genres.pkl', 'wb'))\n",
    "\n",
    "# Transform the genres into a one-hot encoded array\n",
    "genres_mlb = mlb.transform(df['Genres'])\n",
    "\n",
    "# Create a dataframe from the one-hot encoded array\n",
    "genres_mlb_df = pd.DataFrame(genres_mlb, columns=mlb.classes_)\n",
    "\n",
    "# Add the one-hot encoded genres to the original dataframe\n",
    "df = pd.concat([df, genres_mlb_df], axis=1)\n",
    "\n",
    "# Drop the original column\n",
    "df = df.drop(['Genres'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Languages preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NLP approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:38.695352Z",
     "start_time": "2023-04-14T10:35:38.599717Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the langs column to a list of strings\n",
    "df['Languages'] = df['Languages'].astype(str)\n",
    "df['Languages'] = df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# Drop the English language from the Languages column\n",
    "df['Languages'] = df['Languages'].apply(lambda x: [lang for lang in x if lang not in ['EN']])\n",
    "\n",
    "# Join the list of langs into a single string\n",
    "languages = df['Languages'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Create a count Vectorizer and fit it to the langs\n",
    "count_vec = CountVectorizer()\n",
    "bow_languages = count_vec.fit_transform(languages)\n",
    "\n",
    "# Apply principal component analysis to reduce the dimensionality\n",
    "pca = PCA(n_components=10)\n",
    "pca_languages = pca.fit_transform(bow_languages.toarray())\n",
    "\n",
    "# Add the PCA-transformed langs to the original dataframe\n",
    "for i in range(len(pca_languages[0])):\n",
    "    df[f'Languages_PCA_{i}'] = pca_languages[:, i]\n",
    "\n",
    "# Drop the original column\n",
    "df = df.drop(['Languages'], axis=1)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df.iloc[:, 15:].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dummy variables approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the langs column to a list of strings\n",
    "df['Languages'] = df['Languages'].astype(str)\n",
    "df['Languages'] = df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# Create a column with the number of languages supported\n",
    "df['langs_count'] = df['Languages'].apply(lambda x: len(x))\n",
    "\n",
    "# Drop the English language from the Languages column (it is the most common language and would dominate the model)\n",
    "df['Languages'] = df['Languages'].apply(lambda x: [lang for lang in x if lang not in ['EN']])\n",
    "\n",
    "# Replace langs with counts less than 500 with 'infrequent_langs' as it would represent a very small percentage of the data (less than 10%)\n",
    "other = df['Languages'].explode().value_counts()[df['Languages'].explode().value_counts() < 500].index\n",
    "df['Languages'] = df['Languages'].apply(lambda x: [lang if lang not in other else 'infrequent' for lang in x])\n",
    "\n",
    "# Get dummy variables for the langs\n",
    "langs = pd.get_dummies(df['Languages'].apply(pd.Series).stack(), prefix='lang', dummy_na=False).sum(level=0)\n",
    "\n",
    "langs.to_csv('encoders/langs.csv', index=False)\n",
    "\n",
    "# Add the dummy variables to the original dataframe\n",
    "df = pd.concat([df, langs], axis=1)\n",
    "\n",
    "# Drop the original column\n",
    "df = df.drop(['Languages'], axis=1)\n",
    "\n",
    "# Fill NaN with 0\n",
    "lang_cols = [col for col in df.columns if col.startswith('lang')] # get all columns with prefix 'lang'\n",
    "df[lang_cols] = df[lang_cols].fillna(0) # fill NaN with 0 for selected columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi-label binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Convert the langs column to a list of strings\n",
    "df['Languages'] = df['Languages'].astype(str)\n",
    "df['Languages'] = df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# Create a column with the number of languages supported\n",
    "df['langs_count'] = df['Languages'].apply(lambda x: len(x))\n",
    "\n",
    "# Drop the English language from the Languages column (it is the most common language and would dominate the model)\n",
    "df['Languages'] = df['Languages'].apply(lambda x: [lang for lang in x if lang not in ['EN']])\n",
    "\n",
    "# Replace langs with counts less than 500 with 'infrequent_langs' as it would represent a very small percentage of the data (less than 10%)\n",
    "other = df['Languages'].explode().value_counts()[df['Languages'].explode().value_counts() < 400].index\n",
    "df['Languages'] = df['Languages'].apply(lambda x: [lang if lang not in other else 'infrequent_lang' for lang in x])\n",
    "\n",
    "# Instantiate the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit the MultiLabelBinarizer to the langs\n",
    "mlb.fit(df['Languages'])\n",
    "\n",
    "# Drop nan from the classes\n",
    "mlb.classes_ = np.delete(mlb.classes_, np.where(mlb.classes_ == 'nan'))\n",
    "\n",
    "# Save the mlb for later use with the test data\n",
    "pickle.dump(mlb, open('encoders/mlb_langs.pkl', 'wb'))\n",
    "\n",
    "# Transform the langs into a one-hot encoded array\n",
    "langs_mlb = mlb.transform(df['Languages'])\n",
    "\n",
    "# Create a dataframe from the one-hot encoded array\n",
    "langs_mlb_df = pd.DataFrame(langs_mlb, columns=mlb.classes_)\n",
    "\n",
    "# Add the encoded langs to the original dataframe\n",
    "df = pd.concat([df, langs_mlb_df], axis=1)\n",
    "\n",
    "# Drop the original column\n",
    "df = df.drop(['Languages'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## In-app Purchases preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:52.935497Z",
     "start_time": "2023-04-14T10:35:52.881403Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Free apps might skew the in-app purchases column,\n",
    "# so we might split the dataset into free and paid apps\n",
    "\n",
    "df['In-app Purchases'] = df['In-app Purchases'].astype(str)\n",
    "df['In-app Purchases'] = df['In-app Purchases'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:54.565885Z",
     "start_time": "2023-04-14T10:35:54.456681Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to float\n",
    "df['In-app Purchases'] = df['In-app Purchases'].apply(lambda x: [float(i) for i in x])\n",
    "\n",
    "# Get the number of in-app purchases\n",
    "df['purchases_count'] = df['In-app Purchases'].apply(lambda x: len(x))\n",
    "\n",
    "# Get the lowest, highest and average purchase\n",
    "df['lowest_purchase'] = df['In-app Purchases'].apply(lambda x: min(x) if len(x) > 0 else 0)\n",
    "df['highest_purchase'] = df['In-app Purchases'].apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "df['average_purchase'] = df['In-app Purchases'].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:36:00.624093Z",
     "start_time": "2023-04-14T10:36:00.576764Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop the original column\n",
    "df = df.drop(['In-app Purchases'], axis=1)\n",
    "\n",
    "df['lowest_purchase'] = df['lowest_purchase'].fillna(0)\n",
    "df['highest_purchase'] = df['highest_purchase'].fillna(0)\n",
    "df['average_purchase'] = df['average_purchase'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Age Rating preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:36:03.614263Z",
     "start_time": "2023-04-14T10:36:03.579637Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to string\n",
    "df['Age Rating'] = df['Age Rating'].astype(str)\n",
    "\n",
    "# Remove the + sign\n",
    "df['Age Rating'] = df['Age Rating'].str.replace('+', '')\n",
    "\n",
    "# Convert to int\n",
    "df['Age Rating'] = df['Age Rating'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NLP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:43:13.386866Z",
     "start_time": "2023-04-14T10:43:13.386866Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Yusuf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "def preprocess_nlp(col):\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "    # Remove URLs and email addresses\n",
    "    df[col] = df[col].apply(lambda x: re.sub(r'http\\S+|www.\\S+|\\S+@\\S+', '', x))\n",
    "\n",
    "    # Remove the punctuation, numbers, and convert to lowercase\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(re.findall(r'\\w+', x.lower())))\n",
    "\n",
    "    # Remove the stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "    # Stemming\n",
    "    st = nltk.PorterStemmer()\n",
    "    df[col] = df[col].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "    # Lemmatization\n",
    "    lem = nltk.WordNetLemmatizer()\n",
    "    df[col] = df[col].apply(lambda x: \" \".join([lem.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "    # Remove the frequent and rare words\n",
    "    freq = pd.Series(' '.join(df[col]).split()).value_counts()\n",
    "    common_freq = list(freq[:10].index)\n",
    "    rare_freq = list(freq[-10:].index)\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in common_freq+rare_freq))\n",
    "\n",
    "    # Remove the whitespaces\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(x.strip() for x in x.split()))\n",
    "\n",
    "    # Replace NaN values with empty string\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "    # Convert text data to bag-of-words representation\n",
    "    vectorizer = CountVectorizer()\n",
    "    BoW = vectorizer.fit_transform(df[col])\n",
    "\n",
    "    # Apply principal component analysis to reduce the dimensionality\n",
    "    pca_ = PCA(n_components=2)\n",
    "    pca_col = pca_.fit_transform(BoW.toarray())\n",
    "    \n",
    "    # Save the vectorizer and pca for later use with the test data\n",
    "    pickle.dump(vectorizer, open(f'encoders/vectorizer_{col}.pkl', 'wb'))\n",
    "    pickle.dump(pca_, open(f'encoders/pca_{col}.pkl', 'wb'))\n",
    "\n",
    "    # Add the PCA-transformed col to the original dataframe\n",
    "    for feat in range(len(pca_col[0])):\n",
    "        df[f'{col}_PCA_{feat}'] = pca_col[:, feat]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:43:49.505557Z",
     "start_time": "2023-04-14T10:43:49.505557Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Icon URL</th>\n",
       "      <th>User Rating Count</th>\n",
       "      <th>Price</th>\n",
       "      <th>Developer</th>\n",
       "      <th>Age Rating</th>\n",
       "      <th>Size</th>\n",
       "      <th>Original Release Date</th>\n",
       "      <th>Current Version Release Date</th>\n",
       "      <th>Average User Rating</th>\n",
       "      <th>game_age</th>\n",
       "      <th>...</th>\n",
       "      <th>purchases_count</th>\n",
       "      <th>lowest_purchase</th>\n",
       "      <th>highest_purchase</th>\n",
       "      <th>average_purchase</th>\n",
       "      <th>Description_PCA_0</th>\n",
       "      <th>Description_PCA_1</th>\n",
       "      <th>Subtitle_PCA_0</th>\n",
       "      <th>Subtitle_PCA_1</th>\n",
       "      <th>Name_PCA_0</th>\n",
       "      <th>Name_PCA_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2173</th>\n",
       "      <td>icons/2173.png</td>\n",
       "      <td>348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.022830</td>\n",
       "      <td>9.0</td>\n",
       "      <td>479550464</td>\n",
       "      <td>735920</td>\n",
       "      <td>736359</td>\n",
       "      <td>5.0</td>\n",
       "      <td>439</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>1.99</td>\n",
       "      <td>19.99</td>\n",
       "      <td>6.69</td>\n",
       "      <td>-0.323639</td>\n",
       "      <td>-0.112218</td>\n",
       "      <td>-0.024027</td>\n",
       "      <td>-0.013052</td>\n",
       "      <td>-0.023463</td>\n",
       "      <td>-0.027233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>icons/927.png</td>\n",
       "      <td>95466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>9.0</td>\n",
       "      <td>58547200</td>\n",
       "      <td>733705</td>\n",
       "      <td>736758</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3053</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>1.99</td>\n",
       "      <td>5.99</td>\n",
       "      <td>2.49</td>\n",
       "      <td>-0.324829</td>\n",
       "      <td>-0.023542</td>\n",
       "      <td>0.121598</td>\n",
       "      <td>0.450904</td>\n",
       "      <td>-0.024346</td>\n",
       "      <td>-0.029358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2499</th>\n",
       "      <td>icons/2499.png</td>\n",
       "      <td>822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.022830</td>\n",
       "      <td>17.0</td>\n",
       "      <td>22884352</td>\n",
       "      <td>735558</td>\n",
       "      <td>735977</td>\n",
       "      <td>3.5</td>\n",
       "      <td>419</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.311595</td>\n",
       "      <td>-0.108739</td>\n",
       "      <td>-0.024027</td>\n",
       "      <td>-0.013052</td>\n",
       "      <td>-0.056160</td>\n",
       "      <td>-0.029074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>icons/45.png</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.022830</td>\n",
       "      <td>4.0</td>\n",
       "      <td>164545536</td>\n",
       "      <td>736684</td>\n",
       "      <td>737228</td>\n",
       "      <td>4.5</td>\n",
       "      <td>544</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2.99</td>\n",
       "      <td>2.99</td>\n",
       "      <td>2.99</td>\n",
       "      <td>-0.332548</td>\n",
       "      <td>-0.128409</td>\n",
       "      <td>-0.031585</td>\n",
       "      <td>-0.019428</td>\n",
       "      <td>-0.031128</td>\n",
       "      <td>-0.014843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>icons/1741.png</td>\n",
       "      <td>147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.022830</td>\n",
       "      <td>17.0</td>\n",
       "      <td>117317632</td>\n",
       "      <td>736536</td>\n",
       "      <td>736661</td>\n",
       "      <td>4.0</td>\n",
       "      <td>125</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.99</td>\n",
       "      <td>-0.368250</td>\n",
       "      <td>-0.048989</td>\n",
       "      <td>-0.026865</td>\n",
       "      <td>-0.015441</td>\n",
       "      <td>-0.023557</td>\n",
       "      <td>-0.027381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Icon URL  User Rating Count  Price  Developer  Age Rating  \\\n",
       "2173  icons/2173.png                348    0.0   4.022830         9.0   \n",
       "927    icons/927.png              95466    0.0   4.166667         9.0   \n",
       "2499  icons/2499.png                822    0.0   4.022830        17.0   \n",
       "45      icons/45.png                 14    0.0   4.022830         4.0   \n",
       "1741  icons/1741.png                147    0.0   4.022830        17.0   \n",
       "\n",
       "           Size  Original Release Date  Current Version Release Date  \\\n",
       "2173  479550464                 735920                        736359   \n",
       "927    58547200                 733705                        736758   \n",
       "2499   22884352                 735558                        735977   \n",
       "45    164545536                 736684                        737228   \n",
       "1741  117317632                 736536                        736661   \n",
       "\n",
       "      Average User Rating  game_age  ...  purchases_count  lowest_purchase  \\\n",
       "2173                  5.0       439  ...               10             1.99   \n",
       "927                   4.0      3053  ...               10             1.99   \n",
       "2499                  3.5       419  ...                1             0.00   \n",
       "45                    4.5       544  ...                2             2.99   \n",
       "1741                  4.0       125  ...                1             9.99   \n",
       "\n",
       "      highest_purchase  average_purchase  Description_PCA_0  \\\n",
       "2173             19.99              6.69          -0.323639   \n",
       "927               5.99              2.49          -0.324829   \n",
       "2499              0.00              0.00          -0.311595   \n",
       "45                2.99              2.99          -0.332548   \n",
       "1741              9.99              9.99          -0.368250   \n",
       "\n",
       "      Description_PCA_1  Subtitle_PCA_0  Subtitle_PCA_1  Name_PCA_0  \\\n",
       "2173          -0.112218       -0.024027       -0.013052   -0.023463   \n",
       "927           -0.023542        0.121598        0.450904   -0.024346   \n",
       "2499          -0.108739       -0.024027       -0.013052   -0.056160   \n",
       "45            -0.128409       -0.031585       -0.019428   -0.031128   \n",
       "1741          -0.048989       -0.026865       -0.015441   -0.023557   \n",
       "\n",
       "      Name_PCA_1  \n",
       "2173   -0.027233  \n",
       "927    -0.029358  \n",
       "2499   -0.029074  \n",
       "45     -0.014843  \n",
       "1741   -0.027381  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_nlp('Description')\n",
    "preprocess_nlp('Subtitle')\n",
    "preprocess_nlp('Name')\n",
    "\n",
    "df = df.drop(['Description', 'Subtitle', 'Name'], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Icon preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Extract features from the icons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:52:48.299695Z",
     "start_time": "2023-04-14T10:52:13.578629Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess_icon(img_path):\n",
    "    # Load the game icon image\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (100, 100))\n",
    "\n",
    "    # Extract color features using color histograms\n",
    "    colors = ('b', 'g', 'r')\n",
    "    color_features = []\n",
    "    for k, col in enumerate(colors):\n",
    "        hist = cv2.calcHist([img], [k], None, [256], [0, 256])\n",
    "        color_features.append(hist)\n",
    "\n",
    "    # Reshape the color features to have a single dimension\n",
    "    color_features = np.concatenate(color_features).ravel()\n",
    "\n",
    "    # Extract shape features using edge detection\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "    edge_features = np.array(edges).flatten()\n",
    "\n",
    "    # Combine the color and shape features into a single feature vector\n",
    "    feature_vector = np.concatenate((color_features, edge_features))\n",
    "\n",
    "    # Normalize the feature vector to have unit length\n",
    "    normalized_feature_vector = feature_vector / np.linalg.norm(feature_vector)\n",
    "    \n",
    "    return normalized_feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4171/4171 [01:17<00:00, 53.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Create a list to store the feature vectors\n",
    "icon_features = []\n",
    "\n",
    "df['Icon URL'] = df['Icon URL'].astype(str)\n",
    "\n",
    "# Iterate over the images and extract the features\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    feature_vec = preprocess_icon(row['Icon URL'])\n",
    "    icon_features.append((row['Icon URL'], feature_vec))\n",
    "    \n",
    "# Apply PCA to reduce the number of features\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit([f[1] for f in icon_features])\n",
    "reduced_features = pca.transform([f[1] for f in icon_features])\n",
    "\n",
    "# Save the pca instance for later use\n",
    "pickle.dump(pca, open('encoders/icon_pca.pkl', 'wb'))\n",
    "\n",
    "# Convert the reduced features to a dataframe\n",
    "icon_features_df = pd.DataFrame({'Icon URL': [f[0] for f in icon_features],\n",
    "                                    'Icon1': reduced_features[:,0],\n",
    "                                    'Icon2': reduced_features[:,1],\n",
    "                                    'Icon3': reduced_features[:,2],\n",
    "                                    'Icon4': reduced_features[:,3]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Add the icon features to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:53:05.530442Z",
     "start_time": "2023-04-14T10:53:04.975260Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge the icon features with the original dataframe on the icon URL\n",
    "df = df.merge(icon_features_df, on='Icon URL', how='left')\n",
    "\n",
    "# Drop the icon URL column\n",
    "df = df.drop(['Icon URL'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed data\n",
    "df.to_csv('preprocessed_data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df.drop(['Average User Rating'], axis=1)\n",
    "df_y = df['Average User Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print only the columns with nan values and their count\n",
    "print(df_x.isnull().sum()[df_x.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cols = df_x.columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_x = scaler.fit_transform(df_x)\n",
    "\n",
    "df_x = pd.DataFrame(df_x, columns=cols)\n",
    "\n",
    "# Save the scaler\n",
    "pickle.dump(scaler, open('scalers/std_scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Convert all the features to float\n",
    "df_x = df_x.astype(float)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_x = scaler.fit_transform(df_x)\n",
    "\n",
    "# Save the scaler\n",
    "pickle.dump(scaler, open('scalers/min_max_scaler.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04742799,  0.46874204,  0.07180454, ...,  0.15172123,\n",
       "        -0.12848698, -0.15789988],\n",
       "       [ 0.44506702, -2.15771858,  0.58441265, ..., -0.32696391,\n",
       "        -0.1333208 , -0.17021877],\n",
       "       [-0.04742799,  0.03949656, -0.41896312, ..., -0.61075582,\n",
       "        -0.30753749, -0.16857245],\n",
       "       ...,\n",
       "       [-0.04742799, -0.66484548,  1.13684746, ..., -0.61075582,\n",
       "        -0.51073252, -0.34075156],\n",
       "       [-0.04742799, -1.51266459, -2.59401458, ..., -0.49792289,\n",
       "        -0.13829267, -0.14767906],\n",
       "       [-0.04742799, -2.05455737,  0.80924077, ..., -0.61075582,\n",
       "        -0.11635123, -0.32946449]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature selection\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "selector = SelectKBest(f_regression, k=10)\n",
    "df_x_select =  selector.fit_transform(df_x, df_y)\n",
    "\n",
    "# Save the selector\n",
    "pickle.dump(selector, open('encoders/selector.pkl', 'wb'))\n",
    "\n",
    "df_x_select"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T14:40:09.683179Z",
     "start_time": "2023-04-14T14:40:09.629559Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(df_x_select, df_y)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/LR_model.pkl', 'wb'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T11:00:32.712100Z",
     "start_time": "2023-04-14T11:00:32.642347Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a ridge regression model\n",
    "model = Ridge(alpha=0.5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(df_x_select, df_y)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/Ridge_model.pkl', 'wb'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T11:00:35.138470Z",
     "start_time": "2023-04-14T11:00:35.102309Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a lasso regression model\n",
    "model = Lasso(alpha=0.5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(df_x_select, df_y)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/Lasso_model.pkl', 'wb'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T11:00:37.282470Z",
     "start_time": "2023-04-14T11:00:37.226872Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an elastic net regression model\n",
    "model = ElasticNet(alpha=0.5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(df_x_select, df_y)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/ElasticNet_model.pkl', 'wb'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T11:02:11.403793Z",
     "start_time": "2023-04-14T11:00:55.155863Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a polynomial regression model\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_train_poly = poly.fit_transform(df_x_select)\n",
    "\n",
    "# Save the polynomial features\n",
    "pickle.dump(poly, open('encoders/poly.pkl', 'wb'))\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_poly, df_y)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/Polynomial_model.pkl', 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
