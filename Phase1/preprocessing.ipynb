{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T14:19:38.839351Z",
     "start_time": "2023-04-14T14:19:38.815268Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import plotly.graph_objs as go\n",
    "import seaborn as sns\n",
    "\n",
    "import cv2\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif, f_regression, mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "from category_encoders import TargetEncoder,  CountEncoder\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download nltk packages if not already downloaded\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:33:46.390000Z",
     "start_time": "2023-04-14T10:33:46.279076Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dateparse = lambda x: datetime.strptime(x, '%d/%m/%Y')\n",
    "\n",
    "df_origin = pd.read_csv('games-regression-dataset.csv', \n",
    "                        parse_dates=['Original Release Date' , 'Current Version Release Date'], \n",
    "                        date_parser=dateparse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Download the icons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to string\n",
    "df_origin['Icon URL'] = df_origin['Icon URL'].astype(str)\n",
    "\n",
    "# Download the images\n",
    "def download_image(url, filename):\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(filename, 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "\n",
    "# Create a folder to store the images\n",
    "if not os.path.exists('icons'):\n",
    "    os.makedirs('icons')\n",
    "\n",
    "# Download the images if they don't exist\n",
    "for i, row in tqdm(df_origin.iterrows(), total=df_origin.shape[0]):\n",
    "    if not os.path.exists('icons/' + str(i) + '.png'):\n",
    "        download_image(row['Icon URL'], 'icons/' + str(i) + '.png')\n",
    "        \n",
    "# Replace the URL with the icon filename which is the index of the row\n",
    "df_origin['Icon URL'] = df_origin.apply(lambda row : f'icons/{row.name}.png', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scrapping():\n",
    "    data = pd.DataFrame(columns = [\"ID\",\"Reviews\"])\n",
    "    # Read CSV file\n",
    "    # with open('games-regression-dataset.csv', newline='') as csvfile:\n",
    "    with open('games-regression-dataset.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader)  # Skip header row\n",
    "        for row in reader:\n",
    "            url = row[0]  # URL is in first column\n",
    "            filename = 'Reviews/'+os.path.basename(url)  # Extract filename from URL\n",
    "            url +=  \"?see-all=reviews\"\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:  # Check if request was successful\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                blocks = soup.findAll(\"blockquote\")\n",
    "                review_list = []\n",
    "                for blockquote in blocks:\n",
    "                    review = blockquote.find('p').text\n",
    "                    review_list.append(review)\n",
    "                if len(review_list)!=0:\n",
    "                    filename = re.sub(r'[^\\d]+', '', filename)\n",
    "                    new_row = {'ID': filename,\"Reviews\": review_list}\n",
    "                    data = data._append(new_row, ignore_index=True)\n",
    "                    \n",
    "    data.to_csv('Reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviews_splitting(data):\n",
    "\n",
    "    for i in range (len(data)):\n",
    "        data.at[i, 'Reviews'] = data.at[i, \"Reviews\"].split(\"',\")\n",
    "        data.at[i,\"ID\"] =data.at[i,\"ID\"]\n",
    "        \n",
    "    data = data.explode('Reviews')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviews_cleaning(data):\n",
    "  # Convert text to lowercase\n",
    "  data['Reviews'] = data['Reviews'].apply(lambda x: str(x).lower())\n",
    "\n",
    "  # Replace newline characters with an empty string\n",
    "  data['Reviews'] = data['Reviews'].apply(lambda x: re.sub(r'\\\\n', ' ', x))\n",
    "\n",
    "  # Remove black squares\n",
    "  data['Reviews'] = data['Reviews'].apply(lambda x: re.sub(r'\\\\u25a0', '', x))\n",
    "\n",
    "  # Remove special characters and punctuations\n",
    "  data['Reviews'] = data['Reviews'].apply(lambda x: re.sub(r'[^\\w\\s]+', '', x))\n",
    "\n",
    "  # Remove numbers\n",
    "  data['Reviews'] = data['Reviews'].apply(lambda x: \" \".join([word for word in x.split() if not any(char.isdigit() for char in word)]))\n",
    "\n",
    "  # Remove extra whitespaces\n",
    "  data['Reviews'] = data['Reviews'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "\n",
    "  # Remove stop words\n",
    "  data['Reviews'] = data['Reviews'].apply(lambda x: \" \".join([word for word in x.lower().split() if word not in stop_words]))\n",
    "\n",
    "  # Remove empty strings\n",
    "  data = data[data['Reviews'].apply(lambda x: len(x)>0)]\n",
    "  \n",
    "  # Group by ID\n",
    "  data = data.groupby('ID')['Reviews'].apply(list).reset_index()\n",
    "  \n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web_scrapping()\n",
    "\n",
    "df_reviews = pd.read_csv('Reviews.csv')\n",
    "\n",
    "df_reviews = reviews_splitting(df_reviews)\n",
    "\n",
    "df_reviews = reviews_cleaning(df_reviews)\n",
    "\n",
    "## Merge The Sentiment with the original dataset\n",
    "df_origin = df_origin.merge(df_reviews, on='ID', how='left')\n",
    "\n",
    "## drop the rows with no reviews\n",
    "df_origin = df_origin.dropna(subset=['Reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origin.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviews_preprocess(data, test=False):\n",
    "  \n",
    "  # Apply sentiment_analysis\n",
    "  \n",
    "  if test == False:\n",
    "    sia_reviews = SentimentIntensityAnalyzer()\n",
    "    pickle.dump(sia_reviews, open('encoders/sia_reviews.pkl', 'wb'))\n",
    "  else:\n",
    "    sia_reviews = pickle.load(open('encoders/sia_reviews.pkl', 'rb'))\n",
    "  \n",
    "  data['Reviews'] = data['Reviews'].apply(lambda x: [sia_reviews.polarity_scores(review)['compound'] for review in x])\n",
    "  \n",
    "  # Get the lowest, highest and average Reviews\n",
    "  data['lowest_review'] = data['Reviews'].apply(lambda x: min(x) if len(x) > 0 else 0)\n",
    "  data['highest_review'] = data['Reviews'].apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "  data['average_review'] = data['Reviews'].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
    "  \n",
    "  ## Drop nulls of sentiment\n",
    "  # data = data.dropna(subset=['lowest_review','highest_review','average_review'])\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T16:07:00.183708Z",
     "start_time": "2023-04-13T16:07:00.127616Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:05.138536Z",
     "start_time": "2023-04-13T14:33:05.138536Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:07.893814Z",
     "start_time": "2023-04-13T14:33:07.893814Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:21.139604Z",
     "start_time": "2023-04-13T14:33:21.139604Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def genres_analysis(_df):\n",
    "    _df['Genres'] = _df['Genres'].astype(str)\n",
    "    _df['Genres'] = _df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "    genre_counts = _df.explode('Genres').groupby('Genres').size().sort_values(ascending=False)\n",
    "    print(genre_counts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:43.637371Z",
     "start_time": "2023-04-13T14:33:43.637371Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dev_analysis(_df):\n",
    "\n",
    "    print(_df['Developer'].value_counts())\n",
    "    \n",
    "    # print the number developers with more than 1 game\n",
    "    print(len(_df['Developer'].value_counts()[_df['Developer'].value_counts() > 1]))\n",
    "\n",
    "    print(_df['Developer'].unique().size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_analysis(_df):\n",
    "    # Plot the distribution of the date columns\n",
    "\n",
    "    fig, ax = plt.subplots(5, 2, figsize=(20, 20))\n",
    "\n",
    "    # df = date_preprocessing(df)\n",
    "\n",
    "    # game_age distribution\n",
    "    sns.histplot(_df['game_age'], ax=ax[0, 0])\n",
    "    sns.boxplot(_df['game_age'], ax=ax[0, 1], orient='h')\n",
    "\n",
    "    # last_update distribution\n",
    "    sns.histplot(_df['last_update'], ax=ax[1, 0])\n",
    "    sns.boxplot(_df['last_update'], ax=ax[1, 1], orient='h')\n",
    "\n",
    "    # Original Release Date distribution\n",
    "    sns.histplot(_df['Original Release Date'], ax=ax[2, 0])\n",
    "    sns.boxplot(_df['Original Release Date'], ax=ax[2, 1], orient='h')\n",
    "\n",
    "    # Current Version Release Date distribution\n",
    "    sns.histplot(_df['Current Version Release Date'], ax=ax[3, 0])\n",
    "    sns.boxplot(_df['Current Version Release Date'], ax=ax[3, 1], orient='h')\n",
    "\n",
    "    # maintaning_period distribution\n",
    "    sns.histplot(_df['maintaning_period'], ax=ax[4, 0])\n",
    "    sns.boxplot(_df['maintaning_period'], ax=ax[4, 1], orient='h')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Date outliers are legitimate data points that are worth keeping, they are not errors nor anomalies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:49.943309Z",
     "start_time": "2023-04-13T14:33:49.943309Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lang_analysis(_df):\n",
    "    _df['Languages'] = _df['Languages'].astype(str)\n",
    "    _df['Languages'] = _df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "    langs_counts = _df.explode('Languages').groupby('Languages').size().sort_values(ascending=False)\n",
    "    print(langs_counts[1:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.hist(figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dates preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:36:05.737187Z",
     "start_time": "2023-04-14T10:36:05.699973Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def date_preprocess(_df, test=False, df_old=None):\n",
    "    # Convert the datetime to ordinal\n",
    "    _df['Original Release Date'] = _df['Original Release Date'].apply(lambda x: x.toordinal())\n",
    "    _df['Current Version Release Date'] = _df['Current Version Release Date'].apply(lambda x: x.toordinal())\n",
    "    \n",
    "    if test == False:\n",
    "        # Fill the missing values with the median\n",
    "        _df['Original Release Date'] = _df['Original Release Date'].fillna(_df['Original Release Date'].median())\n",
    "        _df['Current Version Release Date'] = _df['Current Version Release Date'].fillna(_df['Current Version Release Date'].median())\n",
    "    else:\n",
    "        # Fill the missing values with the median of the old dataset\n",
    "        _df['Original Release Date'] = _df['Original Release Date'].fillna(df_old['Original Release Date'].median())\n",
    "        _df['Current Version Release Date'] = _df['Current Version Release Date'].fillna(df_old['Current Version Release Date'].median())\n",
    "    \n",
    "    # Create a new column with the age of the game\n",
    "    _df['game_age'] = datetime.now().toordinal() - _df['Original Release Date']\n",
    "\n",
    "    # Create a new column with the time since the last update\n",
    "    _df['last_update'] = datetime.now().toordinal() - _df['Current Version Release Date']\n",
    "    \n",
    "    # Create a new column with the maintaning period\n",
    "    _df['maintaning_period'] = _df['game_age'] - _df['last_update']\n",
    "\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dates preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target encoding approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_preprocess_target_enc(_df, test=False):\n",
    "    \"\"\"Preprocesses and encodes the 'Developer' column using target encoding.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame (either train or test, depending on the value of test)\n",
    "        test (bool, optional): Boolean flag indicating whether the data is for testing (True) or training (False). Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: the preprocessed DataFrame with the new 'dev_avg' column\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert Developer column to string\n",
    "    _df['Developer'] = _df['Developer'].astype(str)\n",
    "    _df['Developer'] = _df['Developer'].str.replace(\"'\", \"\").str.strip('[]')  \n",
    "      \n",
    "    # Replace Developers with less than 2 games with 'Other'\n",
    "    if not test:\n",
    "        dev_counts = _df['Developer'].value_counts()\n",
    "        other = dev_counts[dev_counts < 2].index\n",
    "        _df['Developer'] = _df['Developer'].replace(other, 'Other')\n",
    "\n",
    "    # Perform target encoding on Developer column\n",
    "    if test:\n",
    "        te = pickle.load(open('encoders/dev_te.pkl', 'rb'))\n",
    "    else:\n",
    "        te = TargetEncoder(cols=['Developer']).fit(_df[['Developer']], _df['Average User Rating'])\n",
    "        pickle.dump(te, open('encoders/dev_te.pkl', 'wb'))\n",
    "\n",
    "    _df['dev_avg'] = te.transform(_df[['Developer']])\n",
    "\n",
    "    return _df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency encoding approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_preprocess_freq_enc(_df, test):\n",
    "    # Convert to string\n",
    "    _df['Developer'] = _df['Developer'].astype(str)\n",
    "    _df['Developer'] = _df['Developer'].str.replace(\"'\", \"\").str.strip('[]')\n",
    "    \n",
    "    if not test:\n",
    "        ce = CountEncoder(cols=['Developer']).fit(_df[['Developer']])\n",
    "        pickle.dump(ce, open('encoders/dev_ce.pkl', 'wb'))\n",
    "    else:\n",
    "        ce = pickle.load(open('encoders/dev_ce.pkl', 'rb'))\n",
    "    \n",
    "    _df['dev_freq'] = ce.transform(_df[['Developer']])['Developer']\n",
    "        \n",
    "    return _df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Genres preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NLP approach (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:21.245168Z",
     "start_time": "2023-04-14T10:35:20.885068Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def genres_preprocess_bow(_df, test=False):\n",
    "    # Convert the genres column to a list of strings\n",
    "    _df['Genres'] = _df['Genres'].astype(str)\n",
    "    _df['Genres'] = _df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "    # drop Games, Strategy, Entertainment from the Genres column\n",
    "    _df['Genres'] = _df['Genres'].apply(lambda x: [genre for genre in x if genre not in ['Games', 'Strategy', 'Entertainment']])\n",
    "\n",
    "    # Join the list of genres into a single string\n",
    "    genres = _df['Genres'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    # Create a count Vectorizer and fit it to the genres\n",
    "    count_vec = CountVectorizer()\n",
    "    bow_genres = count_vec.fit_transform(genres)\n",
    "    \n",
    "    # Save the count vectorizer to be used on the test set\n",
    "    pickle.dump(count_vec, open('encoders/count_vec_genre.pkl', 'wb'))\n",
    "\n",
    "    # Apply principal component analysis to reduce the dimensionality\n",
    "    pca = PCA(n_components=10)\n",
    "    pca_genres = pca.fit_transform(bow_genres.toarray())\n",
    "    \n",
    "    # Save the pca to be used on the test set\n",
    "    pickle.dump(pca, open('encoders/genre_pca.pkl', 'wb'))\n",
    "\n",
    "    # Add the PCA-transformed genres to the original dataframe\n",
    "    for i in range(10):\n",
    "        _df[f'genreN_{i}'] = pca_genres[:, i]\n",
    "\n",
    "    return _df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dummy variables approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genres_preprocess_dummies(_df, test=False):\n",
    "    # Convert the genres column to a list of strings\n",
    "    _df['Genres'] = _df['Genres'].astype(str)\n",
    "    _df['Genres'] = _df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "    \n",
    "    # drop Games, Strategy, Entertainment from the Genres column\n",
    "    _df['Genres'] = _df['Genres'].apply(lambda x: [genre for genre in x if genre not in ['Games', 'Strategy', 'Entertainment']])\n",
    "    \n",
    "    if not test:\n",
    "        \n",
    "        # Replace genres with counts less than 100 with 'infrequent' as it would represent a very small percentage of the data (less than 2%)\n",
    "        threshold = _df.shape[0] * 0.02\n",
    "        \n",
    "        other = _df['Genres'].explode().value_counts()[_df['Genres'].explode().value_counts() < threshold].index\n",
    "        _df['Genres'] = _df['Genres'].apply(lambda x: [genre if genre not in other else 'infrequent' for genre in x])\n",
    "        \n",
    "        # Get dummy variables for the genres\n",
    "        genres = pd.get_dummies(_df['Genres'].apply(pd.Series).stack(), prefix=\"genre\", dummy_na=False).sum(level=0)\n",
    "        \n",
    "        # Save the genres dummies to be used on the test set\n",
    "        genres.to_csv('encoders/genres.csv', index=False)\n",
    "    \n",
    "    else:\n",
    "        # Load saved genres dummy variables\n",
    "        saved_dummies = pd.read_csv('encoders/genres.csv')\n",
    "\n",
    "        # Get the genres that are not in the saved dummy variables\n",
    "        other = [genre for genre in _df['Genres'].explode().unique() if genre not in saved_dummies.columns]\n",
    "\n",
    "        # Replace the genres that are not in the saved dummy variables with 'infrequent'\n",
    "        _df['Genres'] = _df['Genres'].apply(lambda x: ['infrequent' if genre in other else genre for genre in x])\n",
    "\n",
    "        # Preprocess test data using the saved dummy variables\n",
    "        genres = pd.get_dummies(_df['Genres'].apply(pd.Series).stack(), prefix=\"genre\", dummy_na=False).sum(level=0)\n",
    "        genres = genres.reindex(columns=saved_dummies.columns, fill_value=0)\n",
    "        \n",
    "        # Fill the dummy columns with 0 if nan\n",
    "        genres = genres.fillna(0)\n",
    "\n",
    "    \n",
    "    # Add the dummy variables to the original dataframe\n",
    "    _df = pd.concat([_df, genres], axis=1)\n",
    "    \n",
    "    # Fill the NaN values with 0\n",
    "    genre_cols = [col for col in _df.columns if col.startswith('genre')] # get all columns with prefix 'genre'\n",
    "    _df[genre_cols] = _df[genre_cols].fillna(0) # fill the NaN values with 0\n",
    "    \n",
    "    return _df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Languages preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NLP approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:38.695352Z",
     "start_time": "2023-04-14T10:35:38.599717Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lang_preprocessing_bow(_df):\n",
    "    # Convert the langs column to a list of strings\n",
    "    _df['Languages'] = _df['Languages'].astype(str)\n",
    "    _df['Languages'] = _df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "    \n",
    "    # drop English from the Languages column\n",
    "    _df['Languages'] = _df['Languages'].apply(lambda x: [lang for lang in x if lang not in ['EN']])\n",
    "    \n",
    "    # Join the list of langs into a single string\n",
    "    languages = _df['Languages'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # Create a count Vectorizer and fit it to the langs\n",
    "    count_vec = CountVectorizer()\n",
    "    bow_languages = count_vec.fit_transform(languages)\n",
    "    \n",
    "    # Save the count vectorizer to be used on the test set\n",
    "    pickle.dump(count_vec, open('encoders/count_vec_lang.pkl', 'wb'))\n",
    "    \n",
    "    # Apply principal component analysis to reduce the dimensionality\n",
    "    pca = PCA(n_components=10)\n",
    "    pca_languages = pca.fit_transform(bow_languages.toarray())\n",
    "    \n",
    "    # Save the pca to be used on the test set\n",
    "    pickle.dump(pca, open('encoders/pca_lang.pkl', 'wb'))\n",
    "    \n",
    "    # Add the PCA-transformed langs to the original dataframe\n",
    "    for i in range(len(pca_languages[0])):\n",
    "        _df[f'lang_{i}'] = pca_languages[:, i]\n",
    "        \n",
    "    return _df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dummy variables approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langs_preprocess_dummies(_df, test = False):\n",
    "    # Convert the langs column to a list of strings\n",
    "    _df['Languages'] = _df['Languages'].astype(str)\n",
    "    _df['Languages'] = _df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "    \n",
    "    # Create a column with the number of languages supported\n",
    "    _df['langs_count'] = _df['Languages'].apply(lambda x: len(x)) \n",
    "    \n",
    "    # drop English from the Languages column\n",
    "    _df['Languages'] = _df['Languages'].apply(lambda x: [lang for lang in x if lang not in ['EN']])\n",
    "    \n",
    "    if not test:\n",
    "        # Replace languages with counts less than 100 with 'infrequent' as it would represent a very small percentage of the data (less than 10%)\n",
    "        threshold = _df.shape[0] * 0.1\n",
    "        \n",
    "        # Replace langs with counts less than 500 with 'infrequent_langs' as it would represent a very small percentage of the data (less than 10%)\n",
    "        other = _df['Languages'].explode().value_counts()[_df['Languages'].explode().value_counts() < threshold].index\n",
    "        _df['Languages'] = _df['Languages'].apply(lambda x: [lang if lang not in other else 'infrequent' for lang in x])\n",
    "\n",
    "        # Get dummy variables for the langs\n",
    "        langs = pd.get_dummies(_df['Languages'].apply(pd.Series).stack(), prefix='lang', dummy_na=False).sum(level=0)\n",
    "\n",
    "        langs.to_csv('encoders/langs.csv', index=False)\n",
    "    else:\n",
    "        saved_dummies = pd.read_csv('encoders/langs.csv')\n",
    "\n",
    "        # Get the languages that are not in the saved dummy variables\n",
    "        other = [lang for lang in _df['Languages'].explode().unique() if lang not in saved_dummies.columns]\n",
    "\n",
    "        # Replace the languages that are not in the saved dummy variables with 'infrequent'\n",
    "        _df['Languages'] = _df['Languages'].apply(lambda x: ['infrequent' if lang in other else lang for lang in x])\n",
    "\n",
    "        # Preprocess test data using the saved dummy variables\n",
    "        langs = pd.get_dummies(_df['Languages'].apply(pd.Series).stack(), prefix=\"lang\", dummy_na=False).sum(level=0)\n",
    "        langs = langs.reindex(columns=saved_dummies.columns, fill_value=0)\n",
    "\n",
    "        # Fill the dummy columns with 0 if nan\n",
    "        langs = langs.fillna(0)\n",
    "\n",
    "    # Add the dummy variables to the original dataframe\n",
    "    _df = pd.concat([_df, langs], axis=1)\n",
    "\n",
    "    # Fill NaN with 0\n",
    "    lang_cols = [col for col in _df.columns if col.startswith('lang')] # get all columns with prefix 'lang'\n",
    "    _df[lang_cols] = _df[lang_cols].fillna(0) # fill NaN with 0 for selected columns\n",
    "    \n",
    "    return _df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## In-app Purchases preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:52.935497Z",
     "start_time": "2023-04-14T10:35:52.881403Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Free apps might skew the in-app purchases column,\n",
    "# so we might split the dataset into free and paid apps\n",
    "\n",
    "def purchases_preprocess(_df):\n",
    "    # Convert the In-app Purchases column to a list of floats\n",
    "    _df['In-app Purchases'] = _df['In-app Purchases'].astype(str)\n",
    "    _df['In-app Purchases'] = _df['In-app Purchases'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "    # Convert to float  \n",
    "    _df['In-app Purchases'] = _df['In-app Purchases'].apply(lambda x: [float(i) for i in x])\n",
    "\n",
    "    # Get the number of in-app purchases\n",
    "    _df['purchases_count'] = _df['In-app Purchases'].apply(lambda x: len(x))\n",
    "\n",
    "    # Get the lowest, highest and average purchase\n",
    "    _df['lowest_purchase'] = _df['In-app Purchases'].apply(lambda x: min(x) if len(x) > 0 else 0)\n",
    "    _df['highest_purchase'] = _df['In-app Purchases'].apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "    _df['average_purchase'] = _df['In-app Purchases'].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n",
    "\n",
    "    _df['lowest_purchase'] = _df['lowest_purchase'].fillna(0)\n",
    "    _df['highest_purchase'] = _df['highest_purchase'].fillna(0)\n",
    "    _df['average_purchase'] = _df['average_purchase'].fillna(0)\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Age Rating & Price preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:36:03.614263Z",
     "start_time": "2023-04-14T10:36:03.579637Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def age_preprocess(_df, test=False, df_old=None):\n",
    "    # Convert to string\n",
    "    _df['Age Rating'] = _df['Age Rating'].astype(str)\n",
    "\n",
    "    # Remove the + sign\n",
    "    _df['Age Rating'] = _df['Age Rating'].str.replace('+', '')\n",
    "\n",
    "    # Convert to int\n",
    "    _df['Age Rating'] = _df['Age Rating'].astype(float)\n",
    "    \n",
    "    if test:\n",
    "        _df['Age Rating'] = _df['Age Rating'].fillna(df_old['Age Rating'].median())\n",
    "    else:\n",
    "        _df['Age Rating'] = _df['Age Rating'].fillna(_df['Age Rating'].median())\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_preprocess(_df):\n",
    "    # Convert to float\n",
    "    _df['Price'] = _df['Price'].astype(float)\n",
    "\n",
    "    # fill the missing values with 0 (free)\n",
    "    _df['Price'] = _df['Price'].fillna(0)\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NLP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:43:13.386866Z",
     "start_time": "2023-04-14T10:43:13.386866Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_nlp(_df, col):\n",
    "    # Convert to string\n",
    "    _df[col] = _df[col].astype(str)\n",
    "\n",
    "    # Remove URLs and email addresses\n",
    "    _df[col] = _df[col].apply(lambda x: re.sub(r'http\\S+|www.\\S+|\\S+@\\S+', '', x))\n",
    "\n",
    "    # Remove the punctuation, numbers, and convert to lowercase\n",
    "    _df[col] = _df[col].apply(lambda x: \" \".join(re.findall(r'\\w+', x.lower())))\n",
    "\n",
    "    # Remove the stopwords\n",
    "    _df[col] = _df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n",
    "\n",
    "    # Stemming\n",
    "    st = nltk.PorterStemmer()\n",
    "    _df[col] = _df[col].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "    # Lemmatization\n",
    "    lem = nltk.WordNetLemmatizer()\n",
    "    _df[col] = _df[col].apply(lambda x: \" \".join([lem.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "    # Remove the frequent and rare words\n",
    "    freq = pd.Series(' '.join(_df[col]).split()).value_counts()\n",
    "    common_freq = list(freq[:10].index)\n",
    "    rare_freq = list(freq[-10:].index)\n",
    "    _df[col] = _df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in common_freq+rare_freq))\n",
    "\n",
    "    # Remove the whitespaces\n",
    "    _df[col] = _df[col].apply(lambda x: \" \".join(x.strip() for x in x.split()))\n",
    "\n",
    "    # Replace NaN values with empty string\n",
    "    _df[col] = _df[col].fillna('')\n",
    "\n",
    "    # Convert text data to bag-of-words representation\n",
    "    vectorizer = CountVectorizer()\n",
    "    BoW = vectorizer.fit_transform(_df[col])\n",
    "\n",
    "    # Apply principal component analysis to reduce the dimensionality\n",
    "    pca_ = PCA(n_components=2)\n",
    "    pca_col = pca_.fit_transform(BoW.toarray())\n",
    "    \n",
    "    # Save the vectorizer and pca for later use with the test data\n",
    "    pickle.dump(vectorizer, open(f'encoders/vectorizer_{col}.pkl', 'wb'))\n",
    "    pickle.dump(pca_, open(f'encoders/pca_{col}.pkl', 'wb'))\n",
    "\n",
    "    # Add the PCA-transformed col to the original dataframe\n",
    "    for feat in range(len(pca_col[0])):\n",
    "        _df[f'{col}_PCA_{feat}'] = pca_col[:, feat]\n",
    "        \n",
    "    return _df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description, Name & Subtitle preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_excitement_score(text, _sia):\n",
    "    \n",
    "    # compute the polarity scores for the given text\n",
    "    scores = _sia.polarity_scores(text)\n",
    "    \n",
    "    # compute the excitement score as the sum of the positive and negative polarity scores\n",
    "    excitement_score = scores['pos'] + abs(scores['neg'])\n",
    "    \n",
    "    return excitement_score\n",
    "\n",
    "# define a function to compute an attractive score from a given text\n",
    "def compute_attractive_score(text, tokenizer):\n",
    "    # define a list of keywords that might make a game attractive to users\n",
    "    attractive_keywords = ['graphics', 'gameplay', 'storyline', 'characters']\n",
    "    \n",
    "    # tokenize the text into words and count how many attractive keywords appear\n",
    "    words = tokenizer(text.lower())\n",
    "    \n",
    "    num_attractive_keywords = len([word for word in words if word in attractive_keywords])\n",
    "    \n",
    "    # compute the attractive score as the ratio of attractive keywords to total words\n",
    "    attractive_score = num_attractive_keywords / len(words) if len(words) > 0 else 0\n",
    "    \n",
    "    return attractive_score\n",
    "\n",
    "def desc_preprocess(_df, test=False):\n",
    "    _df['Description'] = _df['Description'].astype(str)\n",
    "    \n",
    "    if not test:        \n",
    "        # load the Sentiment Intensity Analyzer model from NLTK\n",
    "        sia_desc = SentimentIntensityAnalyzer()\n",
    "        pickle.dump(sia_desc, open('encoders/sia_desc.pkl', 'wb'))\n",
    "\n",
    "        tokenizer = nltk.word_tokenize\n",
    "        pickle.dump(nltk.word_tokenize, open('encoders/desc_tokenizer.pkl', 'wb'))\n",
    "        \n",
    "    else:\n",
    "        sia_desc = pickle.load(open('encoders/sia_desc.pkl', 'rb'))\n",
    "        tokenizer = pickle.load(open('encoders/desc_tokenizer.pkl', 'rb'))\n",
    "    \n",
    "    _df['excitement_score'] = _df['Description'].apply(lambda x : compute_excitement_score(x, sia_desc))\n",
    "    _df['attractive_score'] = _df['Description'].apply(lambda x: compute_attractive_score(x, tokenizer))\n",
    "    \n",
    "    return _df\n",
    "\n",
    "def name_preprocess(_df, test=False):\n",
    "    _df['Name'] = _df['Name'].astype(str)\n",
    "    \n",
    "    if not test:\n",
    "        sia_name = SentimentIntensityAnalyzer()\n",
    "        pickle.dump(sia_name, open('encoders/sia_name.pkl', 'wb'))\n",
    "    else:\n",
    "        sia_name = pickle.load(open('encoders/sia_name.pkl', 'rb'))\n",
    "    \n",
    "    _df['name_sia'] = _df['Name'].apply(lambda x : compute_excitement_score(x, sia_name))\n",
    "    \n",
    "    return _df\n",
    "\n",
    "def sub_preprocess(_df, test=False):\n",
    "    _df['Subtitle'] = _df['Subtitle'].astype(str)\n",
    "    \n",
    "    if not test:\n",
    "        sia_sub = SentimentIntensityAnalyzer()\n",
    "        pickle.dump(sia_sub, open('encoders/sia_sub.pkl', 'wb'))\n",
    "    else:\n",
    "        sia_sub = pickle.load(open('encoders/sia_sub.pkl', 'rb'))\n",
    "    \n",
    "    _df['sub_sia'] = _df['Subtitle'].apply(lambda x : compute_excitement_score(x, sia_sub))\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Icon preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:52:48.299695Z",
     "start_time": "2023-04-14T10:52:13.578629Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_icon(img_path):\n",
    "    \n",
    "        # Load the game icon image\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "\n",
    "    # Extract color features using color histograms\n",
    "    img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    color_features = []\n",
    "    for i in range(3):\n",
    "        hist = cv2.calcHist([img_lab], [i], None, [256], [0, 256])\n",
    "        color_features.append(hist.ravel())\n",
    "    color_features = np.concatenate(color_features)\n",
    "\n",
    "    # Extract shape features using local binary patterns\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    lbp = local_binary_pattern(gray, 8, 1, method='uniform')\n",
    "    hist_lbp, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 10), range=(0, 9))\n",
    "    edge_features = hist_lbp.astype(float)\n",
    "\n",
    "    # Combine the color and shape features into a single feature vector\n",
    "    feature_vector = np.concatenate((color_features, edge_features))\n",
    "\n",
    "    # Normalize the feature vector to have unit length\n",
    "    normalized_feature_vector = feature_vector / np.linalg.norm(feature_vector)\n",
    "    \n",
    "    return normalized_feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def icons_preprocess(_df, test=False):\n",
    "\n",
    "    # Create a list to store the feature vectors\n",
    "    icon_features = []\n",
    "\n",
    "    _df['Icon URL'] = _df['Icon URL'].astype(str)\n",
    "\n",
    "    # Iterate over the images and extract the features\n",
    "    for i, row in tqdm(_df.iterrows(), total=_df.shape[0]):\n",
    "        feature_vec = preprocess_icon(row['Icon URL'])\n",
    "        icon_features.append((row['Icon URL'], feature_vec))\n",
    "        \n",
    "    if not test:\n",
    "        # Apply PCA to reduce the number of features\n",
    "        pca = PCA(n_components=4)\n",
    "        pca.fit([f[1] for f in icon_features])\n",
    "\n",
    "        # Save the pca instance for later use\n",
    "        pickle.dump(pca, open('encoders/icon_pca.pkl', 'wb'))\n",
    "    \n",
    "    else:\n",
    "        pca = pickle.load(open('encoders/icon_pca.pkl', 'rb'))\n",
    "    \n",
    "    reduced_features = pca.transform([f[1] for f in icon_features])\n",
    "\n",
    "    # Convert the reduced features to a dataframe\n",
    "    icon_features_df = pd.DataFrame({'Icon URL': [f[0] for f in icon_features],\n",
    "                                        'Icon1': reduced_features[:,0],\n",
    "                                        'Icon2': reduced_features[:,1],\n",
    "                                        'Icon3': reduced_features[:,2],\n",
    "                                        'Icon4': reduced_features[:,3]})\n",
    "    \n",
    "    # Merge the icon features with the original dataframe on the icon URL\n",
    "    _df = _df.merge(icon_features_df, on='Icon URL', how='left')\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(_df, test=False):\n",
    "    _df = _df.drop(['Primary Genre', 'ID', 'URL'], axis=1)\n",
    "    \n",
    "    if test:\n",
    "        df_old = pd.read_csv('preprocessed_data.csv') \n",
    "    else:\n",
    "        df_old = None\n",
    "          \n",
    "    _df = reviews_preprocess(_df, test)\n",
    "    _df = date_preprocess(_df, test, df_old)\n",
    "    \n",
    "    _df = dev_preprocess_freq_enc(_df, test)\n",
    "    _df = dev_preprocess_target_enc(_df, test)\n",
    "    \n",
    "    _df = genres_preprocess_dummies(_df, test)\n",
    "    _df = langs_preprocess_dummies(_df, test)\n",
    "    \n",
    "    _df = purchases_preprocess(_df)\n",
    "    \n",
    "    _df = age_preprocess(_df, test, df_old)\n",
    "    _df = price_preprocess(_df)\n",
    "    \n",
    "    _df = name_preprocess(_df, test)\n",
    "    _df = sub_preprocess(_df, test)\n",
    "    _df = desc_preprocess(_df, test)\n",
    "    \n",
    "    _df = icons_preprocess(_df, test)\n",
    "    \n",
    "    _df = _df.drop(['Developer',\n",
    "              'Genres',\n",
    "              'Languages',\n",
    "              'Reviews',\n",
    "              'In-app Purchases',\n",
    "              'Description',\n",
    "              'Subtitle',\n",
    "              'Name', \n",
    "              'Icon URL'], axis=1)\n",
    "    \n",
    "    if not test:\n",
    "        _df.to_csv('preprocessed_data.csv', index=False)\n",
    "        \n",
    "    # Columns needed for the testing phase but nor for the prediction model anymore\n",
    "    _df = _df.drop(['Original Release Date',\n",
    "              'Current Version Release Date'], axis=1)\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, df_test = train_test_split(df_origin, test_size=0.2, random_state=42)\n",
    "\n",
    "df = preprocess_pipeline(df)\n",
    "df_test = preprocess_pipeline(df_test, test=True)\n",
    "\n",
    "df_x, df_y = df.drop('Average User Rating', axis=1), df['Average User Rating']\n",
    "df_test_x, df_test_y = df_test.drop('Average User Rating', axis=1), df_test['Average User Rating']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(mse_train, r2_train, mse_test, r2_test):\n",
    "    # Calculate the differences between the training and testing scores\n",
    "    mse_diff =  abs(mse_train - mse_test)\n",
    "    r2_diff = abs(r2_train - r2_test)\n",
    "\n",
    "    # Determine whether each score is better or worse than the other\n",
    "    mse_color = 'green' if mse_diff < 0.1 else 'red'\n",
    "    r2_color = 'green' if r2_diff < 0.1 else 'red'\n",
    "    \n",
    "    # Create a bar plot of the scores with colors based on the value\n",
    "    fig = go.Figure(data=[\n",
    "        go.Bar(name='MSE Train', x=['MSE'], y=[mse_train], marker_color='navy'),\n",
    "        go.Bar(name='MSE Test', x=['MSE'], y=[mse_test], marker_color='blue'),\n",
    "        go.Bar(name='MSE Difference', x=['MSE'], y=[mse_diff], marker_color=mse_color),\n",
    "        go.Bar(name='R2 Train', x=['R2'], y=[r2_train], marker_color='purple'),\n",
    "        go.Bar(name='R2 Test', x=['R2'], y=[r2_test], marker_color='pink'),\n",
    "        go.Bar(name='R2 Difference', x=['R2'], y=[r2_diff], marker_color=r2_color)\n",
    "    ])\n",
    "\n",
    "    # Add labels and title\n",
    "    fig.update_layout(title='Model Performance', xaxis_title='Score Type', yaxis_title='Score')\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "\n",
    "def scale_data_std(_df, test=False):\n",
    "    cols = _df.columns\n",
    "    \n",
    "    if not test:\n",
    "        scaler = StandardScaler()\n",
    "        _df = scaler.fit_transform(_df)\n",
    "        \n",
    "        # Save the scaler\n",
    "        pickle.dump(scaler, open('scalers/std_scaler.pkl', 'wb'))\n",
    "    else:\n",
    "        scaler = pickle.load(open('scalers/std_scaler.pkl', 'rb'))\n",
    "        _df = scaler.transform(_df)\n",
    "    \n",
    "    _df = pd.DataFrame(_df, columns=cols)\n",
    "    return _df\n",
    "\n",
    "def scale_data_minmax(_df, test=False):\n",
    "    cols = _df.columns\n",
    "    \n",
    "    if not test:\n",
    "        scaler = MinMaxScaler()\n",
    "        _df = scaler.fit_transform(_df)\n",
    "        \n",
    "        # Save the scaler\n",
    "        pickle.dump(scaler, open('scalers/minmax_scaler.pkl', 'wb'))\n",
    "    \n",
    "    else:\n",
    "        scaler = pickle.load(open('scalers/minmax_scaler.pkl', 'rb'))\n",
    "        _df = scaler.transform(_df)\n",
    "    \n",
    "    _df = pd.DataFrame(_df, columns=cols)\n",
    "    return _df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation matrix between features and target\n",
    "\n",
    "corr = df_x.corrwith(df_y)\n",
    "corr = corr.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x=corr.values, y=corr.index)\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(_df_x, _df_y, num_features=8, test=False):\n",
    "    if not test:\n",
    "        selector = SelectKBest(f_regression, k=num_features)\n",
    "        selector.fit(_df_x, _df_y)\n",
    "        \n",
    "        # Save the selector\n",
    "        pickle.dump(selector, open('encoders/selector.pkl', 'wb'))\n",
    "        \n",
    "    else:\n",
    "        selector = pickle.load(open('encoders/selector.pkl', 'rb'))\n",
    "        \n",
    "    _df_x = selector.transform(_df_x)\n",
    "    \n",
    "    return _df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "df_x_scaled = scale_data_minmax(df_x)\n",
    "df_test_x_scaled = scale_data_minmax(df_test_x, test=True)\n",
    "\n",
    "# Select the features\n",
    "df_x_select = select_features(df_x_scaled, df_y, 12)\n",
    "df_test_x_select = select_features(df_test_x_scaled, df_test_y, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = pickle.load(open('encoders/selector.pkl', 'rb'))\n",
    "# Print the selected features\n",
    "for i in range(len(selector.get_support())): \n",
    "    if selector.get_support()[i]:\n",
    "        print(df_x.columns[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T14:40:09.683179Z",
     "start_time": "2023-04-14T14:40:09.629559Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(df_x_select, df_y)\n",
    "\n",
    "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
    "mse_train = mean_squared_error(df_y, model.predict(df_x_select))\n",
    "r2_train = r2_score(df_y, model.predict(df_x_select))\n",
    "mse_test = mean_squared_error(df_test_y, model.predict(df_test_x_select))\n",
    "r2_test = r2_score(df_test_y, model.predict(df_test_x_select))\n",
    "\n",
    "# Print the training MSE and R2 score\n",
    "print('Training MSE: ', mse_train)\n",
    "print('Training R2: ', r2_train)\n",
    "\n",
    "# Print the testing MSE and R2 score\n",
    "print('Testing MSE: ', mse_test)\n",
    "print('Testing R2: ', r2_test)\n",
    "\n",
    "# Display the scores\n",
    "plot_scores(mse_train, r2_train, mse_test, r2_test)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/LR_model.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T11:00:32.712100Z",
     "start_time": "2023-04-14T11:00:32.642347Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a ridge regression model\n",
    "model = Ridge(alpha=20)\n",
    "\n",
    "# Train the model\n",
    "model.fit(df_x_select, df_y)\n",
    "\n",
    "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
    "mse_train = mean_squared_error(df_y, model.predict(df_x_select))\n",
    "r2_train = r2_score(df_y, model.predict(df_x_select))\n",
    "mse_test = mean_squared_error(df_test_y, model.predict(df_test_x_select))\n",
    "r2_test = r2_score(df_test_y, model.predict(df_test_x_select))\n",
    "\n",
    "# Print the training MSE and R2 score\n",
    "print('Training MSE: ', mse_train)\n",
    "print('Training R2: ', r2_train)\n",
    "\n",
    "# Print the testing MSE and R2 score\n",
    "print('Testing MSE: ', mse_test)\n",
    "print('Testing R2: ', r2_test)\n",
    "\n",
    "# Display the scores\n",
    "plot_scores(mse_train, r2_train, mse_test, r2_test)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/Ridge_model.pkl', 'wb'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T11:00:35.138470Z",
     "start_time": "2023-04-14T11:00:35.102309Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a lasso regression model\n",
    "model = Lasso(alpha=0.001)\n",
    "\n",
    "# Train the model\n",
    "model.fit(df_x_select, df_y)\n",
    "\n",
    "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
    "mse_train = mean_squared_error(df_y, model.predict(df_x_select))\n",
    "r2_train = r2_score(df_y, model.predict(df_x_select))\n",
    "mse_test = mean_squared_error(df_test_y, model.predict(df_test_x_select))\n",
    "r2_test = r2_score(df_test_y, model.predict(df_test_x_select))\n",
    "\n",
    "# Print the training MSE and R2 score\n",
    "print('Training MSE: ', mse_train)\n",
    "print('Training R2: ', r2_train)\n",
    "\n",
    "# Print the testing MSE and R2 score\n",
    "print('Testing MSE: ', mse_test)\n",
    "print('Testing R2: ', r2_test)\n",
    "\n",
    "# Display the scores\n",
    "plot_scores(mse_train, r2_train, mse_test, r2_test)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/Lasso_model.pkl', 'wb'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T11:00:37.282470Z",
     "start_time": "2023-04-14T11:00:37.226872Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an elastic net regression model\n",
    "model = ElasticNet(alpha=0.001)\n",
    "\n",
    "# Train the model\n",
    "model.fit(df_x_select, df_y)\n",
    "\n",
    "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
    "mse_train = mean_squared_error(df_y, model.predict(df_x_select))\n",
    "r2_train = r2_score(df_y, model.predict(df_x_select))\n",
    "mse_test = mean_squared_error(df_test_y, model.predict(df_test_x_select))\n",
    "r2_test = r2_score(df_test_y, model.predict(df_test_x_select))\n",
    "\n",
    "# Print the training MSE and R2 score\n",
    "print('Training MSE: ', mse_train)\n",
    "print('Training R2: ', r2_train)\n",
    "\n",
    "# Print the testing MSE and R2 score\n",
    "print('Testing MSE: ', mse_test)\n",
    "print('Testing R2: ', r2_test)\n",
    "\n",
    "# Display the scores\n",
    "plot_scores(mse_train, r2_train, mse_test, r2_test)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/ElasticNet_model.pkl', 'wb'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T11:02:11.403793Z",
     "start_time": "2023-04-14T11:00:55.155863Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a polynomial regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Generate the polynomial features\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "df_x_poly = poly.fit_transform(df_x_select)\n",
    "df_test_x_poly = poly.transform(df_test_x_select)\n",
    "\n",
    "# Save the polynomial features\n",
    "pickle.dump(poly, open('encoders/poly.pkl', 'wb'))\n",
    "\n",
    "# Train the model\n",
    "model.fit(df_x_poly, df_y)\n",
    "\n",
    "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
    "mse_train = mean_squared_error(df_y, model.predict(df_x_poly))\n",
    "r2_train = r2_score(df_y, model.predict(df_x_poly))\n",
    "mse_test = mean_squared_error(df_test_y, model.predict(df_test_x_poly))\n",
    "r2_test = r2_score(df_test_y, model.predict(df_test_x_poly))\n",
    "\n",
    "# Print the training MSE and R2 score\n",
    "print('Training MSE: ', mse_train)\n",
    "print('Training R2: ', r2_train)\n",
    "\n",
    "# Print the testing MSE and R2 score\n",
    "print('Testing MSE: ', mse_test)\n",
    "print('Testing R2: ', r2_test)\n",
    "\n",
    "# Display the scores\n",
    "plot_scores(mse_train, r2_train, mse_test, r2_test)\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/Polynomial_model.pkl', 'wb'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an XGBoost model\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBRegressor(subsample= 0.8,\n",
    "                         reg_lambda= 0.1,\n",
    "                         reg_alpha= 0.1,\n",
    "                         n_estimators=500,\n",
    "                         min_child_weight=1,\n",
    "                         max_depth= 4,\n",
    "                         learning_rate= 0.01,\n",
    "                         gamma =0.1,\n",
    "                         colsample_bytree= 0.8)\n",
    "\n",
    "# Train the model\n",
    "model.fit(df_x_select, df_y)\n",
    "\n",
    "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
    "mse_train = mean_squared_error(df_y, model.predict(df_x_select))\n",
    "r2_train = r2_score(df_y, model.predict(df_x_select))\n",
    "mse_test = mean_squared_error(df_test_y, model.predict(df_test_x_select))\n",
    "r2_test = r2_score(df_test_y, model.predict(df_test_x_select))\n",
    "\n",
    "# Print the training MSE and R2 score\n",
    "print('Training MSE: ', mse_train)\n",
    "print('Training R2: ', r2_train)\n",
    "\n",
    "# Print the testing MSE and R2 score\n",
    "print('Testing MSE: ', mse_test)\n",
    "print('Testing R2: ', r2_test)\n",
    "\n",
    "# Display the scores\n",
    "plot_scores(mse_train, r2_train, mse_test, r2_test)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/XGBoost_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the XGBoost regressor\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "# Define the hyperparameter grid for RandomizedSearchCV\n",
    "params = {\n",
    "    'learning_rate': [0.01],\n",
    "    'max_depth': [3, 4],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'n_estimators': [500, 1000],\n",
    "    'min_child_weight': [1],\n",
    "    'gamma': [0.1],\n",
    "    'reg_alpha': [0.1],\n",
    "    'reg_lambda': [0.1]\n",
    "}\n",
    "\n",
    "# Define the RandomizedSearchCV object\n",
    "rs_cv = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=params,\n",
    "    n_iter=10,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV object to the data\n",
    "rs_cv.fit(df_x_select, df_y)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(rs_cv.best_params_)\n",
    "\n",
    "# Use the best hyperparameters to train the final model\n",
    "final_xgb_model = xgb.XGBRegressor(**rs_cv.best_params_)\n",
    "final_xgb_model.fit(df_x_select, df_y)\n",
    "\n",
    "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
    "mse_train = mean_squared_error(df_y, final_xgb_model.predict(df_x_select))\n",
    "r2_train = r2_score(df_y, final_xgb_model.predict(df_x_select))\n",
    "mse_test = mean_squared_error(df_test_y, final_xgb_model.predict(df_test_x_select))\n",
    "r2_test = r2_score(df_test_y, final_xgb_model.predict(df_test_x_select))\n",
    "\n",
    "# Print the training MSE and R2 score\n",
    "print('Training MSE: ', mse_train)\n",
    "print('Training R2: ', r2_train)\n",
    "\n",
    "# Print the testing MSE and R2 score\n",
    "print('Testing MSE: ', mse_test)\n",
    "print('Testing R2: ', r2_test)\n",
    "\n",
    "# Display the scores\n",
    "plot_scores(mse_train, r2_train, mse_test, r2_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gradient Boosting model\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model = GradientBoostingRegressor(learning_rate=0.01, \n",
    "                                  max_depth=5, \n",
    "                                  max_features='sqrt', \n",
    "                                  min_samples_leaf=1, \n",
    "                                  min_samples_split=2, \n",
    "                                  n_estimators=500, \n",
    "                                  subsample=0.8)\n",
    "\n",
    "# Train the model\n",
    "model.fit(df_x_select, df_y)\n",
    "\n",
    "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
    "mse_train = mean_squared_error(df_y, model.predict(df_x_select))\n",
    "r2_train = r2_score(df_y, model.predict(df_x_select))\n",
    "mse_test = mean_squared_error(df_test_y, model.predict(df_test_x_select))\n",
    "r2_test = r2_score(df_test_y, model.predict(df_test_x_select))\n",
    "\n",
    "# Print the training MSE and R2 score\n",
    "print('Training MSE: ', mse_train)\n",
    "print('Training R2: ', r2_train)\n",
    "\n",
    "# Print the testing MSE and R2 score\n",
    "print('Testing MSE: ', mse_test)\n",
    "print('Testing R2: ', r2_test)\n",
    "\n",
    "# Display the scores\n",
    "plot_scores(mse_train, r2_train, mse_test, r2_test)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/GradientBoosting_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting with RandomizedSearchCV on GPU\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the Gradient Boosting regressor\n",
    "gb_model = GradientBoostingRegressor()\n",
    "\n",
    "# Define the hyperparameter grid for RandomizedSearchCV\n",
    "params = {\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'max_depth': [4, 5],\n",
    "    'subsample': [0.8],\n",
    "    'n_estimators': [500, 1000],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Define the RandomizedSearchCV object\n",
    "rs_cv = RandomizedSearchCV(\n",
    "    estimator=gb_model,\n",
    "    param_distributions=params,\n",
    "    n_iter=10,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV object to the data\n",
    "rs_cv.fit(df_x_select, df_y)\n",
    " \n",
    "# Print the best hyperparameters\n",
    "print(rs_cv.best_params_)\n",
    "\n",
    "# Use the best hyperparameters to train the final model\n",
    "final_gb_model = GradientBoostingRegressor(**rs_cv.best_params_)\n",
    "final_gb_model.fit(df_x_select, df_y)\n",
    "\n",
    "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
    "mse_train = mean_squared_error(df_y, final_gb_model.predict(df_x_select))\n",
    "r2_train = r2_score(df_y, final_gb_model.predict(df_x_select))\n",
    "mse_test = mean_squared_error(df_test_y, final_gb_model.predict(df_test_x_select))\n",
    "r2_test = r2_score(df_test_y, final_gb_model.predict(df_test_x_select))\n",
    "\n",
    "# Print the training MSE and R2 score\n",
    "print('Training MSE: ', mse_train)\n",
    "print('Training R2: ', r2_train)\n",
    "\n",
    "# Print the testing MSE and R2 score\n",
    "print('Testing MSE: ', mse_test)\n",
    "print('Testing R2: ', r2_test)\n",
    " \n",
    "# Display the scores\n",
    "plot_scores(mse_train, r2_train, mse_test, r2_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest Regressor model\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(max_depth=5, \n",
    "                              n_estimators=500, \n",
    "                              n_jobs=-1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(df_x_select, df_y)\n",
    "\n",
    "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
    "mse_train = mean_squared_error(df_y, model.predict(df_x_select))\n",
    "r2_train = r2_score(df_y, model.predict(df_x_select))\n",
    "mse_test = mean_squared_error(df_test_y, model.predict(df_test_x_select))\n",
    "r2_test = r2_score(df_test_y, model.predict(df_test_x_select))\n",
    "\n",
    "# Print the training MSE and R2 score\n",
    "print('Training MSE: ', mse_train)\n",
    "print('Training R2: ', r2_train)\n",
    "\n",
    "# Print the testing MSE and R2 score\n",
    "print('Testing MSE: ', mse_test)\n",
    "print('Testing R2: ', r2_test)\n",
    "\n",
    "# Display the scores\n",
    "plot_scores(mse_train, r2_train, mse_test, r2_test)\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/RandomForest_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest with GridSearchCV on GPU\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the Random Forest regressor\n",
    "rf_model = RandomForestRegressor()\n",
    "\n",
    "# Define the hyperparameter grid for GridSearchCV\n",
    "params = {\n",
    "    'max_depth': [4, 5],\n",
    "    'n_estimators': [500, 1000],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'random_state': [42],\n",
    "    'verbose': [1],\n",
    "    'warm_start': [True],\n",
    "    'bootstrap': [True],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'criterion': ['squared_error'],\n",
    "    'oob_score': [True]\n",
    "}\n",
    "\n",
    "# Define the GridSearchCV object\n",
    "gs_cv = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=params,\n",
    "    cv=5,\n",
    "    n_jobs=6,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    return_train_score=True,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "gs_cv.fit(df_x_select, df_y)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(gs_cv.best_params_)\n",
    "\n",
    "# Use the best hyperparameters to train the final model\n",
    "final_rf_model = RandomForestRegressor(**gs_cv.best_params_)\n",
    "\n",
    "final_rf_model.fit(df_x_select, df_y)\n",
    "\n",
    "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
    "mse_train = mean_squared_error(df_y, final_rf_model.predict(df_x_select))\n",
    "r2_train = r2_score(df_y, final_rf_model.predict(df_x_select))\n",
    "mse_test = mean_squared_error(df_test_y, final_rf_model.predict(df_test_x_select))\n",
    "r2_test = r2_score(df_test_y, final_rf_model.predict(df_test_x_select))\n",
    "\n",
    "# Print the training MSE and R2 score\n",
    "print('Training MSE: ', mse_train)\n",
    "print('Training R2: ', r2_train)\n",
    "\n",
    "# Print the testing MSE and R2 score\n",
    "print('Testing MSE: ', mse_test)\n",
    "print('Testing R2: ', r2_test)\n",
    "\n",
    "# Display the scores\n",
    "plot_scores(mse_train, r2_train, mse_test, r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create CatBoost Regressor model\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "model = CatBoostRegressor(iterations=250,\n",
    "                            learning_rate=0.05,\n",
    "                            depth=3,\n",
    "                            l2_leaf_reg=1,\n",
    "                            border_count=32,\n",
    "                            bagging_temperature=1,\n",
    "                            fold_permutation_block=1,\n",
    "                            boosting_type='Plain',\n",
    "                            random_seed=42,\n",
    "                            subsample=1.0,\n",
    "                            colsample_bylevel=0.7,\n",
    "                            early_stopping_rounds=5,\n",
    "                            loss_function='RMSE', eval_metric='RMSE',\n",
    "                          verbose=50)\n",
    "\n",
    "# Train the model\n",
    "model.fit(df_x_select, df_y)\n",
    "\n",
    "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
    "mse_train = mean_squared_error(df_y, model.predict(df_x_select))\n",
    "r2_train = r2_score(df_y, model.predict(df_x_select))\n",
    "mse_test = mean_squared_error(df_test_y, model.predict(df_test_x_select))\n",
    "r2_test = r2_score(df_test_y, model.predict(df_test_x_select))\n",
    "\n",
    "# Print the training MSE and R2 score\n",
    "print('Training MSE: ', mse_train)\n",
    "print('Training R2: ', r2_train)\n",
    "\n",
    "# Print the testing MSE and R2 score\n",
    "print('Testing MSE: ', mse_test)\n",
    "print('Testing R2: ', r2_test)\n",
    "\n",
    "# Display the scores\n",
    "plot_scores(mse_train, r2_train, mse_test, r2_test)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/CatBoost_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'iterations': [250],\n",
    "    'learning_rate': [0.05],\n",
    "    'depth': [3],\n",
    "    'l2_leaf_reg': [1],\n",
    "    'border_count': [32],\n",
    "    'bagging_temperature': [1],\n",
    "    'fold_permutation_block': [1],\n",
    "    'boosting_type': ['Plain'],\n",
    "    'random_seed': [42],\n",
    "    'subsample': [1.0],\n",
    "    'colsample_bylevel': [0.7],\n",
    "    'early_stopping_rounds': [5],\n",
    "}\n",
    "\n",
    "# Create a CatBoost regressor\n",
    "clf = CatBoostRegressor(loss_function='RMSE', eval_metric='RMSE', verbose=250)\n",
    "\n",
    "# Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(df_x_select, df_y)\n",
    "\n",
    "# Print best hyperparameters and corresponding score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "# Train final model with best hyperparameters on entire dataset\n",
    "best_clf = CatBoostRegressor(loss_function='RMSE', eval_metric='RMSE', logging_level='Silent', **grid_search.best_params_)\n",
    "best_clf.fit(df_x_select, df_y, eval_set=(df_test_x_select, df_test_y))\n",
    "\n",
    "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
    "mse_train = mean_squared_error(df_y, best_clf.predict(df_x_select))\n",
    "r2_train = r2_score(df_y, best_clf.predict(df_x_select))\n",
    "mse_test = mean_squared_error(df_test_y, best_clf.predict(df_test_x_select))\n",
    "r2_test = r2_score(df_test_y, best_clf.predict(df_test_x_select))\n",
    "\n",
    "# Print the training MSE and R2 score\n",
    "print('Training MSE: ', mse_train)\n",
    "print('Training R2: ', r2_train)\n",
    "\n",
    "# Print the testing MSE and R2 score\n",
    "print('Testing MSE: ', mse_test)\n",
    "print('Testing R2: ', r2_test)\n",
    "\n",
    "# Display the scores\n",
    "plot_scores(mse_train, r2_train, mse_test, r2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print catboost weights with the feature names and biases\n",
    "print(model.get_feature_importance(prettified=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AdaBoost Regressor model\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "model = AdaBoostRegressor(n_estimators=100, learning_rate=0.01)\n",
    "\n",
    "# Train the model\n",
    "model.fit(df_x_select, df_y)\n",
    "\n",
    "# Get the training MSE, R2 score, Testing MSE, and R2 score\n",
    "mse_train = mean_squared_error(df_y, model.predict(df_x_select))\n",
    "r2_train = r2_score(df_y, model.predict(df_x_select))\n",
    "mse_test = mean_squared_error(df_test_y, model.predict(df_test_x_select))\n",
    "r2_test = r2_score(df_test_y, model.predict(df_test_x_select))\n",
    "\n",
    "# Print the training MSE and R2 score\n",
    "print('Training MSE: ', mse_train)\n",
    "print('Training R2: ', r2_train)\n",
    "\n",
    "# Print the testing MSE and R2 score\n",
    "print('Testing MSE: ', mse_test)\n",
    "print('Testing R2: ', r2_test)\n",
    "\n",
    "# Display the scores\n",
    "plot_scores(mse_train, r2_train, mse_test, r2_test)\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/AdaBoost_model.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "57bc2b6ce032b5f0e93daa91901b7ea38a856826ef43aa9e95b6d3999f5310df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
