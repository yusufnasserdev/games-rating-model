{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T14:19:38.839351Z",
     "start_time": "2023-04-14T14:19:38.815268Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:33:46.390000Z",
     "start_time": "2023-04-14T10:33:46.279076Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "dateparse = lambda x: datetime.strptime(x, '%d/%m/%Y')\n",
    "\n",
    "df_origin = pd.read_csv('games-regression-dataset.csv', parse_dates=['Original Release Date' , 'Current Version Release Date'], date_parser=dateparse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Download the icons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to string\n",
    "df_origin['Icon URL'] = df_origin['Icon URL'].astype(str)\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "def download_image(url, filename):\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(filename, 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "\n",
    "# Create a folder to store the images\n",
    "if not os.path.exists('icons'):\n",
    "    os.makedirs('icons')\n",
    "\n",
    "# Download the images\n",
    "for i, row in df_origin.iterrows():\n",
    "    download_image(row['Icon URL'], f'icons/{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the URL with the icon filename which is the index of the row\n",
    "df_origin['Icon URL'] = df_origin.apply(lambda row : f'icons/{row.name}.png', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df, df_test = train_test_split(df_origin, test_size=0.2, random_state=42)\n",
    "df_test.to_csv('df_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dropping unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:07.218975Z",
     "start_time": "2023-04-14T10:35:07.199405Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop Primary Genre\n",
    "df.drop(['Primary Genre', 'ID', 'URL'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dates preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:36:05.737187Z",
     "start_time": "2023-04-14T10:36:05.699973Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessed_data.csv', parse_dates=['Original Release Date' , 'Current Version Release Date'])\n",
    "\n",
    "# Convert the datetime to ordinal\n",
    "\n",
    "df['Original Release Date'] = df['Original Release Date'].apply(lambda x: x.toordinal())\n",
    "df['Current Version Release Date'] = df['Current Version Release Date'].apply(lambda x: x.toordinal())\n",
    "df[['Original Release Date', 'Current Version Release Date']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with the age of the game\n",
    "df['game_age'] = df['Current Version Release Date'] - df['Original Release Date']\n",
    "\n",
    "# Create a new column with the time since the last update\n",
    "df['last_update'] = datetime.now().toordinal() - df['Current Version Release Date'] \n",
    "\n",
    "df[['game_age', 'last_update']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T16:07:00.183708Z",
     "start_time": "2023-04-13T16:07:00.127616Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:05.138536Z",
     "start_time": "2023-04-13T14:33:05.138536Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:07.893814Z",
     "start_time": "2023-04-13T14:33:07.893814Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:21.139604Z",
     "start_time": "2023-04-13T14:33:21.139604Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Genres'] = df['Genres'].astype(str)\n",
    "df['Genres'] = df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "genre_counts = df.explode('Genres').groupby('Genres').size().sort_values(ascending=False)\n",
    "genre_counts\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:43.637371Z",
     "start_time": "2023-04-13T14:33:43.637371Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Developer'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:47.598084Z",
     "start_time": "2023-04-13T14:33:47.598084Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Developer'].unique().size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T14:33:49.943309Z",
     "start_time": "2023-04-13T14:33:49.943309Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Languages'] = df['Languages'].astype(str)\n",
    "\n",
    "df['Languages'] = df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "langs_counts = df.explode('Languages').groupby('Languages').size().sort_values(ascending=False)\n",
    "print(langs_counts[1:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Developer preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:14.110217Z",
     "start_time": "2023-04-14T10:35:13.123201Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to string\n",
    "df['Developer'] = df['Developer'].astype(str)\n",
    "df['Developer'] = df['Developer'].str.replace(\"'\", \"\").str.strip('[]')\n",
    "\n",
    "# Replace the developer names with less than 3 games with 'Other'\n",
    "dev_counts = df['Developer'].value_counts()\n",
    "other = dev_counts[dev_counts < 3].index\n",
    "df['Developer'] = df['Developer'].replace(other, 'Other')\n",
    "\n",
    "dev_df = df[['Developer', 'Average User Rating']].groupby('Developer').mean()\n",
    "\n",
    "# Save dev_df to be used on the test set\n",
    "dev_df.to_csv('encoders/dev_df.csv')\n",
    "\n",
    "# Replace the developer names with the average user rating from dev_df\n",
    "df['Developer'] = df['Developer'].replace(dev_df.index, dev_df['Average User Rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Genres preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NLP approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:21.245168Z",
     "start_time": "2023-04-14T10:35:20.885068Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the genres column to a list of strings\n",
    "df['Genres'] = df['Genres'].astype(str)\n",
    "df['Genres'] = df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# drop Games, Strategy, Entertainment from the Genres column\n",
    "df['Genres'] = df['Genres'].apply(lambda x: [genre for genre in x if genre not in ['Games', 'Strategy', 'Entertainment']])\n",
    "\n",
    "# Join the list of genres into a single string\n",
    "genres = df['Genres'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Create a count Vectorizer and fit it to the genres\n",
    "count_vec = CountVectorizer()\n",
    "bow_genres = count_vec.fit_transform(genres)\n",
    "\n",
    "# Apply principal component analysis to reduce the dimensionality\n",
    "pca = PCA(n_components=10)\n",
    "pca_genres = pca.fit_transform(bow_genres.toarray())\n",
    "\n",
    "# Add the PCA-transformed genres to the original dataframe\n",
    "for i in range(len(pca_genres[0])):\n",
    "    df[f'Genre_PCA_{i}'] = pca_genres[:, i]\n",
    "\n",
    "# Drop the original column\n",
    "df = df.drop(['Genres'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dummy variables approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the genres column to a list of strings\n",
    "df['Genres'] = df['Genres'].astype(str)\n",
    "df['Genres'] = df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# drop Games, Strategy, Entertainment from the Genres column\n",
    "df['Genres'] = df['Genres'].apply(lambda x: [genre for genre in x if genre not in ['Games', 'Strategy', 'Entertainment']])\n",
    "\n",
    "# Replace genres with counts less than 100 with 'infrequent' as it would represent a very small percentage of the data (less than 2%)\n",
    "other = df['Genres'].explode().value_counts()[df['Genres'].explode().value_counts() < 100].index\n",
    "df['Genres'] = df['Genres'].apply(lambda x: [genre if genre not in other else 'infrequent' for genre in x])\n",
    "\n",
    "# Get dummy variables for the genres\n",
    "genres = pd.get_dummies(df['Genres'].apply(pd.Series).stack(), prefix=\"genre\", dummy_na=False).sum(level=0)\n",
    "\n",
    "# Save the genres dummies to be used on the test set\n",
    "genres.to_csv('encoders/genres.csv', index=False)\n",
    "\n",
    "# Add the dummy variables to the original dataframe\n",
    "df = pd.concat([df, genres], axis=1)\n",
    "\n",
    "# Drop the original column\n",
    "df = df.drop(['Genres'], axis=1)\n",
    "\n",
    "# Fill NaN with 0\n",
    "genre_cols = [col for col in df.columns if col.startswith('genre')] # get all columns with prefix 'genre'\n",
    "df[genre_cols] = df[genre_cols].fillna(0) # fill NaN with 0 for selected columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi-label binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the genres column to a list of strings\n",
    "df['Genres'] = df['Genres'].astype(str)\n",
    "df['Genres'] = df['Genres'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# drop Games, Strategy, Entertainment from the Genres column\n",
    "df['Genres'] = df['Genres'].apply(lambda x: [genre for genre in x if genre not in ['Games', 'Strategy', 'Entertainment']])\n",
    "\n",
    "# Replace genres with counts less than 100 with 'infrequent' as it would represent a very small percentage of the data (less than 2%)\n",
    "other = df['Genres'].explode().value_counts()[df['Genres'].explode().value_counts() < 100].index\n",
    "df['Genres'] = df['Genres'].apply(lambda x: [genre if genre not in other else 'infrequent_genre' for genre in x])\n",
    "\n",
    "# Instantiate the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit the MultiLabelBinarizer to the genres\n",
    "mlb.fit(df['Genres'])\n",
    "\n",
    "# Drop nan from the classes\n",
    "mlb.classes_ = np.delete(mlb.classes_, np.where(mlb.classes_ == 'nan'))\n",
    "\n",
    "# Save the mlb for later use with the test data\n",
    "pickle.dump(mlb, open('encoders/mlb_genres.pkl', 'wb'))\n",
    "\n",
    "# Transform the genres into a one-hot encoded array\n",
    "genres_mlb = mlb.transform(df['Genres'])\n",
    "\n",
    "# Create a dataframe from the one-hot encoded array\n",
    "genres_mlb_df = pd.DataFrame(genres_mlb, columns=mlb.classes_)\n",
    "\n",
    "# Add the one-hot encoded genres to the original dataframe\n",
    "df = pd.concat([df, genres_mlb_df], axis=1)\n",
    "\n",
    "# Drop the original column\n",
    "df = df.drop(['Genres'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Languages preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NLP approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:38.695352Z",
     "start_time": "2023-04-14T10:35:38.599717Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the langs column to a list of strings\n",
    "df['Languages'] = df['Languages'].astype(str)\n",
    "df['Languages'] = df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# Drop the English language from the Languages column\n",
    "df['Languages'] = df['Languages'].apply(lambda x: [lang for lang in x if lang not in ['EN']])\n",
    "\n",
    "# Join the list of langs into a single string\n",
    "languages = df['Languages'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Create a count Vectorizer and fit it to the langs\n",
    "count_vec = CountVectorizer()\n",
    "bow_languages = count_vec.fit_transform(languages)\n",
    "\n",
    "# Apply principal component analysis to reduce the dimensionality\n",
    "pca = PCA(n_components=10)\n",
    "pca_languages = pca.fit_transform(bow_languages.toarray())\n",
    "\n",
    "# Add the PCA-transformed langs to the original dataframe\n",
    "for i in range(len(pca_languages[0])):\n",
    "    df[f'Languages_PCA_{i}'] = pca_languages[:, i]\n",
    "\n",
    "# Drop the original column\n",
    "df = df.drop(['Languages'], axis=1)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df.iloc[:, 15:].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dummy variables approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the langs column to a list of strings\n",
    "df['Languages'] = df['Languages'].astype(str)\n",
    "df['Languages'] = df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# Create a column with the number of languages supported\n",
    "df['langs_count'] = df['Languages'].apply(lambda x: len(x))\n",
    "\n",
    "# Drop the English language from the Languages column (it is the most common language and would dominate the model)\n",
    "df['Languages'] = df['Languages'].apply(lambda x: [lang for lang in x if lang not in ['EN']])\n",
    "\n",
    "# Replace langs with counts less than 500 with 'infrequent_langs' as it would represent a very small percentage of the data (less than 10%)\n",
    "other = df['Languages'].explode().value_counts()[df['Languages'].explode().value_counts() < 500].index\n",
    "df['Languages'] = df['Languages'].apply(lambda x: [lang if lang not in other else 'infrequent' for lang in x])\n",
    "\n",
    "# Get dummy variables for the langs\n",
    "langs = pd.get_dummies(df['Languages'].apply(pd.Series).stack(), prefix='lang', dummy_na=False).sum(level=0)\n",
    "\n",
    "langs.to_csv('encoders/langs.csv', index=False)\n",
    "\n",
    "# Add the dummy variables to the original dataframe\n",
    "df = pd.concat([df, langs], axis=1)\n",
    "\n",
    "# Drop the original column\n",
    "df = df.drop(['Languages'], axis=1)\n",
    "\n",
    "# Fill NaN with 0\n",
    "lang_cols = [col for col in df.columns if col.startswith('lang')] # get all columns with prefix 'lang'\n",
    "df[lang_cols] = df[lang_cols].fillna(0) # fill NaN with 0 for selected columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi-label binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Convert the langs column to a list of strings\n",
    "df['Languages'] = df['Languages'].astype(str)\n",
    "df['Languages'] = df['Languages'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")\n",
    "\n",
    "# Create a column with the number of languages supported\n",
    "df['langs_count'] = df['Languages'].apply(lambda x: len(x))\n",
    "\n",
    "# Drop the English language from the Languages column (it is the most common language and would dominate the model)\n",
    "df['Languages'] = df['Languages'].apply(lambda x: [lang for lang in x if lang not in ['EN']])\n",
    "\n",
    "# Replace langs with counts less than 500 with 'infrequent_langs' as it would represent a very small percentage of the data (less than 10%)\n",
    "other = df['Languages'].explode().value_counts()[df['Languages'].explode().value_counts() < 400].index\n",
    "df['Languages'] = df['Languages'].apply(lambda x: [lang if lang not in other else 'infrequent_lang' for lang in x])\n",
    "\n",
    "# Instantiate the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit the MultiLabelBinarizer to the langs\n",
    "mlb.fit(df['Languages'])\n",
    "\n",
    "# Drop nan from the classes\n",
    "mlb.classes_ = np.delete(mlb.classes_, np.where(mlb.classes_ == 'nan'))\n",
    "\n",
    "# Save the mlb for later use with the test data\n",
    "pickle.dump(mlb, open('encoders/mlb_langs.pkl', 'wb'))\n",
    "\n",
    "# Transform the langs into a one-hot encoded array\n",
    "langs_mlb = mlb.transform(df['Languages'])\n",
    "\n",
    "# Create a dataframe from the one-hot encoded array\n",
    "langs_mlb_df = pd.DataFrame(langs_mlb, columns=mlb.classes_)\n",
    "\n",
    "# Add the encoded langs to the original dataframe\n",
    "df = pd.concat([df, langs_mlb_df], axis=1)\n",
    "\n",
    "# Drop the original column\n",
    "df = df.drop(['Languages'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## In-app Purchases preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:52.935497Z",
     "start_time": "2023-04-14T10:35:52.881403Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Free apps might skew the in-app purchases column,\n",
    "# so we might split the dataset into free and paid apps\n",
    "\n",
    "df['In-app Purchases'] = df['In-app Purchases'].astype(str)\n",
    "df['In-app Purchases'] = df['In-app Purchases'].str.strip('[]').str.replace(\"'\", \"\").str.split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:35:54.565885Z",
     "start_time": "2023-04-14T10:35:54.456681Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to float\n",
    "df['In-app Purchases'] = df['In-app Purchases'].apply(lambda x: [float(i) for i in x])\n",
    "\n",
    "# Get the number of in-app purchases\n",
    "df['purchases_count'] = df['In-app Purchases'].apply(lambda x: len(x))\n",
    "\n",
    "# Get the lowest, highest and average purchase\n",
    "df['lowest_purchase'] = df['In-app Purchases'].apply(lambda x: min(x) if len(x) > 0 else 0)\n",
    "df['highest_purchase'] = df['In-app Purchases'].apply(lambda x: max(x) if len(x) > 0 else 0)\n",
    "df['average_purchase'] = df['In-app Purchases'].apply(lambda x: np.mean(x) if len(x) > 0 else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:36:00.624093Z",
     "start_time": "2023-04-14T10:36:00.576764Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop the original column\n",
    "df = df.drop(['In-app Purchases'], axis=1)\n",
    "\n",
    "df['lowest_purchase'] = df['lowest_purchase'].fillna(0)\n",
    "df['highest_purchase'] = df['highest_purchase'].fillna(0)\n",
    "df['average_purchase'] = df['average_purchase'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Age Rating preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:36:03.614263Z",
     "start_time": "2023-04-14T10:36:03.579637Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to string\n",
    "df['Age Rating'] = df['Age Rating'].astype(str)\n",
    "\n",
    "# Remove the + sign\n",
    "df['Age Rating'] = df['Age Rating'].str.replace('+', '')\n",
    "\n",
    "# Convert to int\n",
    "df['Age Rating'] = df['Age Rating'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NLP preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:43:13.386866Z",
     "start_time": "2023-04-14T10:43:13.386866Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "def preprocess_nlp(col):\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "    # Remove URLs and email addresses\n",
    "    df[col] = df[col].apply(lambda x: re.sub(r'http\\S+|www.\\S+|\\S+@\\S+', '', x))\n",
    "\n",
    "    # Remove the punctuation, numbers, and convert to lowercase\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(re.findall(r'\\w+', x.lower())))\n",
    "\n",
    "    # Remove the stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "    # Stemming\n",
    "    st = nltk.PorterStemmer()\n",
    "    df[col] = df[col].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "    # Lemmatization\n",
    "    lem = nltk.WordNetLemmatizer()\n",
    "    df[col] = df[col].apply(lambda x: \" \".join([lem.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "    # Remove the frequent and rare words\n",
    "    freq = pd.Series(' '.join(df[col]).split()).value_counts()\n",
    "    common_freq = list(freq[:10].index)\n",
    "    rare_freq = list(freq[-10:].index)\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(x for x in x.split() if x not in common_freq+rare_freq))\n",
    "\n",
    "    # Remove the whitespaces\n",
    "    df[col] = df[col].apply(lambda x: \" \".join(x.strip() for x in x.split()))\n",
    "\n",
    "    # Replace NaN values with empty string\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "    # Convert text data to bag-of-words representation\n",
    "    vectorizer = CountVectorizer()\n",
    "    BoW = vectorizer.fit_transform(df[col])\n",
    "\n",
    "    # Apply principal component analysis to reduce the dimensionality\n",
    "    pca_ = PCA(n_components=2)\n",
    "    pca_col = pca_.fit_transform(BoW.toarray())\n",
    "\n",
    "    # Add the PCA-transformed col to the original dataframe\n",
    "    for feat in range(len(pca_col[0])):\n",
    "        df[f'{col}_PCA_{feat}'] = pca_col[:, feat]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:43:49.505557Z",
     "start_time": "2023-04-14T10:43:49.505557Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocess_nlp('Description')\n",
    "preprocess_nlp('Subtitle')\n",
    "preprocess_nlp('Name')\n",
    "\n",
    "df = df.drop(['Description', 'Subtitle', 'Name'], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Icon preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Extract features from the icons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:52:48.299695Z",
     "start_time": "2023-04-14T10:52:13.578629Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess_icon(img_path):\n",
    "    # Load the game icon image\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (100, 100))\n",
    "\n",
    "    # Extract color features using color histograms\n",
    "    colors = ('b', 'g', 'r')\n",
    "    color_features = []\n",
    "    for k, col in enumerate(colors):\n",
    "        hist = cv2.calcHist([img], [k], None, [256], [0, 256])\n",
    "        color_features.append(hist)\n",
    "\n",
    "    # Reshape the color features to have a single dimension\n",
    "    color_features = np.concatenate(color_features).ravel()\n",
    "\n",
    "    # Extract shape features using edge detection\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "    edge_features = np.array(edges).flatten()\n",
    "\n",
    "    # Combine the color and shape features into a single feature vector\n",
    "    feature_vector = np.concatenate((color_features, edge_features))\n",
    "\n",
    "    # Normalize the feature vector to have unit length\n",
    "    normalized_feature_vector = feature_vector / np.linalg.norm(feature_vector)\n",
    "    \n",
    "    return normalized_feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Create a list to store the feature vectors\n",
    "icon_features = []\n",
    "\n",
    "df['Icon URL'] = df['Icon URL'].astype(str)\n",
    "\n",
    "# Iterate over the images and extract the features\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    feature_vec = preprocess_icon(row['Icon URL'])\n",
    "    icon_features.append((row['Icon URL'], feature_vec))\n",
    "    \n",
    "# Apply PCA to reduce the number of features\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit([f[1] for f in icon_features])\n",
    "reduced_features = pca.transform([f[1] for f in icon_features])\n",
    "\n",
    "# Save the pca instance for later use\n",
    "pickle.dump(pca, open('encoders/icon_pca.pkl', 'wb'))\n",
    "\n",
    "# Convert the reduced features to a dataframe\n",
    "icon_features_df = pd.DataFrame({'Icon URL': [f[0] for f in icon_features],\n",
    "                                    'Icon1': reduced_features[:,0],\n",
    "                                    'Icon2': reduced_features[:,1],\n",
    "                                    'Icon3': reduced_features[:,2],\n",
    "                                    'Icon4': reduced_features[:,3]})\n",
    "\n",
    "icon_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Add the icon features to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T10:53:05.530442Z",
     "start_time": "2023-04-14T10:53:04.975260Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge the icon features with the original dataframe on the icon URL\n",
    "df = df.merge(icon_features_df, on='Icon URL', how='left')\n",
    "\n",
    "# Drop the icon URL column\n",
    "df = df.drop(['Icon URL'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed data\n",
    "df.to_csv('preprocessed_data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df.drop(['Average User Rating'], axis=1)\n",
    "df_y = df['Average User Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print only the columns with nan values and their count\n",
    "print(df_x.isnull().sum()[df_x.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cols = df_x.columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_x = scaler.fit_transform(df_x)\n",
    "\n",
    "df_x = pd.DataFrame(df_x, columns=cols)\n",
    "\n",
    "# Save the scaler\n",
    "pickle.dump(scaler, open('scalers/std_scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Convert all the features to float\n",
    "df_x = df_x.astype(float)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_x = scaler.fit_transform(df_x)\n",
    "\n",
    "# Save the scaler\n",
    "pickle.dump(scaler, open('scalers/min_max_scaler.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "selector = SelectKBest(f_regression, k=10)\n",
    "selector.fit_transform(df_x, df_y)\n",
    "\n",
    "# Save the selector\n",
    "pickle.dump(selector, open('encoders/selector.pkl', 'wb'))\n",
    "\n",
    "# Drop the columns that were not selected\n",
    "df_x_select = df_x[:, selector.get_support()]\n",
    "\n",
    "\n",
    "type(df_x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T14:40:09.683179Z",
     "start_time": "2023-04-14T14:40:09.629559Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/LR_model.pkl', 'wb'))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))\n",
    "\n",
    "# print the features weights\n",
    "for i in range(len(model.coef_)):\n",
    "    print(f'Feature {selected_features[i]}: {model.coef_[i]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T11:00:32.712100Z",
     "start_time": "2023-04-14T11:00:32.642347Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a ridge regression model\n",
    "model = Ridge(alpha=0.5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/Ridge_model.pkl', 'wb'))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T11:00:35.138470Z",
     "start_time": "2023-04-14T11:00:35.102309Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a lasso regression model\n",
    "model = Lasso(alpha=0.5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/Lasso_model.pkl', 'wb'))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T11:00:37.282470Z",
     "start_time": "2023-04-14T11:00:37.226872Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an elastic net regression model\n",
    "model = ElasticNet(alpha=0.5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/ElasticNet_model.pkl', 'wb'))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T11:02:11.403793Z",
     "start_time": "2023-04-14T11:00:55.155863Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a polynomial regression model\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.fit_transform(X_test)\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Save the model\n",
    "pickle.dump(model, open('models/Polynomial_model.pkl', 'wb'))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_poly)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n",
    "print('Coefficient of determination: %.2f' % r2_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
